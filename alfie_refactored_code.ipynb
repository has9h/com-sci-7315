{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/has9h/com-sci-7315/blob/main/alfie_refactored_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cf6f21f",
      "metadata": {
        "id": "1cf6f21f"
      },
      "source": [
        "#Cell 01 â€” Environment, paths, reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wUIfIEo7_0O6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUIfIEo7_0O6",
        "outputId": "ddc3eddc-740a-4ed0-9d30-32672eacb76f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GOEyLl7WDmoG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOEyLl7WDmoG",
        "outputId": "ada58c48-b8d4-4e56-cf3b-f7a5b1279908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore) (2.0.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from fvcore) (3.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from fvcore) (11.3.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from iopath>=0.1.7->fvcore) (4.14.1)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=075e3479a32abf9bec8cce073e7b5f82f2e193a0f5baa84b228e13db2b01683a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=b4459cfdae7264b02b290e1d6454f5655c6ecd77e09abd4d46ca3155d1740e7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/96/04/4f5f31ff812f684f69f40cb1634357812220aac58d4698048c\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.2.0 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install fvcore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "599964ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "599964ba",
        "outputId": "9037366b-1e45-438a-96e3-f23e448f7de5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[Env] Google Drive mounted.\n",
            "[Env] torch=2.8.0+cu126, torchvision=0.23.0+cu126, device=cuda\n",
            "[Env] CSV path â†’ /content/drive/MyDrive/fer2013.csv\n"
          ]
        }
      ],
      "source": [
        "# === Cell 01: Environment, paths, reproducibility ===\n",
        "# Purpose: create deterministic environment; define paths used everywhere; pick device.\n",
        "\n",
        "import os, sys, random, warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def in_colab() -> bool:\n",
        "    return \"google.colab\" in sys.modules\n",
        "\n",
        "# (Optional) Mount Drive if running in Colab so FER csv/ckpts can live there.\n",
        "if in_colab():\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "        print(\"[Env] Google Drive mounted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"[Env][WARN] Drive mount failed: {e}\")\n",
        "\n",
        "# --- project folders\n",
        "PROJECT_ROOT = Path(\"./project\"); PROJECT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "CKPT_DIR     = PROJECT_ROOT / \"checkpoints\"; CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LOG_DIR      = PROJECT_ROOT / \"logs\";        LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# --- deterministic cuDNN (slower but stable)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# --- device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"[Env] torch={torch.__version__}, torchvision={torchvision.__version__}, device={device}\")\n",
        "\n",
        "# --- dataset CSV (edit path if needed)\n",
        "FER_CSV_PATH = Path(\"/content/drive/MyDrive/fer2013.csv\") if in_colab() else Path(\"./fer2013.csv\")\n",
        "print(f\"[Env] CSV path â†’ {FER_CSV_PATH.resolve()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36994e40",
      "metadata": {
        "id": "36994e40"
      },
      "source": [
        "#Cell 02 â€” Global config (single source of truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54257985",
      "metadata": {
        "id": "54257985"
      },
      "source": [
        "### Cell 02 â€” Global Configuration (Tuned)\n",
        "\n",
        "| **Category**      | **Key**               | **Value (Default in this cell)** | **Description** |\n",
        "|-------------------|-----------------------|----------------------------------|-----------------|\n",
        "| **I/O**           | `FER_CSV_PATH`        | `./fer2013.csv`                  | Path to FER2013 dataset (CSV). |\n",
        "|                   | `SAVE_BEST_PATH`      | `./project/checkpoints/best_fer.pth` | Filepath to save best model checkpoint. |\n",
        "| **Data**          | `IMG_SIZE`            | `96`                             | Input image size (HxW). |\n",
        "|                   | `BATCH_SIZE`          | `300`                            | Training batch size. |\n",
        "|                   | `NUM_WORKERS`         | `max(2, cpu_count - 2)`          | DataLoader workers for parallelism. |\n",
        "| **Compute**       | `USE_AMP`             | Auto (True if CUDA/MPS available) | Enables Automatic Mixed Precision (AMP). |\n",
        "| **Augmentation**  | `USE_AUG`             | `True`                           | Apply standard augmentations. |\n",
        "|                   | `USE_AUG_ADV`         | `True`                           | Use advanced FER augmentation policy. |\n",
        "|                   | `USE_MIXUP`           | `True`                           | Enable MixUp augmentation. |\n",
        "|                   | `USE_CUTMIX`          | `True`                           | Enable CutMix augmentation. |\n",
        "|                   | `USE_EMA`             | `True`                           | Track EMA weights during training. |\n",
        "|                   | `USE_TTA`             | `True`                           | Use Test-Time Augmentation (test only). |\n",
        "| **Late-phase**    | `AUG_CAP_LATE`        | `True`                           | Cap augmentation strength in later epochs. |\n",
        "|                   | `TAPER_MIX_LATE`      | `True`                           | Gradually reduce MixUp/CutMix late training. |\n",
        "| **Optimiser**     | `USE_SGD`             | `False`                          | Optimizer choice: `False=AdamW`, `True=SGD`. |\n",
        "|                   | `SCHEDULER`           | `\"onecycle\"`                     | LR scheduler type (`onecycle`, `cosine`, `plateau`). |\n",
        "| **Mix/Taper**     | `BASE_MIXUP_PROB`     | `0.45`                           | Initial MixUp probability. |\n",
        "|                   | `BASE_CUTMIX_PROB`    | `0.25`                           | Initial CutMix probability. |\n",
        "|                   | `TAPER_START_FRAC`    | `0.20`                           | Fraction of epochs to start taper. |\n",
        "|                   | `TAPER_END_FRAC`      | `0.85`                           | Fraction of epochs to end taper. |\n",
        "| **Label Smooth**  | `LABEL_SMOOTH_START`  | `0.08`                           | Early-phase label smoothing. |\n",
        "|                   | `LABEL_SMOOTH_END`    | `0.02`                           | Late-phase label smoothing. |\n",
        "| **Fine-tune**     | `FINE_TUNE_FRACTION`  | `0.12`                           | Fraction of total epochs reserved for clean fine-tuning. |\n",
        "\n",
        "---\n",
        "\n",
        "### Hyperparameters (HP)\n",
        "\n",
        "| **Key**            | **Value**  | **Description** |\n",
        "|---------------------|-----------|-----------------|\n",
        "| `EPOCHS`            | `70`      | Total training epochs. |\n",
        "| `LR`                | `3e-4`    | Base learning rate. |\n",
        "| `WD`                | `5e-5`    | Weight decay. |\n",
        "| `EMA_DECAY`         | `0.9995`  | EMA decay factor. |\n",
        "| `AUG_RAMP_EPOCHS`   | `0.40`    | Ramp-up duration for augmentation (fraction of epochs). |\n",
        "| `MIXUP_ALPHA`       | `0.30`    | MixUp Beta distribution parameter. |\n",
        "| `CUTMIX_ALPHA`      | `1.00`    | CutMix Beta distribution parameter. |\n",
        "| `PATIENCE`          | `18`      | Early stopping patience. |\n",
        "| `LR_MIN`            | `5e-5`    | Minimum LR for cosine scheduler. |\n",
        "| `WARMUP_EPOCHS`     | `4`       | Cosine warmup length. |\n",
        "| `PLATEAU_FACTOR`    | `0.5`     | Factor for ReduceLROnPlateau. |\n",
        "| `PLATEAU_PATIENCE`  | `5`       | Patience for ReduceLROnPlateau. |\n",
        "| `MIN_LR`            | `1e-6`    | Min LR for ReduceLROnPlateau. |\n",
        "| `SGD_MOMENTUM`      | `0.9`     | Momentum for SGD. |\n",
        "| `SGD_NESTEROV`      | `True`    | Use Nesterov momentum in SGD. |\n",
        "| `LR_MAX`            | `1e-3`    | Peak LR for OneCycle scheduler. |\n",
        "| `OCL_PCT_START`     | `0.15`    | Fraction of cycle for LR rise (OneCycle). |\n",
        "| `OCL_DIV_FACTOR`    | `12.0`    | Initial LR factor for OneCycle. |\n",
        "| `OCL_FINAL_DIV`     | `20.0`    | Final LR decay for OneCycle. |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1223cbb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1223cbb",
        "outputId": "94b6eea5-f092-48ff-b78e-10947b00da4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[CONFIG]\n",
            "  AUG_CAP_LATE        : True\n",
            "  BASE_CUTMIX_PROB    : 0.25\n",
            "  BASE_MIXUP_PROB     : 0.45\n",
            "  BATCH_SIZE          : 300\n",
            "  FER_CSV_PATH        : /content/drive/MyDrive/fer2013.csv\n",
            "  FINE_TUNE_EPOCHS    : 1\n",
            "  FINE_TUNE_FRACTION  : 0.12\n",
            "  IMG_SIZE            : 96\n",
            "  LABEL_SMOOTH_END    : 0.02\n",
            "  LABEL_SMOOTH_START  : 0.08\n",
            "  NUM_WORKERS         : 10\n",
            "  PRINT_EVERY         : 10\n",
            "  SAVE_BEST_PATH      : project/checkpoints/best_fer.pth\n",
            "  SCHEDULER           : onecycle\n",
            "  TAPER_END_EPOCH     : 9\n",
            "  TAPER_END_FRAC      : 0.85\n",
            "  TAPER_MIX_LATE      : True\n",
            "  TAPER_START_EPOCH   : 2\n",
            "  TAPER_START_FRAC    : 0.2\n",
            "  USE_AMP             : True\n",
            "  USE_AUG             : True\n",
            "  USE_AUG_ADV         : True\n",
            "  USE_CUTMIX          : True\n",
            "  USE_EMA             : True\n",
            "  USE_MIXUP           : True\n",
            "  USE_SGD             : False\n",
            "  USE_TTA             : True\n",
            "\n",
            "[HP]\n",
            "  AUG_RAMP_EPOCHS     : 4\n",
            "  CUTMIX_ALPHA        : 1.0\n",
            "  EMA_DECAY           : 0.9995\n",
            "  EPOCHS              : 11\n",
            "  LR                  : 0.0003\n",
            "  LR_MAX              : 0.001\n",
            "  LR_MIN              : 5e-05\n",
            "  MIN_LR              : 1e-06\n",
            "  MIXUP_ALPHA         : 0.3\n",
            "  OCL_DIV_FACTOR      : 12.0\n",
            "  OCL_FINAL_DIV       : 20.0\n",
            "  OCL_PCT_START       : 0.15\n",
            "  PATIENCE            : 18\n",
            "  PLATEAU_FACTOR      : 0.5\n",
            "  PLATEAU_PATIENCE    : 5\n",
            "  SGD_MOMENTUM        : 0.9\n",
            "  SGD_NESTEROV        : True\n",
            "  WARMUP_EPOCHS       : 4\n",
            "  WD                  : 5e-05\n",
            "\n",
            "[DERIVED]\n",
            "  TAPER_START_EPOCH : 2\n",
            "  TAPER_END_EPOCH   : 9\n",
            "  FINE_TUNE_EPOCHS  : 1\n",
            "  AUG_RAMP_EPOCHS   : 4\n"
          ]
        }
      ],
      "source": [
        "# Purpose: one place to control hyperâ€‘params, toggles, scheduler, and derived numbers.\n",
        "# Notes:\n",
        "# - Added CONFIG[\"PRINT_EVERY\"] so you can control logging stride from config.\n",
        "# - Keep the rest identical to your current setup so other cells donâ€™t break.\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from pathlib import Path\n",
        "from torch.optim import AdamW, SGD\n",
        "from torch.optim.lr_scheduler import (\n",
        "    ReduceLROnPlateau, CosineAnnealingLR, LinearLR, SequentialLR, OneCycleLR\n",
        ")\n",
        "\n",
        "# Respect upstream paths if defined; otherwise use local defaults\n",
        "FER_CSV_PATH = FER_CSV_PATH if 'FER_CSV_PATH' in globals() else Path(\"./fer2013.csv\")\n",
        "CKPT_DIR     = CKPT_DIR     if 'CKPT_DIR'     in globals() else Path(\"./project/checkpoints\")\n",
        "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _amp_available() -> bool:\n",
        "    mps_ok = getattr(torch.backends, \"mps\", None)\n",
        "    return bool(torch.cuda.is_available() or (mps_ok and torch.backends.mps.is_available()))\n",
        "\n",
        "CONFIG = {\n",
        "    # I/O\n",
        "    \"FER_CSV_PATH\": FER_CSV_PATH,\n",
        "    \"SAVE_BEST_PATH\": CKPT_DIR / \"best_fer.pth\",  # legacy single path (still used by baseline)\n",
        "\n",
        "    # Data\n",
        "    \"IMG_SIZE\": 96,                 # 48 also works; 96 helps MBv3/GhostNet\n",
        "    \"BATCH_SIZE\": 300,\n",
        "    \"NUM_WORKERS\": max(2, (os.cpu_count() or 4) - 2),\n",
        "\n",
        "    # Compute\n",
        "    \"USE_AMP\": _amp_available(),\n",
        "\n",
        "    # Augmentation & evaluation toggles\n",
        "    \"USE_AUG\": True,\n",
        "    \"USE_AUG_ADV\": True,\n",
        "    \"USE_MIXUP\": True,\n",
        "    \"USE_CUTMIX\": True,\n",
        "    \"USE_EMA\": True,\n",
        "    \"USE_TTA\": True,\n",
        "\n",
        "    # Late-phase controls\n",
        "    \"AUG_CAP_LATE\": True,\n",
        "    \"TAPER_MIX_LATE\": True,\n",
        "\n",
        "    # Optimiser/scheduler\n",
        "    \"USE_SGD\": False,               # False=AdamW, True=SGD+Nesterov\n",
        "    \"SCHEDULER\": \"onecycle\",        # 'onecycle' | 'cosine' | 'plateau'\n",
        "\n",
        "    # Mix prob base/taper\n",
        "    \"BASE_MIXUP_PROB\": 0.45,\n",
        "    \"BASE_CUTMIX_PROB\": 0.25,\n",
        "    \"TAPER_START_FRAC\": 0.20,\n",
        "    \"TAPER_END_FRAC\":   0.85,\n",
        "\n",
        "    # Label smoothing\n",
        "    \"LABEL_SMOOTH_START\": 0.08,\n",
        "    \"LABEL_SMOOTH_END\":   0.02,\n",
        "\n",
        "    # Clean fine-tune tail (no mix/aug)\n",
        "    \"FINE_TUNE_FRACTION\": 0.12,\n",
        "\n",
        "    # ðŸ”¹ NEW: console logging stride\n",
        "    \"PRINT_EVERY\": 10,              # print every 10 epochs (and on improvement / final)\n",
        "}\n",
        "\n",
        "HP = {\n",
        "    \"EPOCHS\": 11,\n",
        "    \"LR\": 3e-4,\n",
        "    \"WD\": 5e-5,\n",
        "\n",
        "    # EMA\n",
        "    \"EMA_DECAY\": 0.9995,\n",
        "\n",
        "    # Aug/mix schedule\n",
        "    \"AUG_RAMP_EPOCHS\": 0.40,   # fraction of total epochs\n",
        "\n",
        "    \"MIXUP_ALPHA\": 0.30,\n",
        "    \"CUTMIX_ALPHA\": 1.00,\n",
        "\n",
        "    # Early stopping\n",
        "    \"PATIENCE\": 18,\n",
        "\n",
        "    # Cosine path\n",
        "    \"LR_MIN\": 5e-5,\n",
        "    \"WARMUP_EPOCHS\": 4,\n",
        "\n",
        "    # Plateau path\n",
        "    \"PLATEAU_FACTOR\": 0.5,\n",
        "    \"PLATEAU_PATIENCE\": 5,\n",
        "    \"MIN_LR\": 1e-6,\n",
        "\n",
        "    # SGD\n",
        "    \"SGD_MOMENTUM\": 0.9,\n",
        "    \"SGD_NESTEROV\": True,\n",
        "\n",
        "    # One-Cycle\n",
        "    \"LR_MAX\": 1.0e-3,\n",
        "    \"OCL_PCT_START\": 0.15,\n",
        "    \"OCL_DIV_FACTOR\": 12.0,\n",
        "    \"OCL_FINAL_DIV\": 20.0,\n",
        "}\n",
        "\n",
        "# ---- Derived numbers (kept from your notebook) ----\n",
        "E = int(HP[\"EPOCHS\"])\n",
        "assert CONFIG[\"SCHEDULER\"] in (\"plateau\", \"cosine\", \"onecycle\")\n",
        "\n",
        "if HP[\"AUG_RAMP_EPOCHS\"] < 1.0:\n",
        "    HP[\"AUG_RAMP_EPOCHS\"] = max(1, int(round(E * HP[\"AUG_RAMP_EPOCHS\"])))\n",
        "else:\n",
        "    HP[\"AUG_RAMP_EPOCHS\"] = int(HP[\"AUG_RAMP_EPOCHS\"])\n",
        "\n",
        "CONFIG[\"TAPER_START_EPOCH\"] = int(round(CONFIG[\"TAPER_START_FRAC\"] * E))\n",
        "CONFIG[\"TAPER_END_EPOCH\"]   = max(CONFIG[\"TAPER_START_EPOCH\"] + 1, int(round(CONFIG[\"TAPER_END_FRAC\"] * E)))\n",
        "CONFIG[\"FINE_TUNE_EPOCHS\"]  = int(round(CONFIG[\"FINE_TUNE_FRACTION\"] * E))\n",
        "CONFIG[\"TAPER_START_EPOCH\"] = min(CONFIG[\"TAPER_START_EPOCH\"], E - 1)\n",
        "CONFIG[\"TAPER_END_EPOCH\"]   = min(CONFIG[\"TAPER_END_EPOCH\"],   E)\n",
        "CONFIG[\"FINE_TUNE_EPOCHS\"]  = min(CONFIG[\"FINE_TUNE_EPOCHS\"],  max(0, E - 1))\n",
        "\n",
        "# ---- Optimizer/scheduler factories (unchanged) ----\n",
        "def make_optimizer(model):\n",
        "    if CONFIG[\"USE_SGD\"]:\n",
        "        return SGD(model.parameters(), lr=HP[\"LR\"], momentum=HP[\"SGD_MOMENTUM\"],\n",
        "                   nesterov=HP[\"SGD_NESTEROV\"], weight_decay=HP[\"WD\"])\n",
        "    return AdamW(model.parameters(), lr=HP[\"LR\"], weight_decay=HP[\"WD\"])\n",
        "\n",
        "def _make_cosine(optimizer):\n",
        "    warmup_e = max(0, int(HP[\"WARMUP_EPOCHS\"]))\n",
        "    warmup = LinearLR(optimizer, start_factor=1.0, end_factor=1.0, total_iters=max(1, warmup_e))\n",
        "    cosine = CosineAnnealingLR(optimizer, T_max=max(1, E - warmup_e), eta_min=HP[\"LR_MIN\"])\n",
        "    return SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_e]) if warmup_e > 0 else cosine\n",
        "\n",
        "def _make_plateau(optimizer):\n",
        "    return ReduceLROnPlateau(optimizer, mode=\"min\", factor=HP[\"PLATEAU_FACTOR\"],\n",
        "                             patience=HP[\"PLATEAU_PATIENCE\"], min_lr=HP[\"MIN_LR\"], verbose=True)\n",
        "\n",
        "def _make_onecycle(optimizer, steps_per_epoch: int):\n",
        "    return OneCycleLR(optimizer, max_lr=float(HP[\"LR_MAX\"]), epochs=E, steps_per_epoch=int(steps_per_epoch),\n",
        "                      pct_start=float(HP[\"OCL_PCT_START\"]), anneal_strategy=\"cos\",\n",
        "                      cycle_momentum=False, div_factor=float(HP[\"OCL_DIV_FACTOR\"]),\n",
        "                      final_div_factor=float(HP[\"OCL_FINAL_DIV\"]))\n",
        "\n",
        "def build_scheduler(optimizer, steps_per_epoch: int | None = None):\n",
        "    sched = CONFIG[\"SCHEDULER\"]\n",
        "    if sched == \"onecycle\":\n",
        "        if steps_per_epoch is None:\n",
        "            raise ValueError(\"OneCycleLR requires steps_per_epoch; pass len(train_dl).\")\n",
        "        return _make_onecycle(optimizer, steps_per_epoch)\n",
        "    if sched == \"cosine\":\n",
        "        return _make_cosine(optimizer)\n",
        "    if sched == \"plateau\":\n",
        "        return _make_plateau(optimizer)\n",
        "    raise ValueError(f\"Unknown scheduler: {sched}\")\n",
        "\n",
        "def current_lr(optimizer) -> float: return float(optimizer.param_groups[0][\"lr\"])\n",
        "def scheduler_steps_per_batch() -> bool: return CONFIG[\"SCHEDULER\"] == \"onecycle\"\n",
        "\n",
        "def print_config():\n",
        "    print(\"\\n[CONFIG]\");   [print(f\"  {k:20s}: {CONFIG[k]}\") for k in sorted(CONFIG.keys())]\n",
        "    print(\"\\n[HP]\");       [print(f\"  {k:20s}: {HP[k]}\") for k in sorted(HP.keys())]\n",
        "    print(\"\\n[DERIVED]\")\n",
        "    print(f\"  TAPER_START_EPOCH : {CONFIG['TAPER_START_EPOCH']}\")\n",
        "    print(f\"  TAPER_END_EPOCH   : {CONFIG['TAPER_END_EPOCH']}\")\n",
        "    print(f\"  FINE_TUNE_EPOCHS  : {CONFIG['FINE_TUNE_EPOCHS']}\")\n",
        "    print(f\"  AUG_RAMP_EPOCHS   : {HP['AUG_RAMP_EPOCHS']}\")\n",
        "\n",
        "print_config()\n",
        "\n",
        "# --- helper to create unique ckpt path for each backbone (no clashes)\n",
        "def ckpt_path_for(backbone_tag: str) -> Path:\n",
        "    return (CKPT_DIR / f\"best_{backbone_tag}_fer.pth\").resolve()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "883ec470",
      "metadata": {
        "id": "883ec470"
      },
      "source": [
        "#Cell 03 â€” Load FER2013 and split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_cdznT_YnsPh",
      "metadata": {
        "id": "_cdznT_YnsPh"
      },
      "source": [
        "| **Step**                | **Explanation**                                                                                                                                          |\n",
        "| ----------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Import & Path Setup** | Uses `Path` to ensure `FER_CSV_PATH` is correctly resolved from `CONFIG`. Also includes an assertion to stop execution if the file is missing.           |\n",
        "| **CSV Read**            | Loads the FER2013 dataset via `pd.read_csv(FER_CSV_PATH)`.                                                                                               |\n",
        "| **Schema Check**        | Ensures dataset has at least `[\"emotion\", \"pixels\"]` columns, otherwise throws an error.                                                                 |\n",
        "| **Dataset Splits**      | Splits the dataset into: <br>â€¢ **Training**: 28,709 samples <br>â€¢ **Validation (PublicTest)**: 3,589 samples <br>â€¢ **Test (PrivateTest)**: 3,589 samples |\n",
        "| **Index Reset**         | Each split has `.reset_index(drop=True)` so that indices are clean and independent across splits.                                                        |\n",
        "| **Diagnostics**         | Prints the split sizes and confirms counts (`[Split] train=28709, val=3589, test=3589`).                                                                 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc13acbe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc13acbe",
        "outputId": "cf7edf3b-c438-435b-ad87-d34694cf1ffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage\n",
            "Training       28709\n",
            "PublicTest      3589\n",
            "PrivateTest     3589\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# === Cell 03: Load FER2013 and split ===\n",
        "from pathlib import Path\n",
        "FER_CSV_PATH = Path(CONFIG[\"FER_CSV_PATH\"])\n",
        "assert FER_CSV_PATH.exists(), f\"CSV not found: {FER_CSV_PATH}\"\n",
        "\n",
        "df = pd.read_csv(FER_CSV_PATH)\n",
        "assert {\"emotion\",\"pixels\"}.issubset(set(df.columns)), f\"Bad columns: {df.columns.tolist()}\"\n",
        "print(df[\"Usage\"].value_counts())\n",
        "\n",
        "data_df = df  # Alias for compatibility with reference code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3yB_G51lP9tt",
      "metadata": {
        "id": "3yB_G51lP9tt"
      },
      "source": [
        "### Reference Codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rZujksoEOem5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZujksoEOem5",
        "outputId": "3e50897d-2465-45da-de2b-a88287cd3942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   emotion                                             pixels     Usage\n",
            "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
            "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
            "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
            "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
            "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n",
            "       emotion                                             pixels       Usage\n",
            "28709        0  254 254 254 254 254 249 255 160 2 58 53 70 77 ...  PublicTest\n",
            "28710        1  156 184 198 202 204 207 210 212 213 214 215 21...  PublicTest\n",
            "28711        4  69 118 61 60 96 121 103 87 103 88 70 90 115 12...  PublicTest\n",
            "28712        6  205 203 236 157 83 158 120 116 94 86 155 180 2...  PublicTest\n",
            "28713        3  87 79 74 66 74 96 77 80 80 84 83 89 102 91 84 ...  PublicTest\n",
            "...        ...                                                ...         ...\n",
            "32292        3  0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 3 4 21 40 53 65 ...  PublicTest\n",
            "32293        4  178 176 172 173 173 174 176 173 166 166 206 22...  PublicTest\n",
            "32294        3  25 34 42 44 42 47 57 59 59 58 54 51 50 56 63 6...  PublicTest\n",
            "32295        4  255 255 255 255 255 255 255 255 255 255 255 25...  PublicTest\n",
            "32296        4  33 25 31 36 36 42 69 103 132 163 175 183 187 1...  PublicTest\n",
            "\n",
            "[3588 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# Categorizing the dataset to three categories.\n",
        "# Training: To train the model.\n",
        "# PrivateTest: To test the train model; commonly known as Validation.\n",
        "# PublicTest: To test the final model on Test set to check how your model perfomed. Do not use this data as your validation data.\n",
        "train_df = data_df[data_df['Usage']=='Training']\n",
        "valid_df = data_df[data_df['Usage']=='PublicTest']\n",
        "test_df = data_df[data_df['Usage']=='PrivateTest']\n",
        "print(train_df.head())\n",
        "print(valid_df.head(-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ear8_B0XOlop",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ear8_B0XOlop",
        "outputId": "dd0bea34-3e01-44d4-dd89-dcb6351e38eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Split] train=28709, val=3589, test=3589\n"
          ]
        }
      ],
      "source": [
        "print(f\"[Split] train={len(train_df)}, val={len(valid_df)}, test={len(test_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l4meIfqRP4cR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4meIfqRP4cR",
        "outputId": "563dfd12-c1f3-4476-d98c-5e97feaebc7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   emotion                                             pixels        Usage\n",
            "0        0  170 118 101 88 88 75 78 82 66 74 68 59 63 64 6...  PrivateTest\n",
            "1        5  7 5 8 6 7 3 2 6 5 4 4 5 7 5 5 5 6 7 7 7 10 10 ...  PrivateTest\n",
            "2        6  232 240 241 239 237 235 246 117 24 24 22 13 12...  PrivateTest\n",
            "3        4  200 197 149 139 156 89 111 58 62 95 113 117 11...  PrivateTest\n",
            "4        2  40 28 33 56 45 33 31 78 152 194 200 186 196 20...  PrivateTest\n",
            "   -----   -------    -------    --------     -----    -------\n",
            "   emotion                                             pixels       Usage\n",
            "0        0  254 254 254 254 254 249 255 160 2 58 53 70 77 ...  PublicTest\n",
            "1        1  156 184 198 202 204 207 210 212 213 214 215 21...  PublicTest\n",
            "2        4  69 118 61 60 96 121 103 87 103 88 70 90 115 12...  PublicTest\n",
            "3        6  205 203 236 157 83 158 120 116 94 86 155 180 2...  PublicTest\n",
            "4        3  87 79 74 66 74 96 77 80 80 84 83 89 102 91 84 ...  PublicTest\n"
          ]
        }
      ],
      "source": [
        "# Test-check to see wether usage labels have been allocated to the dataset/not.\n",
        "valid_df = valid_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "print(test_df.head())\n",
        "print('   -----   -------    -------    --------     -----    -------')\n",
        "print(valid_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28703c52",
      "metadata": {
        "id": "28703c52"
      },
      "source": [
        "# â€” Dataset (48â†’96), returns tensor in [0..255], 1Ã—HÃ—W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OynQ5TCQP8Cg",
      "metadata": {
        "id": "OynQ5TCQP8Cg"
      },
      "outputs": [],
      "source": [
        "# Normalization of the train and validation data.\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "\n",
        "class expressions(Dataset):\n",
        "    def __init__(self, df, transforms=None):\n",
        "        self.df = df\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.loc[index]\n",
        "        image, label = np.array([x.split() for x in self.df.loc[index, ['pixels']]]), row['emotion']\n",
        "        #image = image.reshape(1,48,48)\n",
        "        image = np.asarray(image).astype(np.uint8).reshape(48,48,1)\n",
        "        #image = np.reshape(image,(1,48,48))\n",
        "\n",
        "\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return image.clone().detach(), label\n",
        "\n",
        "#import albumentations as A\n",
        "stats = ([0.5],[0.5])\n",
        "\n",
        "Labels = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']\n",
        "\n",
        "train_tsfm = T.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.Grayscale(num_output_channels=1),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(*stats,inplace=True),\n",
        "])\n",
        "valid_tsfm = T.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.Grayscale(num_output_channels=1),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(*stats,inplace=True)\n",
        "])\n",
        "\n",
        "train_ds = expressions(train_df, train_tsfm)\n",
        "valid_ds = expressions(valid_df, valid_tsfm)\n",
        "test_ds = expressions(test_df, valid_tsfm)\n",
        "val_ds = valid_ds  # Alias for existing code that expects val_ds\n",
        "\n",
        "batch_size = 400\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True,\n",
        "                      num_workers=2, pin_memory=True)\n",
        "valid_dl = DataLoader(valid_ds, batch_size*2,\n",
        "                    num_workers=2, pin_memory=True)\n",
        "test_dl = DataLoader(test_ds, batch_size*2,\n",
        "                    num_workers=2, pin_memory=True)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "input_size = 48*48\n",
        "output_size = len(Labels)\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    predictions, preds = torch.max(output, dim=1)\n",
        "    return torch.tensor(torch.sum(preds==labels).item()/len(preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21641599",
      "metadata": {
        "id": "21641599"
      },
      "source": [
        "### Cell â€” Expression Model Class\n",
        "\n",
        "This cell defines the `expression_model` class, a custom subclass of `torch.nn.Module`, which encapsulates the training and validation workflow for facial expression recognition.\n",
        "\n",
        "**Key Responsibilities:**\n",
        "- **`training_step(batch)`**  \n",
        "  - Runs a forward pass on a training batch.  \n",
        "  - Computes and returns the **cross-entropy loss** for optimization.  \n",
        "\n",
        "- **`validation_step(batch)`**  \n",
        "  - Evaluates the model on a validation batch.  \n",
        "  - Returns both **validation loss** and **accuracy** as a dictionary.  \n",
        "\n",
        "- **`validation_epoch_end(outputs)`**  \n",
        "  - Aggregates validation metrics across all batches in an epoch.  \n",
        "  - Computes the **mean loss** and **mean accuracy** for the full epoch.  \n",
        "\n",
        "- **`epoch_end(epoch, result)`**  \n",
        "  - Logs the epoch number, validation loss, and validation accuracy in a formatted string.  \n",
        "\n",
        "**Why this structure?**  \n",
        "- Separates training and validation logic cleanly.  \n",
        "- Makes the training loop simpler, since loss computation, metric calculation, and logging are encapsulated in the class.  \n",
        "- Ensures validation metrics are detached from gradients to save memory.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JvyBvhPpTASX",
      "metadata": {
        "id": "JvyBvhPpTASX"
      },
      "outputs": [],
      "source": [
        "# Expression model class for training and validation purpose.\n",
        "\n",
        "class expression_model(nn.Module):\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)\n",
        "        loss = F.cross_entropy(out, labels)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)\n",
        "        loss = F.cross_entropy(out, labels)\n",
        "        acc = accuracy(out, labels)\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()\n",
        "        batch_acc = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_acc).mean()\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "\n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch[{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7TBBlepTA8K",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7TBBlepTA8K",
        "outputId": "db9ed33a-1cf5-41c7-9348-4f78a99dfeb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are training on: cuda.\n"
          ]
        }
      ],
      "source": [
        "# To check wether Google Colab GPU has been assigned/not.\n",
        "torch.cuda.is_available()\n",
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "device = get_default_device()\n",
        "print(f'You are training on: {device}.')\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nyyv0X9ATFsR",
      "metadata": {
        "id": "nyyv0X9ATFsR"
      },
      "outputs": [],
      "source": [
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "valid_dl = DeviceDataLoader(valid_dl, device)\n",
        "test_dl = DeviceDataLoader(test_dl, device)\n",
        "\n",
        "val_dl = valid_dl  # Alias for existing code that expects val_dl\n",
        "\n",
        "baseline_train_dl = train_dl  # Reference code training DataLoader (48x48, DeviceDataLoader wrapped)\n",
        "baseline_valid_dl = valid_dl  # Reference code validation DataLoader (48x48, DeviceDataLoader wrapped)\n",
        "baseline_test_dl = test_dl    # Reference code test DataLoader (48x48, DeviceDataLoader wrapped)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9676d27f",
      "metadata": {
        "id": "9676d27f"
      },
      "source": [
        "#BASE LAYER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zM_d5oNZTGUD",
      "metadata": {
        "id": "zM_d5oNZTGUD"
      },
      "outputs": [],
      "source": [
        "# Model - 7 layer\n",
        "class expression(expression_model):\n",
        "    def __init__(self,classes):\n",
        "        super().__init__()\n",
        "        self.num_classes = classes\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=3, padding=1),  #(input channels, output channels)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(8, 32, kernel_size=3, padding=1),  #(input channels, output channels)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 64 x 24 x 24\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 128 x 12 x 12\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 256 x 6 x 6\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256*6*6, 2304),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2304, 1152),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1152, 576),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(576,288),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(288,144),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(144,self.num_classes))\n",
        "\n",
        "    def forward(self, xb):\n",
        "        return self.network(xb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zpm7mwdI_N8i",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zpm7mwdI_N8i",
        "outputId": "438e9533-9a31-49a8-9a7a-84d850140eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== BASELINE MODEL EXPERIMENT (Reference Code) ===\n",
            "[Baseline Probe] logits=(800, 7), loss=1.9523\n",
            "=== END BASELINE MODEL EXPERIMENT ===\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== BASELINE MODEL EXPERIMENT (Reference Code) ===\")\n",
        "baseline_model = expression(7)\n",
        "baseline_model.train()\n",
        "baseline_model = to_device(baseline_model, device)\n",
        "# Use reference code data loaders for baseline model\n",
        "xb_baseline, yb_baseline = next(iter(valid_dl))  # Use reference code DataLoader\n",
        "with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=CONFIG[\"USE_AMP\"]):\n",
        "    logits_baseline = baseline_model(xb_baseline)\n",
        "    loss_baseline = F.cross_entropy(logits_baseline, yb_baseline)\n",
        "loss_baseline.backward(); baseline_model.zero_grad(set_to_none=True)\n",
        "print(f\"[Baseline Probe] logits={tuple(logits_baseline.shape)}, loss={loss_baseline.item():.4f}\")\n",
        "print(\"=== END BASELINE MODEL EXPERIMENT ===\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yrTm7wQIArSu",
      "metadata": {
        "id": "yrTm7wQIArSu"
      },
      "source": [
        "#======baseline example given in the exam notebook few epoch trial run===================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Tztupbng-9Jr",
      "metadata": {
        "id": "Tztupbng-9Jr"
      },
      "outputs": [],
      "source": [
        "# RUN_BASELINE_EXPERIMENT = True\n",
        "\n",
        "# if RUN_BASELINE_EXPERIMENT:\n",
        "#     print(\"=== TRAINING REFERENCE CODE BASELINE MODEL ===\")\n",
        "#     baseline_model = expression(7)\n",
        "#     baseline_model = to_device(baseline_model, device)\n",
        "\n",
        "#     # Create a simple training function for baseline model using reference code methods\n",
        "#     def train_baseline_model(model, train_loader, valid_loader, epochs=10):\n",
        "#         optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "#         best_acc = 0.0\n",
        "\n",
        "#         for epoch in range(epochs):\n",
        "#             # ---------- Training ----------\n",
        "#             model.train()\n",
        "#             train_loss = 0.0\n",
        "#             n_batches = 0\n",
        "#             for batch in train_loader:\n",
        "#                 loss = model.training_step(batch)\n",
        "#                 optimizer.zero_grad()\n",
        "#                 loss.backward()\n",
        "#                 optimizer.step()\n",
        "#                 train_loss += loss.item()\n",
        "#                 n_batches += 1\n",
        "\n",
        "#             avg_train_loss = train_loss / max(1, n_batches)\n",
        "\n",
        "#             # ---------- Validation ----------\n",
        "#             model.eval()\n",
        "#             val_outputs = []\n",
        "#             with torch.no_grad():\n",
        "#                 for batch in valid_loader:\n",
        "#                     val_out = model.validation_step(batch)\n",
        "#                     val_outputs.append(val_out)\n",
        "\n",
        "#             val_result = model.validation_epoch_end(val_outputs)\n",
        "\n",
        "#             # Epoch summary (Epoch i/N style)\n",
        "#             print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "#                   f\"train_loss: {avg_train_loss:.4f}, \"\n",
        "#                   f\"val_loss: {val_result['val_loss']:.4f}, \"\n",
        "#                   f\"val_acc: {val_result['val_acc']:.4f}\")\n",
        "\n",
        "#             # Keep original per-epoch hook\n",
        "#             model.epoch_end(epoch, val_result)\n",
        "\n",
        "#             # Save best baseline model\n",
        "#             if val_result['val_acc'] > best_acc:\n",
        "#                 best_acc = val_result['val_acc']\n",
        "#                 torch.save(\n",
        "#                     {\"model_state\": model.state_dict()},\n",
        "#                     CONFIG[\"SAVE_BEST_PATH\"].parent / \"best_baseline_model.pth\"\n",
        "#                 )\n",
        "\n",
        "#         return best_acc\n",
        "\n",
        "#     print(f\"[Baseline] Using reference code DataLoaders - \"\n",
        "#           f\"train batches: {len(baseline_train_dl)}, valid batches: {len(baseline_valid_dl)}\")\n",
        "\n",
        "#     # --- Run for 10 full epochs (Epoch 0..9) ---\n",
        "#     best_baseline_acc = train_baseline_model(\n",
        "#         baseline_model, baseline_train_dl, baseline_valid_dl, epochs=10\n",
        "#     )\n",
        "\n",
        "#     print(f\"Best baseline model accuracy: {best_baseline_acc:.4f}\")\n",
        "#     print(\"=== BASELINE MODEL TRAINING COMPLETE ===\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08lTUI2OAYhb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08lTUI2OAYhb",
        "outputId": "6ad7da98-7815-4869-ded6-16e7bef9a735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 3 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs: 0.32751 GFLOPs\n"
          ]
        }
      ],
      "source": [
        "from fvcore.nn import FlopCountAnalysis\n",
        "input = torch.randn(1, 1, 48, 48) # The input size should be the same as the size that you put into your model\n",
        "#Get the network and its FLOPs\n",
        "num_classes = 7\n",
        "model = expression(num_classes)\n",
        "flops = FlopCountAnalysis(model, input)\n",
        "print(f\"FLOPs: {flops.total()/1e9:.5f} GFLOPs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kNEV3fCI-IEo"
      },
      "id": "kNEV3fCI-IEo"
    },
    {
      "cell_type": "markdown",
      "id": "fQSWoD1__R0c",
      "metadata": {
        "id": "fQSWoD1__R0c"
      },
      "source": [
        "#=========ANYTHING TO BASELINE ENDED====================="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2NWP8JxzIVm7",
      "metadata": {
        "id": "2NWP8JxzIVm7"
      },
      "source": [
        "### End of Reference Code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae77ae7c",
      "metadata": {
        "id": "ae77ae7c"
      },
      "source": [
        "### Cell 04 â€” Dataset Logic and Rationale\n",
        "\n",
        "**Logic Flow:**\n",
        "1. **Custom Dataset Class (`FER2013Dataset`)**\n",
        "   - Inherits from `torch.utils.data.Dataset`.\n",
        "   - Reads the FER2013 CSV rows:  \n",
        "     - `pixels`: a string of 2304 numbers â†’ converted to a NumPy array â†’ reshaped into `[48,48]`.\n",
        "     - `emotion`: integer label (0â€“6).\n",
        "   - Converts to a tensor `[1,48,48]` and rescales to the desired resolution `[1,H,W]` using bilinear interpolation.\n",
        "   - Returns `(image, label)` pairs compatible with PyTorch training.\n",
        "\n",
        "2. **Hybrid vs. Reference Datasets**\n",
        "   - **HybridEffNet datasets**: Resized to **96Ã—96** for better alignment with EfficientNet backbone (higher receptive field, more representational power).\n",
        "   - **Reference code datasets**: Kept at **48Ã—48** as a baseline (original FER2013 format, lighter compute).\n",
        "\n",
        "3. **Default Assignment**\n",
        "   - The default datasets (`train_ds`, `val_ds`, `test_ds`) are mapped to the **96Ã—96 HybridEffNet versions**, so the rest of the training pipeline runs on the stronger configuration.\n",
        "   - The original 48Ã—48 versions are still available for controlled comparisons and sanity checks.\n",
        "\n",
        "---\n",
        "\n",
        "**Why This Design?**\n",
        "- **Resizing 48â†’96:** EfficientNet architectures were designed for larger input resolutions; upscaling preserves pipeline compatibility and generally boosts accuracy (at the cost of more FLOPs).\n",
        "- **Keeping Both Versions:** Retaining 48Ã—48 alongside 96Ã—96 allows quick ablation studies (accuracy vs. efficiency trade-off).\n",
        "- **Encapsulation in Class:** All preprocessing logic is encapsulated inside `FER2013Dataset`, keeping the training loop clean and flexible.\n",
        "- **Future-proofing:** Switching resolution is controlled via a single parameter (`img_size`), making experiments reproducible and less error-prone.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c93cf74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c93cf74",
        "outputId": "2b5628fa-1ae4-48f1-f6bf-6c9a45cd52c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Dataset] HybridEffNet datasets (96x96) and reference code datasets (48x48) both ready.\n"
          ]
        }
      ],
      "source": [
        "# === Cell 04: Dataset (48â†’96), returns [1,H,W] in 0..255 float ===\n",
        "import torchvision.transforms.functional as VF\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class FER2013Dataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, img_size: int = 96):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_size = int(img_size)\n",
        "        if len(self.df) > 0:\n",
        "            _ = self._get_x(0)\n",
        "\n",
        "    def _get_x(self, i: int) -> torch.Tensor:\n",
        "        px = self.df.iloc[i][\"pixels\"]\n",
        "        arr = np.fromstring(str(px), sep=\" \", dtype=np.float32)\n",
        "        arr = np.array(str(px).split(), dtype=np.float32)\n",
        "        assert arr.size == 48*48, f\"Row {i}: expected 2304 pixels, got {arr.size}\"\n",
        "        x = torch.from_numpy(arr.reshape(48, 48)).unsqueeze(0)  # [1,48,48], float32 in 0..255\n",
        "        x = VF.resize(\n",
        "            x,\n",
        "            [self.img_size, self.img_size],\n",
        "            interpolation=torchvision.transforms.InterpolationMode.BILINEAR,\n",
        "            antialias=True,\n",
        "        )\n",
        "        return x\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        x = self._get_x(i).contiguous()  # [1,H,W], float32 0..255\n",
        "        y = int(self.df.iloc[i][\"emotion\"])\n",
        "        return x, y\n",
        "\n",
        "# Create HybridEffNet datasets (96x96)\n",
        "IMG_SIZE = int(CONFIG[\"IMG_SIZE\"])\n",
        "train_ds_hybrid = FER2013Dataset(train_df, IMG_SIZE)\n",
        "val_ds_hybrid   = FER2013Dataset(valid_df,   IMG_SIZE)\n",
        "test_ds_hybrid  = FER2013Dataset(test_df,  IMG_SIZE)\n",
        "\n",
        "# Keep reference code datasets available for baseline experiments (48x48)\n",
        "# train_ds, valid_ds, test_ds from reference code are still available above\n",
        "\n",
        "# Set default datasets to HybridEffNet versions for main training\n",
        "train_ds = train_ds_hybrid\n",
        "val_ds = val_ds_hybrid\n",
        "test_ds = test_ds_hybrid\n",
        "\n",
        "print(\"[Dataset] HybridEffNet datasets (96x96) and reference code datasets (48x48) both ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fc72637",
      "metadata": {
        "id": "3fc72637"
      },
      "source": [
        "#Cell 05 â€” DataLoaders"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ywuLnWmpLnz",
      "metadata": {
        "id": "1ywuLnWmpLnz"
      },
      "source": [
        "| **Aspect**             | **Design Choice**             | **Rationale / Benefit**                                                  |\n",
        "| ---------------------- | ----------------------------- | ------------------------------------------------------------------------ |\n",
        "| **Batch size**         | `BATCH` from config           | Flexible tuning; balances GPU memory usage and throughput.               |\n",
        "| **Workers**            | `NUM_WORKERS` from config     | Parallel data loading â†’ faster pipeline.                                 |\n",
        "| **Pin memory**         | Enabled if CUDA available     | Faster CPUâ†’GPU transfer, avoids page faults.                             |\n",
        "| **Training loader**    | Shuffle=True, batch=BATCH     | Randomized sampling prevents bias, improves generalization.              |\n",
        "| **Validation loader**  | Shuffle=False, batch=2Ã—BATCH  | Deterministic eval + speedup with larger batches.                        |\n",
        "| **Test loader**        | Same as validation            | Ensures reproducible benchmarking, efficient evaluation.                 |\n",
        "| **Persistent workers** | Kept alive if workers>0       | Reduces overhead of re-spawning processes per epoch.                     |\n",
        "| **Sanity check**       | Print val batch shape + range | Debug safeguard: verifies `[N,1,96,96]` shape and pixel range `[0,255]`. |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed12317c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed12317c",
        "outputId": "b8d60c81-ba7d-4e3d-9650-8d55db18ff75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DataLoaders] HybridEffNet DataLoaders (96x96) and reference code DataLoaders (48x48) both ready.\n",
            "[Check] val batch: torch.Size([600, 1, 96, 96]), range [0.0,255.0]\n"
          ]
        }
      ],
      "source": [
        "# === Cell 05: DataLoaders ===\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH = int(CONFIG[\"BATCH_SIZE\"])\n",
        "NUM_WORKERS = int(CONFIG[\"NUM_WORKERS\"])\n",
        "PIN = bool(torch.cuda.is_available())  # safer on CPU-only runs\n",
        "\n",
        "train_dl_hybrid = DataLoader(\n",
        "    train_ds, batch_size=BATCH, shuffle=True,\n",
        "    num_workers=NUM_WORKERS, pin_memory=PIN, persistent_workers=(NUM_WORKERS>0)\n",
        ")\n",
        "val_dl_hybrid   = DataLoader(\n",
        "    val_ds,   batch_size=BATCH*2, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=PIN, persistent_workers=(NUM_WORKERS>0)\n",
        ")\n",
        "test_dl_hybrid  = DataLoader(\n",
        "    test_ds,  batch_size=BATCH*2, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=PIN, persistent_workers=(NUM_WORKERS>0)\n",
        ")\n",
        "\n",
        "# Set default DataLoaders to HybridEffNet versions for main training\n",
        "train_dl = train_dl_hybrid\n",
        "val_dl = val_dl_hybrid\n",
        "test_dl = test_dl_hybrid\n",
        "\n",
        "# Reference code DataLoaders (train_dl, valid_dl, test_dl from DeviceDataLoader) still available for baseline experiments\n",
        "\n",
        "print(\"[DataLoaders] HybridEffNet DataLoaders (96x96) and reference code DataLoaders (48x48) both ready.\")\n",
        "\n",
        "xb, yb = next(iter(val_dl))\n",
        "print(f\"[Check] val batch: {xb.shape}, range [{xb.min():.1f},{xb.max():.1f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb43e1aa",
      "metadata": {
        "id": "bb43e1aa"
      },
      "source": [
        "#Cell 06 â€” Advanced augmentation primitives (photometric, geometric, occlusion, elastic)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7860cfeb",
      "metadata": {
        "id": "7860cfeb"
      },
      "source": [
        "### Cell 06 â€” Advanced Augmentation Primitives (Grayscale)\n",
        "\n",
        "**Logic:**\n",
        "- The FER2013 dataset is grayscale (single channel, 48Ã—48 upscaled to 96Ã—96).  \n",
        "- Standard color augmentations (hue, saturation, color jitter) do not apply.  \n",
        "- Instead, this cell builds a **custom augmentation library** specialized for grayscale images.  \n",
        "- Functions are organized into **photometric**, **geometric**, and **elastic/occlusion** categories, all operating directly on `[1,H,W]` tensors in the range `[0..255]`.\n",
        "\n",
        "---\n",
        "\n",
        "**Reasoning Behind Each Category:**\n",
        "\n",
        "1. **Photometric (intensity-based changes)**  \n",
        "   - `gauss_noise`: Adds Gaussian noise to simulate sensor noise.  \n",
        "   - `rand_gamma`: Adjusts brightness curve (mimics lighting changes).  \n",
        "   - `rand_contrast`: Scales contrast around image mean.  \n",
        "   - `rand_equalize`: Histogram equalization for balanced brightness.  \n",
        "   - `rand_jpeg`: Simulates compression artifacts (robustness to real-world data).  \n",
        "   - `rand_vignette`: Darkens edges, mimicking lens/camera imperfections.  \n",
        "   - `rand_blur`: Blurs images in a range-safe way (helps model learn robustness to soft focus).\n",
        "\n",
        "   *Why:* Facial expression datasets are sensitive to lighting/contrast variations â€” simulating these improves generalization.\n",
        "\n",
        "2. **Geometric (spatial distortions)**  \n",
        "   - `rand_affine_small`: Small random rotations, translations, scaling, and shears.  \n",
        "   - `rand_pad_crop`: Random shifts via reflective padding and cropping.  \n",
        "   - `rand_hflip`: Horizontal flip with 50% chance (mimics left/right symmetry of faces).  \n",
        "\n",
        "   *Why:* Expressions should be invariant to slight geometric shifts (head tilt, position in frame).\n",
        "\n",
        "3. **Elastic (local warps and occlusion)**  \n",
        "   - `rand_elastic`: Smooth, random pixel-level displacements, mimicking distortions in facial tissue/expressions.  \n",
        "   - **(occlusion placeholder):** Later additions might cover partial masking (hands, glasses, etc.).\n",
        "\n",
        "   *Why:* Faces in the wild often have small deformations or obstructions; elastic deformations help the model handle them.\n",
        "\n",
        "---\n",
        "\n",
        "**Overall Reasoning:**\n",
        "- **Diversity without color bias:** Since images are grayscale, augmentations target intensity and shape rather than color channels.  \n",
        "- **Robustness to real-world variance:** JPEG artifacts, vignettes, noise, and blur simulate common low-quality capture scenarios.  \n",
        "- **Balanced difficulty:** Small ranges are chosen (e.g., `max_rot=12Â°`) so augmentations improve generalization but donâ€™t destroy key expression features.  \n",
        "- **Modularity:** Each function can be called independently or assembled into augmentation pipelines (used later in Cell 07).\n",
        "\n",
        "**Key Principle:**  \n",
        "These augmentations make the model *less brittle* by forcing it to learn invariant features of expressions, rather than overfitting to perfect, centered, noise-free 48Ã—48 FER images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cba5bd2",
      "metadata": {
        "id": "0cba5bd2"
      },
      "outputs": [],
      "source": [
        "# === Cell 06: Advanced augmentation primitives (grayscale) ===\n",
        "import io\n",
        "from PIL import Image, ImageOps\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def _to_pil_gray(x255: torch.Tensor) -> Image.Image:\n",
        "    x = x255.clamp(0,255).to(torch.uint8).squeeze(0).cpu().numpy()\n",
        "    return Image.fromarray(x, mode='L')\n",
        "\n",
        "def _from_pil_gray(img: Image.Image) -> torch.Tensor:\n",
        "    return torch.tensor(np.array(img, dtype=np.uint8), dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "def _clip(x): return x.clamp(0.0, 255.0)\n",
        "\n",
        "# photometric\n",
        "def gauss_noise(x, sigma=0.02): return _clip(x + torch.randn_like(x)*(sigma*255.))\n",
        "def rand_gamma(x, gmin=0.85, gmax=1.25):\n",
        "    g = random.uniform(gmin,gmax); x01=(x/255.).clamp(0,1); return (x01**g)*255.\n",
        "def rand_contrast(x, scale=0.25):\n",
        "    c = 1.0+random.uniform(-scale,scale); m=x.mean(dim=(1,2),keepdim=True); return _clip((x-m)*c+m)\n",
        "def rand_equalize(x):\n",
        "    img=_to_pil_gray(x); img=ImageOps.equalize(img); return _from_pil_gray(img).to(x.dtype).to(x.device)\n",
        "def rand_jpeg(x, qmin=55, qmax=85):\n",
        "    img=_to_pil_gray(x); buf=io.BytesIO(); img.save(buf,format='JPEG',quality=random.randint(qmin,qmax))\n",
        "    buf.seek(0); img2=Image.open(buf).convert('L'); return _from_pil_gray(img2).to(x.dtype).to(x.device)\n",
        "def rand_vignette(x, strength=0.25):\n",
        "    _,H,W=x.shape; yy,xx=torch.meshgrid(torch.linspace(-1,1,H,device=x.device),\n",
        "                                        torch.linspace(-1,1,W,device=x.device),indexing='ij')\n",
        "    r=torch.sqrt(xx**2+yy**2); mask=1.0-strength*(r/r.max()).clamp(0,1)\n",
        "    return _clip(x*mask.unsqueeze(0))\n",
        "\n",
        "# Range-safe blur: convert to [0,1] â†’ blur â†’ back to [0,255]\n",
        "def rand_blur(x, k=3):\n",
        "    x01 = (x/255.).clamp(0,1)\n",
        "    y01 = torchvision.transforms.functional.gaussian_blur(x01, kernel_size=k)\n",
        "    return (y01 * 255.0).clamp(0,255)\n",
        "\n",
        "# geometric\n",
        "def rand_affine_small(x, max_rot=12, max_trans=0.08, max_shear=8.0, max_scale=0.08):\n",
        "    H,W=x.shape[-2:]\n",
        "    angle=random.uniform(-max_rot,max_rot)\n",
        "    trans=[int(random.uniform(-max_trans,max_trans)*W),int(random.uniform(-max_trans,max_trans)*H)]\n",
        "    scale=1.0+random.uniform(-max_scale,max_scale)\n",
        "    shear=[random.uniform(-max_shear,max_shear),0.0]\n",
        "    return torchvision.transforms.functional.affine(x, angle=angle, translate=trans, scale=scale, shear=shear)\n",
        "\n",
        "def rand_pad_crop(x, pad=3):\n",
        "    _,H,W=x.shape; xpad=F.pad(x,(pad,pad,pad,pad),mode='reflect'); i=random.randint(0,2*pad); j=random.randint(0,2*pad)\n",
        "    return xpad[:,i:i+H, j:j+W]\n",
        "\n",
        "def rand_hflip(x, p=0.5): return torchvision.transforms.functional.hflip(x) if random.random()<p else x\n",
        "\n",
        "# elastic\n",
        "def rand_elastic(x, alpha=1.0, sigma=4.0):\n",
        "    _,H,W=x.shape\n",
        "    def _gkern(k=21,s=sigma):\n",
        "        ax=torch.arange(k,device=x.device)-(k-1)/2; ker=torch.exp(-(ax**2)/(2*s*s)); ker/=ker.sum(); return ker\n",
        "    k=21; gx=_gkern(k).view(1,1,1,k); gy=_gkern(k).view(1,1,k,1)\n",
        "    dx=F.conv2d(F.conv2d(torch.randn(1,1,H,W,device=x.device),gx,padding=(0,k//2)),gy,padding=(k//2,0)).squeeze()*alpha\n",
        "    dy=F.conv2d(F.conv2d(torch.randn(1,1,H,W,device=x.device),gx,padding=(0,k//2)),gy,padding=(k//2,0)).squeeze()*alpha\n",
        "    yy,xx=torch.meshgrid(torch.linspace(-1,1,H,device=x.device),\n",
        "                         torch.linspace(-1,1,W,device=x.device),indexing='ij')\n",
        "    xx=(xx+dx/(W/2)).clamp(-1,1); yy=(yy+dy/(H/2)).clamp(-1,1)\n",
        "    grid=torch.stack([xx,yy],dim=-1).unsqueeze(0)\n",
        "    return F.grid_sample(x.unsqueeze(0), grid, mode='bilinear', padding_mode='border', align_corners=True).squeeze(0)\n",
        "\n",
        "# occlusio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65ce3ae2",
      "metadata": {
        "id": "65ce3ae2"
      },
      "source": [
        "### Cell 06.x â€” Augmentation Utility: Band Occlusion\n",
        "\n",
        "**Logic:**\n",
        "- The function creates **horizontal band occlusions** across specific facial regions (eyes, mouth, top, bottom, or middle).\n",
        "- It modifies a clone of the input tensor `[C,H,W]` by replacing the chosen band with a neutral fill value that matches the input range.\n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "1. **Range Awareness**\n",
        "   - `_neutral_fill_value_range_aware(x)` ensures the fill matches the input imageâ€™s scale:\n",
        "     - `[-1,1]` â†’ fill = `0.0` (neutral gray).\n",
        "     - `[0,1]` â†’ fill = `0.5`.\n",
        "     - `[0,255]` â†’ fill = `127.5`.\n",
        "   - This avoids introducing artificial biases (too bright or too dark occlusions).\n",
        "\n",
        "2. **Band Placement**\n",
        "   - Band height = `frac Ã— H` (e.g., `0.18 Ã— H` â†’ ~18% of image height).\n",
        "   - Vertical anchor depends on `mode`:\n",
        "     - **eyes**: ~30% down from top.\n",
        "     - **mouth**: ~72% down.\n",
        "     - **mid**: centered at 50%.\n",
        "     - **top**: near forehead region (~10%).\n",
        "     - **bottom**: near chin (~85%).\n",
        "     - **default**: random vertical placement.\n",
        "\n",
        "3. **Application**\n",
        "   - The chosen band is overwritten with the neutral fill.\n",
        "   - Output = occluded tensor, same shape as input.\n",
        "\n",
        "---\n",
        "\n",
        "**Reasoning:**\n",
        "- **Why band occlusion?**  \n",
        "  Facial expressions are often localized (eyes, mouth). By masking these regions, the model is forced to rely on *global context* and not overfit to single regions.\n",
        "  \n",
        "- **Why multiple modes?**  \n",
        "  - `eyes` occlusion: tests robustness to missing eye cues.  \n",
        "  - `mouth` occlusion: tests robustness to missing mouth cues.  \n",
        "  - `mid/top/bottom`: adds variability, simulating accessories (scarves, masks, hats).  \n",
        "\n",
        "- **Why neutral fill?**  \n",
        "  Keeps occlusion consistent and range-safe, so the augmentation doesnâ€™t introduce unrealistic artifacts (e.g., black bars that act as shortcuts).\n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "This augmentation simulates real-world occlusions (glasses glare, masks, hands) and forces the model to learn **distributed representations** of emotion, improving robustness and generalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pBRdqZrNfOQO",
      "metadata": {
        "id": "pBRdqZrNfOQO"
      },
      "outputs": [],
      "source": [
        "# === Cell 06.x: Aug Utils â€” band_occlusion  ===\n",
        "import torch\n",
        "import random\n",
        "\n",
        "def _neutral_fill_value_range_aware(x: torch.Tensor) -> float:\n",
        "    \"\"\"\n",
        "    Mid-gray fill compatible with [-1,1], [0,1], or [0,255] ranges.\n",
        "    \"\"\"\n",
        "    xmin = float(x.min())\n",
        "    xmax = float(x.max())\n",
        "    if xmin >= -1.0 and xmax <= 1.0:\n",
        "        return 0.0\n",
        "    if xmin >= 0.0 and xmax <= 1.0:\n",
        "        return 0.5\n",
        "    if xmax > 1.0:\n",
        "        return 127.5\n",
        "    return 0.0\n",
        "\n",
        "def band_occlusion(img: torch.Tensor, mode: str = 'eyes', frac: float = 0.18) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Draw a horizontal band occlusion corresponding to a face region.\n",
        "    Args:\n",
        "        img: [C,H,W] tensor (float). Accepted ranges: [-1,1], [0,1], [0,255].\n",
        "        mode: 'eyes' | 'mouth' | 'top' | 'bottom' | 'mid'\n",
        "        frac: vertical band height as fraction of H.\n",
        "    Returns:\n",
        "        Tensor with an occluded band (on a clone).\n",
        "    \"\"\"\n",
        "    if not (torch.is_tensor(img) and img.ndim == 3):\n",
        "        raise TypeError(\"band_occlusion expects a tensor of shape [C,H,W].\")\n",
        "\n",
        "    C, H, W = img.shape\n",
        "    band_h = max(1, int(H * float(frac)))\n",
        "    fill = _neutral_fill_value_range_aware(img)\n",
        "    out = img.clone()\n",
        "\n",
        "    # Default anchors (approximate facial landmarks for FER crops)\n",
        "    if mode == 'eyes':\n",
        "        top = int(0.30 * H) - band_h // 2\n",
        "    elif mode == 'mouth':\n",
        "        top = int(0.72 * H) - band_h // 2\n",
        "    elif mode == 'mid':\n",
        "        top = int(0.50 * H) - band_h // 2\n",
        "    elif mode == 'top':\n",
        "        top = int(0.10 * H)\n",
        "    elif mode == 'bottom':\n",
        "        top = int(0.85 * H) - band_h\n",
        "    else:\n",
        "        # Fallback: random vertical placement\n",
        "        top = random.randint(0, max(0, H - band_h))\n",
        "\n",
        "    top = max(0, min(top, H - band_h))\n",
        "    out[:, top:top + band_h, :] = fill\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6602b98",
      "metadata": {
        "id": "a6602b98"
      },
      "source": [
        "#Cell 07 â€” AugMixâ€‘lite and advanced augmentation builder (returns [-1,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c7c9d63",
      "metadata": {
        "id": "3c7c9d63"
      },
      "source": [
        "### Cell 07 â€” AugMix-lite and Advanced FER Augmentation (â†’ [-1,1])\n",
        "\n",
        "**Logic:**\n",
        "- Builds a *configurable augmentation pipeline* for FER2013 images, combining **photometric, geometric, and occlusion** augmentations with **AugMix-lite** for diversity.\n",
        "- Normalizes final outputs into the `[-1,1]` range for model input compatibility.\n",
        "- Uses a strength parameter (`0.0â€“1.0`) to scale probabilities and magnitudes of augmentations.\n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "1. **AugMix-lite core**\n",
        "   - `augmix_lite`: Blends the original image with multiple augmentation â€œbranches.â€\n",
        "   - Each branch applies random augmentations from different banks, repeated `depth` times.\n",
        "   - The mixed result = weighted average of original + augmented images â†’ prevents overfitting to one distorted view.\n",
        "\n",
        "2. **Probability scheduling (`build_advanced_fer_augment`)**\n",
        "   - Probabilities (chance of applying a transform) scale with `strength`:\n",
        "     - Photometric (`p_photo`): up to 70% chance.\n",
        "     - Geometric (`p_geom`): up to 60%.\n",
        "     - Occlusion (`p_occl`): up to 40%.\n",
        "     - Histogram equalization (`p_equal`): up to 20%.\n",
        "     - Blur (`p_blur`): up to 15%.\n",
        "   - Magnitudes also scale: gamma range, contrast, JPEG quality, vignette strength, elastic deformation amplitude, rotation/translation/shear/scale bounds.\n",
        "\n",
        "3. **Augmentation banks**\n",
        "   - **Photometric bank**: noise, gamma correction, contrast shift, JPEG artifacts, vignette.\n",
        "   - **Geometric bank**: affine transforms, pad+crop jitter, horizontal flip, elastic warps.\n",
        "   - **Occlusion bank**: band occlusion (eyes, mouth, top) and localized erasing.\n",
        "\n",
        "4. **Final augment function**\n",
        "   - Applies a *probabilistic sequence*:\n",
        "     1. Pad+crop (jitter).\n",
        "     2. Optional blur.\n",
        "     3. Random photometric transform.\n",
        "     4. AugMix-lite (mix across banks).\n",
        "     5. Optional geometric or occlusion.\n",
        "     6. Optional equalization.\n",
        "   - Normalizes from `[0..255]` â†’ `[0..1]` â†’ `[-1,1]`.\n",
        "\n",
        "---\n",
        "\n",
        "**Reasoning:**\n",
        "- **Why AugMix-lite?**  \n",
        "  Blending multiple randomized views stabilizes training, improves robustness, and prevents reliance on a single augmentation type.\n",
        "  \n",
        "- **Why banks?**  \n",
        "  Grouping augmentations by type ensures diversity while controlling probabilities independently (e.g., geometric distortions shouldnâ€™t dominate over photometric ones).\n",
        "\n",
        "- **Why strength scaling?**  \n",
        "  Allows adaptive control: weaker augmentations early (to avoid destabilizing convergence), stronger augmentations later (to prevent overfitting).\n",
        "\n",
        "- **Why normalization to [-1,1]?**  \n",
        "  Matches model input conventions (EfficientNet and many pretrained backbones expect normalized input).\n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "This cell provides a **flexible augmentation factory** (`FER_AUG_FACTORY`) that dynamically assembles augmentations, mixes them with AugMix-lite, and normalizes the results. It balances **diversity**, **realism**, and **training stability** â€” critical for FER, where overfitting to lighting, pose, or occlusion artifacts is common.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d7bef03",
      "metadata": {
        "id": "8d7bef03"
      },
      "outputs": [],
      "source": [
        "# === Cell 07: AugMix-lite + advanced augmentation builder (â†’ [-1,1]) ===\n",
        "def _apply_bank(x, bank, k=2):\n",
        "    y=x.clone()\n",
        "    for _ in range(k):\n",
        "        y = random.choice(bank)(y)\n",
        "    return y\n",
        "\n",
        "def augmix_lite(x, banks, alpha=0.65, branches=2, depth=2):\n",
        "    mix=x.clone()\n",
        "    for _ in range(branches):\n",
        "        y=_apply_bank(x, random.choice(banks), k=depth)\n",
        "        mix=mix+y\n",
        "    mix = mix / (branches+1.0)\n",
        "    return (1-alpha)*x + alpha*mix\n",
        "\n",
        "def build_advanced_fer_augment(strength: float):\n",
        "    s=float(max(0.0,min(1.0,strength)))\n",
        "    # probabilities\n",
        "    p_photo=0.7*(0.5+0.5*s); p_geom=0.6*(0.5+0.5*s); p_occl=0.40*(0.5+0.5*s)\n",
        "    p_equal=0.20*s; p_blur=0.15*s\n",
        "    # magnitudes\n",
        "    gamma_rng=(0.85-0.15*s, 1.20+0.05*s)\n",
        "    contrast=0.20+0.10*s\n",
        "    jpeg_q=(55-int(10*s), 85)\n",
        "    vignette=0.15+0.20*s\n",
        "    elastic_a=0.6+0.8*s\n",
        "    rot=10+5*s; shear=6+4*s; trans=0.06+0.03*s; scale=0.06+0.04*s\n",
        "\n",
        "    photometric_bank = [\n",
        "        lambda z: gauss_noise(z, sigma=0.015+0.02*s),\n",
        "        lambda z: rand_gamma(z, *gamma_rng),\n",
        "        lambda z: rand_contrast(z, scale=contrast),\n",
        "        lambda z: rand_jpeg(z, qmin=jpeg_q[0], qmax=jpeg_q[1]),\n",
        "        lambda z: rand_vignette(z, strength=vignette),\n",
        "    ]\n",
        "    geometric_bank = [\n",
        "        lambda z: rand_affine_small(z, max_rot=rot, max_trans=trans, max_shear=shear, max_scale=scale),\n",
        "        lambda z: rand_pad_crop(z, pad=3),\n",
        "        lambda z: rand_hflip(z, p=0.5),\n",
        "        lambda z: rand_elastic(z, alpha=elastic_a, sigma=4.0),\n",
        "    ]\n",
        "    occlusion_bank = [\n",
        "        lambda z: band_occlusion(z, mode='eyes',  frac=0.16+0.06*s),\n",
        "        lambda z: band_occlusion(z, mode='mouth', frac=0.16+0.06*s),\n",
        "        lambda z: band_occlusion(z, mode='top',   frac=0.14+0.06*s),\n",
        "        lambda z: localized_erasing(z, min_frac=0.01, max_frac=0.05),\n",
        "    ]\n",
        "    banks=[photometric_bank, geometric_bank, occlusion_bank]\n",
        "\n",
        "    def _norm_to_m11(x255):\n",
        "        x01=(x255/255.).clamp(0,1)\n",
        "        return (x01 - 0.5) * 2.0\n",
        "\n",
        "    def _augment(x):\n",
        "        if random.random() < p_geom:  x = rand_pad_crop(x, pad=3)\n",
        "        if random.random() < p_blur:  x = rand_blur(x, k=3)\n",
        "        if random.random() < p_photo: x = random.choice(photometric_bank)(x)\n",
        "        x = augmix_lite(x, banks=banks, alpha=CONFIG.get(\"AUG_ALPHA\",0.65), branches=2, depth=2)\n",
        "        if random.random() < p_geom:  x = random.choice(geometric_bank)(x)\n",
        "        if random.random() < p_occl:  x = random.choice(occlusion_bank)(x)\n",
        "        if random.random() < p_equal: x = rand_equalize(x)\n",
        "        return _norm_to_m11(x)\n",
        "\n",
        "    return _augment\n",
        "\n",
        "FER_AUG_FACTORY = build_advanced_fer_augment if CONFIG.get(\"USE_AUG_ADV\", False) else build_advanced_fer_augment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f016c61",
      "metadata": {
        "id": "9f016c61"
      },
      "source": [
        "#Cell 08 â€” Metrics, class weights, losses (Labelâ€‘Smoothing + Focal composite)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4744386d",
      "metadata": {
        "id": "4744386d"
      },
      "source": [
        "### Cell 08 â€” Metrics, Class Weights, and Composite Loss\n",
        "\n",
        "**Logic:**\n",
        "This cell defines the key **evaluation metric** (accuracy), computes **class weights** for imbalanced data, and implements a family of **loss functions** (Label Smoothing, Focal, and a Composite Smoothed-Focal loss).\n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "1. **Metric**\n",
        "   - `accuracy(logits, targets)`: Compares predicted class (argmax of logits) with ground truth and averages over the batch.\n",
        "   - *Reasoning:* Simple, interpretable metric for classification tasks; complements loss-based monitoring.\n",
        "\n",
        "2. **Class Weights**\n",
        "   - `compute_class_weights(df)`:  \n",
        "     - Counts class frequencies in training set.  \n",
        "     - Computes inverse-frequency weights (`total / count[c]`) to counter class imbalance.  \n",
        "     - Normalizes so the mean weight = 1.  \n",
        "   - *Reasoning:* FER2013 has skewed distributions (e.g., many more \"happy\" than \"disgust\"), so weighting helps the model not ignore minority classes.\n",
        "\n",
        "3. **Loss Functions**\n",
        "   - **LabelSmoothingCE**  \n",
        "     - Softens hard labels by distributing a fraction `eps` of probability mass across other classes.  \n",
        "     - Reduces overconfidence, improves generalization.  \n",
        "\n",
        "   - **FocalLoss**  \n",
        "     - Reweights cross-entropy by `(1 - pt)^Î³` where `pt` = predicted prob.  \n",
        "     - Down-weights easy examples, focuses learning on harder/misclassified ones.  \n",
        "     - `Î³=1.5` chosen as a balanced setting.  \n",
        "\n",
        "   - **SmoothedFocal** (composite)  \n",
        "     - Weighted combination:  \n",
        "       - `Î± * LabelSmoothingCE + (1-Î±) * FocalLoss`.  \n",
        "       - If `weight` is provided, incorporates **class weights** inside the focal loss.  \n",
        "     - *Reasoning:* Combines benefits â€” label smoothing for regularization, focal loss for hard-class mining, and class weights for imbalance.  \n",
        "     - `Î±=0.70` biases slightly more toward label smoothing stability.\n",
        "\n",
        "---\n",
        "\n",
        "**Why this design?**\n",
        "- **Accuracy**: clear and interpretable baseline metric.  \n",
        "- **Class Weights**: handle imbalanced FER2013 classes.  \n",
        "- **Label Smoothing**: prevents overfitting and overconfidence.  \n",
        "- **Focal Loss**: emphasizes difficult examples.  \n",
        "- **Smoothed Focal (composite)**: integrates all three principles into one loss, tuned for robust performance on noisy, imbalanced facial expression data.\n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "This cell balances **robustness** (label smoothing), **focus** (focal loss), and **fairness** (class weights), ensuring the model learns generalizable and equitable decision boundaries across all facial expression categories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c35ae68",
      "metadata": {
        "id": "2c35ae68"
      },
      "outputs": [],
      "source": [
        "# === Cell 08: Metrics, class weights, composite loss ===\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "\n",
        "def accuracy(logits, targets): return (logits.argmax(1) == targets).float().mean()\n",
        "\n",
        "def compute_class_weights(df) -> torch.Tensor:\n",
        "    counts = Counter(int(e) for e in df[\"emotion\"].tolist())\n",
        "    total = sum(counts.values())\n",
        "    w = torch.tensor([total / max(1, counts.get(c,1)) for c in range(7)], dtype=torch.float32)\n",
        "    return w / w.mean()\n",
        "\n",
        "CLASS_WEIGHTS = compute_class_weights(train_df)\n",
        "\n",
        "class LabelSmoothingCE(nn.Module):\n",
        "    def __init__(self, eps=0.10, reduction='mean'):\n",
        "        super().__init__(); self.eps=eps; self.reduction=reduction\n",
        "    def forward(self, logits, targets):\n",
        "        n = logits.size(-1); logp = F.log_softmax(logits, dim=-1)\n",
        "        with torch.no_grad():\n",
        "            true = torch.zeros_like(logp).fill_(self.eps/(n-1))\n",
        "            true.scatter_(1, targets.unsqueeze(1), 1.0 - self.eps)\n",
        "        loss = -(true * logp).sum(dim=1)\n",
        "        return loss.mean() if self.reduction=='mean' else loss\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=1.5, reduction='mean'):\n",
        "        super().__init__(); self.g=gamma; self.reduction=reduction\n",
        "    def forward(self, logits, targets):\n",
        "        ce = F.cross_entropy(logits, targets, reduction='none')\n",
        "        pt = torch.exp(-ce)\n",
        "        fl = (1-pt).pow(self.g) * ce\n",
        "        return fl.mean() if self.reduction=='mean' else fl\n",
        "\n",
        "class SmoothedFocal(nn.Module):\n",
        "    def __init__(self, eps=0.10, gamma=1.5, alpha=0.70, weight=None):\n",
        "        super().__init__(); self.a=alpha; self.w=weight\n",
        "        self.lsce = LabelSmoothingCE(eps); self.focal = FocalLoss(gamma)\n",
        "    def forward(self, logits, targets):\n",
        "        if self.w is not None:\n",
        "            ce = F.cross_entropy(logits, targets, reduction='none', weight=self.w.to(logits.device))\n",
        "            pt = torch.exp(-ce); fl = (1-pt).pow(1.5) * ce\n",
        "            ls = self.lsce(logits, targets)\n",
        "            return self.a*ls + (1-self.a)*fl.mean()\n",
        "        return self.a*self.lsce(logits, targets) + (1-self.a)*self.focal(logits, targets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54368702",
      "metadata": {
        "id": "54368702"
      },
      "source": [
        "#Cell 09 â€” MixUp / CutMix and mixed criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9111c1b",
      "metadata": {
        "id": "a9111c1b"
      },
      "source": [
        "### Cell 09 â€” MixUp / CutMix and Mixed Criterion\n",
        "\n",
        "**Logic:**\n",
        "This cell implements two popular *sample-level augmentation* methods, **MixUp** and **CutMix**, plus a unified loss function (`mixed_criterion`) that can handle mixed targets.\n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "1. **MixUp (`mixup_data`)**\n",
        "   - Blends two random images and their labels with ratio `Î» ~ Beta(Î±, Î±)`.\n",
        "   - Formula:  \n",
        "     - `x_mix = Î»*x_i + (1-Î»)*x_j`  \n",
        "     - `y_mix = (y_i, y_j, Î»)`  \n",
        "   - *Reasoning:* Encourages linear behavior in between samples, smooths decision boundaries, and reduces overfitting.\n",
        "\n",
        "2. **CutMix (`cutmix_data`)**\n",
        "   - Randomly cuts a patch from one image and pastes it into another.  \n",
        "   - Adjusts labels proportionally to the area kept/replaced.  \n",
        "   - `Î»` is recomputed as the effective ratio of preserved pixels.  \n",
        "   - *Reasoning:* Preserves local structures (unlike MixUpâ€™s pixel averaging), improves localization robustness, and combats over-reliance on small features.\n",
        "\n",
        "3. **Mixed Criterion (`mixed_criterion`)**\n",
        "   - Generalizes loss computation for mixed targets:  \n",
        "     - If targets are `(y_a, y_b, Î»)`, computes weighted sum:  \n",
        "       `Î» * loss(y_a) + (1-Î») * loss(y_b)`.  \n",
        "     - If no mix, reduces to standard criterion.  \n",
        "   - *Reasoning:* Ensures compatibility between MixUp/CutMix outputs and arbitrary loss functions (e.g., SmoothedFocal).\n",
        "\n",
        "---\n",
        "\n",
        "**Why This Design?**\n",
        "- **MixUp**: Regularizes by enforcing convex combinations of inputs â†’ smoother decision boundaries.  \n",
        "- **CutMix**: Stronger augmentation that preserves semantic patches, improving robustness to occlusions and positional variance.  \n",
        "- **Shared Criterion**: Simplifies training loop by unifying the loss handling for both augmented and non-augmented batches.  \n",
        "- **Alpha Parameter**: Controls augmentation strength â€” higher Î± increases mixing variability.\n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "MixUp and CutMix both force the model to **share representation across samples**, reducing overconfidence, improving calibration, and boosting robustness to noisy labels and occlusions. This cell provides a **plug-and-play augmentation-loss integration**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aa56ef2",
      "metadata": {
        "id": "9aa56ef2"
      },
      "outputs": [],
      "source": [
        "# === Cell 09: MixUp / CutMix and mixed criterion ===\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    if alpha <= 0.0: return x, y, 1.0, None\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    idx = torch.randperm(x.size(0), device=x.device)\n",
        "    return lam*x + (1-lam)*x[idx], (y, y[idx]), lam, idx\n",
        "\n",
        "def cutmix_data(x, y, alpha=1.0, min_lam=0.3, max_lam=0.7):\n",
        "    if alpha <= 0.0: return x, y, 1.0, None\n",
        "    lam = float(np.clip(np.random.beta(alpha, alpha), min_lam, max_lam))\n",
        "    B,C,H,W = x.size(); idx = torch.randperm(B, device=x.device)\n",
        "    cut_w = int(W * math.sqrt(1 - lam)); cut_h = int(H * math.sqrt(1 - lam))\n",
        "    cx, cy = np.random.randint(W), np.random.randint(H)\n",
        "    x1, x2 = np.clip(cx - cut_w//2, 0, W), np.clip(cx + cut_w//2, 0, W)\n",
        "    y1, y2 = np.clip(cy - cut_h//2, 0, H), np.clip(cy + cut_h//2, 0, H)\n",
        "    x[:, :, y1:y2, x1:x2] = x[idx, :, y1:y2, x1:x2]\n",
        "    lam = 1 - ((x2-x1)*(y2-y1) / (W*H + 1e-9))\n",
        "    return x, (y, y[idx]), lam, idx\n",
        "\n",
        "def mixed_criterion(criterion, logits, targets_mix, lam):\n",
        "    if isinstance(targets_mix, tuple):\n",
        "        y_a, y_b = targets_mix\n",
        "        return lam * criterion(logits, y_a) + (1-lam) * criterion(logits, y_b)\n",
        "    return criterion(logits, targets_mix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b0a3aba",
      "metadata": {
        "id": "3b0a3aba"
      },
      "source": [
        "#Cell 10 â€” EMA (exponential moving average)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "addd1010",
      "metadata": {
        "id": "addd1010"
      },
      "source": [
        "### Cell 10 â€” Exponential Moving Average (EMA)\n",
        "\n",
        "**Logic:**\n",
        "Implements an **Exponential Moving Average (EMA)** wrapper for model parameters. EMA maintains a *smoothed copy* of model weights that often generalizes better than the raw, noisy weights trained via SGD/Adam.\n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "1. **Initialization (`__init__`)**\n",
        "   - Takes a model and a decay factor (default `0.999`).  \n",
        "   - Creates a `shadow` dictionary storing clones of each trainable parameter.  \n",
        "   - *Reasoning:* Provides a slow-moving copy of weights that can track long-term stability.\n",
        "\n",
        "2. **Update (`update`)**\n",
        "   - After each training step, updates the shadow weights:  \n",
        "     - `shadow = decay * shadow + (1 - decay) * current_param`  \n",
        "   - *Reasoning:* Smooths fast-changing weights, reducing the impact of noisy gradient steps.\n",
        "\n",
        "3. **Apply Shadow (`apply_shadow`)**\n",
        "   - Temporarily replaces model parameters with their EMA (shadow) versions.  \n",
        "   - Backs up current parameters in `backup`.  \n",
        "   - *Reasoning:* Enables evaluation or checkpointing with stable EMA weights.\n",
        "\n",
        "4. **Restore (`restore`)**\n",
        "   - Restores the modelâ€™s original parameters from backup.  \n",
        "   - Clears backup dictionary.  \n",
        "   - *Reasoning:* Allows training to continue with unaltered model weights after EMA evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "**Why This Design?**\n",
        "- **Stability:** EMA parameters are less sensitive to mini-batch noise.  \n",
        "- **Generalization:** Often improves validation/test accuracy because EMA acts like a low-pass filter over updates.  \n",
        "- **Flexibility:** Separation of `apply_shadow` and `restore` makes it easy to switch between EMA and raw parameters during evaluation and training.  \n",
        "- **Decay Factor (`0.999`):** High decay â†’ slow updates, longer memory; low decay â†’ tracks current weights more closely.  \n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "EMA acts as a **teacher model inside training** â€” it aggregates past versions of the student (raw model), leading to smoother and more generalizable checkpoints, especially in noisy or augmentation-heavy training regimes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40fbb1f9",
      "metadata": {
        "id": "40fbb1f9"
      },
      "outputs": [],
      "source": [
        "# === Cell 10: EMA ===\n",
        "import torch.nn as nn\n",
        "\n",
        "class EMA:\n",
        "    def __init__(self, model: nn.Module, decay: float = 0.999):\n",
        "        self.decay=float(decay); self.shadow={}; self.backup={}\n",
        "        for n,p in model.named_parameters():\n",
        "            if p.requires_grad: self.shadow[n]=p.data.clone()\n",
        "    def update(self, model):\n",
        "        for n,p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.shadow[n]=(1-self.decay)*p.data + self.decay*self.shadow[n]\n",
        "    def apply_shadow(self, model):\n",
        "        self.backup={}\n",
        "        for n,p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.backup[n]=p.data.clone(); p.data=self.shadow[n].clone()\n",
        "    def restore(self, model):\n",
        "        for n,p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                p.data=self.backup[n].clone()\n",
        "        self.backup={}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "718332c2",
      "metadata": {
        "id": "718332c2"
      },
      "source": [
        "### Cell 11 â€” CBAM (Convolutional Block Attention Module) + Sobel Stem\n",
        "\n",
        "**Logic:**\n",
        "This cell defines two architectural components:\n",
        "1. **CBAM**: A lightweight attention module that improves feature learning by sequentially applying **channel** and **spatial** attention.\n",
        "2. **SobelLayer**: A handcrafted edge-detection stem that enhances the networkâ€™s ability to capture facial contours.\n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "#### 1. CBAM\n",
        "- **Channel Attention (CA)**\n",
        "  - Pools features using both average and max pooling (`adaptive_avg_pool2d`, `adaptive_max_pool2d`).\n",
        "  - Passes pooled features through a small MLP (Conv â†’ ReLU â†’ Conv).\n",
        "  - Outputs attention weights via `Sigmoid`.\n",
        "  - *Reasoning:* Helps the model learn **which feature channels are most important** for expression recognition (e.g., mouth edges vs. forehead textures).\n",
        "\n",
        "- **Spatial Attention (SA)**\n",
        "  - Concatenates mean and max across the channel dimension â†’ `[B,2,H,W]`.\n",
        "  - Passes through a 7Ã—7 Conv + Sigmoid to produce a spatial mask.\n",
        "  - *Reasoning:* Highlights **where** in the image important features lie (eyes, mouth corners).\n",
        "\n",
        "- **Forward Pass**\n",
        "  - Multiplies input by CA weights, then by SA mask.\n",
        "  - *Reasoning:* Enhances discriminative features while suppressing irrelevant background.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Sobel Layer\n",
        "- **Definition**\n",
        "  - Registers horizontal (kx) and vertical (ky) Sobel kernels (edge detectors).\n",
        "  - Applies them via convolution to input grayscale image `[B,1,H,W]`.\n",
        "- **Output**\n",
        "  - Original grayscale channel + horizontal edges + vertical edges â†’ `[B,3,H,W]`.\n",
        "- *Reasoning:* Forces the network to explicitly learn from **edges/gradients**, which are critical for detecting facial muscle movements in expression recognition.\n",
        "\n",
        "---\n",
        "\n",
        "**Why This Design?**\n",
        "- **CBAM**: Provides *dynamic attention* with minimal overhead; improves robustness to irrelevant noise by focusing on salient regions/channels.\n",
        "- **Sobel Stem**: Hardcodes useful inductive bias (edges) into the networkâ€™s first layer, accelerating convergence and improving low-level feature learning.\n",
        "- **Combined Effect:** CBAM refines learned deep features, while Sobel strengthens shallow edge cues â†’ together they enhance performance on FER tasks.\n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "- **CBAM** = learnable *what/where to focus*.  \n",
        "- **Sobel** = fixed *edge prior*.  \n",
        "Together, they form a strong, efficient input stem for facial expression recognition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4548e23b",
      "metadata": {
        "id": "4548e23b"
      },
      "outputs": [],
      "source": [
        "# === Cell 11: CBAM + Sobel stem ===\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, ch, r=8):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Conv2d(ch, max(1,ch//r), 1, bias=True), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(max(1,ch//r), ch, 1, bias=True)\n",
        "        )\n",
        "        self.spatial = nn.Sequential(nn.Conv2d(2,1,kernel_size=7,padding=3,bias=False), nn.Sigmoid())\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        ca = self.sigmoid(self.mlp(F.adaptive_avg_pool2d(x,1) + F.adaptive_max_pool2d(x,1)))\n",
        "        x = x * ca\n",
        "        ms = torch.cat([x.mean(1,keepdim=True), x.max(1,keepdim=True)[0]], dim=1)\n",
        "        return x * self.spatial(ms)\n",
        "\n",
        "class SobelLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        kx = torch.tensor([[1,0,-1],[2,0,-2],[1,0,-1]], dtype=torch.float32)\n",
        "        ky = torch.tensor([[1,2,1],[0,0,0],[-1,-2,-1]], dtype=torch.float32)\n",
        "        w  = torch.stack([kx, ky]).unsqueeze(1)   # (2,1,3,3)\n",
        "        self.register_buffer('w', w)\n",
        "    def forward(self, x):                          # x: [B,1,H,W]\n",
        "        edges = F.conv2d(x, self.w, padding=1)     # [B,2,H,W]\n",
        "        return torch.cat([x, edges], dim=1)        # [B,3,H,W]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b3be854",
      "metadata": {
        "id": "8b3be854"
      },
      "source": [
        "#=== Cell 12a: Blocks (Sobel, CBAM) ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34ac5e4b",
      "metadata": {
        "id": "34ac5e4b"
      },
      "outputs": [],
      "source": [
        "# === Cell 12a: Blocks (Sobel, CBAM) ===\n",
        "# Purpose: tiny, dependencyâ€‘free â€œheadâ€ modules shared by all backbones.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Sobel stem: take 1â€‘ch input and expand to 3â€‘ch using gradients.\n",
        "class SobelLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        kx = torch.tensor([[1,0,-1],[2,0,-2],[1,0,-1]], dtype=torch.float32)\n",
        "        ky = kx.t()\n",
        "        self.register_buffer(\"wx\", kx.view(1,1,3,3))\n",
        "        self.register_buffer(\"wy\", ky.view(1,1,3,3))\n",
        "\n",
        "    def forward(self, x1):  # x1: [B,1,H,W] in [-1,1]\n",
        "        gx = F.conv2d(x1, self.wx, padding=1)\n",
        "        gy = F.conv2d(x1, self.wy, padding=1)\n",
        "        mag = torch.sqrt(gx*gx + gy*gy + 1e-6)\n",
        "        return torch.cat([x1, gx, mag], dim=1)  # â†’ [B,3,H,W]\n",
        "\n",
        "# --- CBAM: lightweight attention (channel + spatial).\n",
        "class ChannelGate(nn.Module):\n",
        "    def __init__(self, ch, r=16):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(ch, max(8, ch//r)), nn.ReLU(inplace=True),\n",
        "            nn.Linear(max(8, ch//r), ch)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.shape\n",
        "        avg = F.adaptive_avg_pool2d(x, 1).view(b, c)\n",
        "        maxp= F.adaptive_max_pool2d(x, 1).view(b, c)\n",
        "        att = torch.sigmoid(self.mlp(avg) + self.mlp(maxp)).view(b,c,1,1)\n",
        "        return x * att\n",
        "\n",
        "class SpatialGate(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3)\n",
        "    def forward(self, x):\n",
        "        att = torch.sigmoid(self.conv(torch.cat([x.mean(1, True), x.max(1, True).values], dim=1)))\n",
        "        return x * att\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, ch):\n",
        "        super().__init__()\n",
        "        self.c = ChannelGate(ch)\n",
        "        self.s = SpatialGate()\n",
        "    def forward(self, x): return self.s(self.c(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59b6cc56",
      "metadata": {
        "id": "59b6cc56"
      },
      "source": [
        "### Cell 12 â€” HybridEffNet (EfficientNet-B0 + CBAM + Sobel)\n",
        "\n",
        "**Logic:**\n",
        "This cell defines a custom model architecture `HybridEffNet`, which integrates:\n",
        "- **EfficientNet-B0** backbone (strong general-purpose CNN).  \n",
        "- **SobelLayer** stem (explicit edge priors).  \n",
        "- **CBAM** attention (channel + spatial focus).  \n",
        "- **Custom classifier head** tuned for FER2013â€™s 7 emotion classes.  \n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "1. **Sobel Stem**\n",
        "   - Input: `[B,1,H,W]` grayscale image normalized to `[-1,1]`.  \n",
        "   - Sobel transforms â†’ `[B,3,H,W]` (original + horizontal + vertical edges).  \n",
        "   - *Reasoning:* Provides explicit edge features that highlight facial muscle contours.\n",
        "\n",
        "2. **EfficientNet-B0 Features**\n",
        "   - Pretrained `efficientnet_b0` feature extractor (`.features` block).  \n",
        "   - Processes 3-channel input to output `[B,1280,h,w]`.  \n",
        "   - *Reasoning:* EfficientNet is a well-scaled architecture with strong accuracy/FLOPs trade-off.\n",
        "\n",
        "3. **CBAM Attention (optional)**\n",
        "   - Applied on `[B,1280,h,w]` feature maps.  \n",
        "   - Learns **what channels to emphasize** and **where spatially to focus**.  \n",
        "   - *Reasoning:* FER requires sensitivity to subtle regions (eyes, mouth); CBAM improves discriminative power.\n",
        "\n",
        "4. **Pooling & Classifier**\n",
        "   - `AdaptiveAvgPool2d(1)` â†’ global pooling to `[B,1280]`.  \n",
        "   - BatchNorm + Dropout (0.3) for regularization.  \n",
        "   - Linear head â†’ `[B,7]` logits.  \n",
        "   - *Reasoning:* Simple, effective classifier design that reduces overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "**Why This Design?**\n",
        "- **Sobel + EfficientNet synergy:** Combines handcrafted low-level edges with pretrained deep features.  \n",
        "- **CBAM:** Adds lightweight attention to refine features without large FLOP overhead.  \n",
        "- **Dropout + BatchNorm:** Controls overfitting, stabilizes training.  \n",
        "- **Pretrained EfficientNet:** Leverages strong ImageNet initialization, improving convergence speed and final accuracy.  \n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "HybridEffNet blends **inductive priors (Sobel edges)** with **modern CNN efficiency (EfficientNet-B0)** and **attention refinement (CBAM)** to create a robust backbone tailored for **facial expression recognition**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6a13161",
      "metadata": {
        "id": "a6a13161"
      },
      "source": [
        "#=== Cell 12b: Backbones (EffNetâ€‘B0, MobileNetV3â€‘Small, GhostNetV2) ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7efc18e5",
      "metadata": {
        "id": "7efc18e5"
      },
      "outputs": [],
      "source": [
        "# === Cell 12b: Backbones (EffNetâ€‘B0, MobileNetV3â€‘Small, GhostNetV2) ===\n",
        "# Purpose: define 3 independent model classes with the same forward signature.\n",
        "# All three use Sobel(1â†’3) + backbone.features + (optional) CBAM + GAP + BN + Dropout + Linear.\n",
        "\n",
        "# Install timm once for GhostNet (noâ€‘op if already installed)\n",
        "try:\n",
        "    import timm\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"timm\"])\n",
        "    import timm\n",
        "\n",
        "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
        "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
        "\n",
        "CLASSIFIER_DROPOUT = 0.30\n",
        "USE_CBAM = True\n",
        "NUM_CLASSES = 7\n",
        "\n",
        "# ----- 1) EfficientNetâ€‘B0 (matches the PDF lineage)\n",
        "class FER_EffNetB0_CBAM(nn.Module):\n",
        "    def __init__(self, num_classes=NUM_CLASSES, classifier_dropout=CLASSIFIER_DROPOUT, use_cbam=USE_CBAM):\n",
        "        super().__init__()\n",
        "        base = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
        "        self.sobel    = SobelLayer()             # 1â†’3\n",
        "        self.features = base.features            # â†’ [B,1280,h,w]\n",
        "        self.cbam     = CBAM(1280) if use_cbam else None\n",
        "        self.pool     = nn.AdaptiveAvgPool2d(1)\n",
        "        self.bn       = nn.BatchNorm1d(1280)\n",
        "        self.drop     = nn.Dropout(p=classifier_dropout)\n",
        "        self.head     = nn.Linear(1280, num_classes)\n",
        "\n",
        "    def forward(self, x1):\n",
        "        x3 = self.sobel(x1)\n",
        "        f  = self.features(x3)\n",
        "        if self.cbam is not None: f = self.cbam(f)\n",
        "        f  = self.pool(f).flatten(1)\n",
        "        f  = self.bn(f); f = self.drop(f)\n",
        "        return self.head(f)\n",
        "\n",
        "# ----- 2) MobileNetV3â€‘Small\n",
        "class FER_MobileNetV3S_CBAM(nn.Module):\n",
        "    def __init__(self, num_classes=NUM_CLASSES, classifier_dropout=CLASSIFIER_DROPOUT, use_cbam=USE_CBAM):\n",
        "        super().__init__()\n",
        "        base = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT)\n",
        "        self.sobel    = SobelLayer()\n",
        "        self.features = base.features            # last channels = 576\n",
        "        last_ch = 576\n",
        "        self.cbam     = CBAM(last_ch) if use_cbam else None\n",
        "        self.pool     = nn.AdaptiveAvgPool2d(1)\n",
        "        self.bn       = nn.BatchNorm1d(last_ch)\n",
        "        self.drop     = nn.Dropout(p=classifier_dropout)\n",
        "        self.head     = nn.Linear(last_ch, num_classes)\n",
        "\n",
        "    def forward(self, x1):\n",
        "        x3 = self.sobel(x1)\n",
        "        f  = self.features(x3)\n",
        "        if self.cbam is not None: f = self.cbam(f)\n",
        "        f  = self.pool(f).flatten(1)\n",
        "        f  = self.bn(f); f = self.drop(f)\n",
        "        return self.head(f)\n",
        "\n",
        "# ----- 3) GhostNetV2â€‘100 (timm). If you prefer v1: 'ghostnet_100' & adjust channels.\n",
        "class FER_GhostNetV2_CBAM(nn.Module):\n",
        "    def __init__(self, num_classes=NUM_CLASSES, classifier_dropout=CLASSIFIER_DROPOUT, use_cbam=USE_CBAM):\n",
        "        super().__init__()\n",
        "        # num_classes=0 & global_pool='' â†’ raw feature map; we add our own head for consistency.\n",
        "        base = timm.create_model('ghostnetv2_100', pretrained=True, in_chans=3, num_classes=0, global_pool='')\n",
        "        self.sobel    = SobelLayer()\n",
        "        self.features = base                     # returns [B,C,h,w]\n",
        "        # Determine last channel count robustly with a tiny dry run\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1,1,int(CONFIG[\"IMG_SIZE\"]),int(CONFIG[\"IMG_SIZE\"]))\n",
        "            ch = self.features(self.sobel(dummy)).shape[1]\n",
        "        self.cbam     = CBAM(ch) if use_cbam else None\n",
        "        self.pool     = nn.AdaptiveAvgPool2d(1)\n",
        "        self.bn       = nn.BatchNorm1d(ch)\n",
        "        self.drop     = nn.Dropout(p=classifier_dropout)\n",
        "        self.head     = nn.Linear(ch, num_classes)\n",
        "\n",
        "    def forward(self, x1):\n",
        "        x3 = self.sobel(x1)\n",
        "        f  = self.features(x3)\n",
        "        if self.cbam is not None: f = self.cbam(f)\n",
        "        f  = self.pool(f).flatten(1)\n",
        "        f  = self.bn(f); f = self.drop(f)\n",
        "        return self.head(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "051e97c2",
      "metadata": {
        "id": "051e97c2"
      },
      "source": [
        "#=== Cell 13: Builders + run list (comment to skip) ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2f4b89f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2f4b89f",
        "outputId": "2bca0034-7a2f-45f7-aa77-5a1f3bd3b14a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Build] Planning to run: ['effnet_b0', 'mobilenet_v3', 'ghostnet_v2']\n",
            "  effnet_b0    â†’ /content/project/checkpoints/best_effnet_b0_fer.pth\n",
            "  mobilenet_v3 â†’ /content/project/checkpoints/best_mobilenet_v3_fer.pth\n",
            "  ghostnet_v2  â†’ /content/project/checkpoints/best_ghostnet_v2_fer.pth\n"
          ]
        }
      ],
      "source": [
        "# === Cell 13: Builders + unique checkpoint names ===\n",
        "# Purpose: single entry point to instantiate any backbone; define which models to run.\n",
        "\n",
        "def build_model(backbone_tag: str) -> nn.Module:\n",
        "    tag = backbone_tag.lower()\n",
        "    if tag == \"effnet_b0\":\n",
        "        m = FER_EffNetB0_CBAM(NUM_CLASSES)\n",
        "    elif tag == \"mobilenet_v3\":\n",
        "        m = FER_MobileNetV3S_CBAM(NUM_CLASSES)\n",
        "    elif tag == \"ghostnet_v2\":\n",
        "        m = FER_GhostNetV2_CBAM(NUM_CLASSES)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown backbone: {backbone_tag}\")\n",
        "    return m.to(device)\n",
        "\n",
        "# === Toggle here: comment out any model you don't want to run ===\n",
        "MODELS_TO_RUN = [\n",
        "    \"effnet_b0\",\n",
        "    \"mobilenet_v3\",\n",
        "    \"ghostnet_v2\",\n",
        "]\n",
        "\n",
        "print(\"[Build] Planning to run:\", MODELS_TO_RUN)\n",
        "for tag in MODELS_TO_RUN:\n",
        "    print(f\"  {tag:12s} â†’ {ckpt_path_for(tag)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf0d8191",
      "metadata": {
        "id": "bf0d8191"
      },
      "source": [
        "#Cell 13 â€” Optimizer, scheduler, early stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9a47ae2",
      "metadata": {
        "id": "d9a47ae2"
      },
      "source": [
        "### Cell 13 â€” Optimizer, Scheduler Helper, and EarlyStopping\n",
        "\n",
        "**Logic:**\n",
        "This cell provides utility components for optimization and training stability:\n",
        "1. **Optimizer Factory (`make_optimizer`)**  \n",
        "2. **Warmup + Cosine Scheduler (back-compat helper)**  \n",
        "3. **EarlyStopping mechanism**  \n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "1. **Optimizer Factory (`make_optimizer`)**\n",
        "   - Chooses optimizer based on `CONFIG[\"USE_SGD\"]`:  \n",
        "     - **AdamW** (default, and the optimizer we are using): Adaptive learning rates with decoupled weight decay.  \n",
        "     - **SGD + Nesterov** (optional): Momentum-driven updates, less memory usage.  \n",
        "   - Uses hyperparameters from `HP`:  \n",
        "     - `LR`, `WD`, `SGD_MOMENTUM`, `SGD_NESTEROV`.  \n",
        "   - *Reasoning:* AdamW is chosen because it provides more stable convergence for this task, handles sparse gradients effectively, and is less sensitive to manual learning rate tuning than SGD.\n",
        "\n",
        "2. **Warmup + Cosine Annealing Scheduler (`WarmupCosine`)**\n",
        "   - Manually implements cosine learning rate schedule with linear warmup:  \n",
        "     - **Warmup phase:** LR rises linearly from `lr_min` to `lr_max` over `warmup_epochs`.  \n",
        "     - **Cosine decay:** LR decreases smoothly from `lr_max` to `lr_min` until `max_epochs`.  \n",
        "   - Each call to `.step()` updates the LR and increments the epoch counter.  \n",
        "   - *Reasoning:* Cosine decay avoids abrupt LR drops, improving convergence stability; warmup prevents divergence in early epochs.\n",
        "\n",
        "   *(Note: In Cell 02 you already have `build_scheduler()`. This class provides backwards compatibility or fine control if needed.)*\n",
        "\n",
        "3. **EarlyStopping**\n",
        "   - Monitors validation loss (`val_loss`).  \n",
        "   - Stops training if no significant improvement (`min_delta`) for `patience` consecutive epochs.  \n",
        "   - Keeps track of best loss so far (`self.best`) and count of bad epochs (`self.bad`).  \n",
        "   - Returns `True` if patience is exceeded â†’ signal to stop training.  \n",
        "   - *Reasoning:* Prevents unnecessary training once the model plateaus or starts overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "**Why This Design?**\n",
        "- **AdamW is the chosen optimizer** because it provides robust convergence and reduces the need for extensive manual hyperparameter tuning.  \n",
        "- **Modularity:** Keeps optimizer/scheduler creation separate from training loop.  \n",
        "- **Flexibility:** Optionally allows SGD, though AdamW is preferred for this FER pipeline.  \n",
        "- **Stability:** EarlyStopping provides a safeguard against wasted compute and overfitting.  \n",
        "- **Best Practices:** Warmup + cosine and early stopping are widely used in modern training pipelines for their balance of performance and stability.\n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "This cell acts as the *training control hub* â€” selecting **AdamW as the optimizer**, managing learning rate schedules, and preventing overtraining through early stopping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4c368f5",
      "metadata": {
        "id": "a4c368f5"
      },
      "outputs": [],
      "source": [
        "# === Cell 13: Optimizer + Scheduler + EarlyStopping (updated) ===\n",
        "import math\n",
        "import torch\n",
        "\n",
        "def make_optimizer(params, hp, cfg):\n",
        "    \"\"\"\n",
        "    AdamW (default) or SGD+Nesterov via CONFIG['USE_SGD'] toggle.\n",
        "    Uses HP['LR'] and HP['WD'].\n",
        "    \"\"\"\n",
        "    if cfg.get(\"USE_SGD\", False):\n",
        "        return torch.optim.SGD(\n",
        "            params,\n",
        "            lr=hp[\"LR\"],\n",
        "            momentum=hp[\"SGD_MOMENTUM\"],\n",
        "            nesterov=hp[\"SGD_NESTEROV\"],\n",
        "            weight_decay=hp[\"WD\"],\n",
        "        )\n",
        "    return torch.optim.AdamW(params, lr=hp[\"LR\"], weight_decay=hp[\"WD\"])\n",
        "\n",
        "# Backâ€‘compat cosine helper (epoch-level stepping)\n",
        "class WarmupCosine:\n",
        "    def __init__(self, opt, warmup_epochs, max_epochs, lr_min=1e-6, lr_max=None):\n",
        "        self.opt = opt\n",
        "        self.warmup = max(1, int(warmup_epochs))\n",
        "        self.maxe = int(max_epochs)\n",
        "        self.t = 0\n",
        "        if lr_max is None:\n",
        "            lr_max = max(pg['lr'] for pg in opt.param_groups)\n",
        "        self.lr_min, self.lr_max = lr_min, lr_max\n",
        "    def step(self):\n",
        "        self.t += 1\n",
        "        if self.t <= self.warmup:\n",
        "            lr = self.lr_min + (self.lr_max - self.lr_min) * (self.t / self.warmup)\n",
        "        else:\n",
        "            tt = (self.t - self.warmup) / max(1, (self.maxe - self.warmup))\n",
        "            lr = self.lr_min + 0.5 * (self.lr_max - self.lr_min) * (1 + math.cos(math.pi * tt))\n",
        "        for g in self.opt.param_groups: g['lr'] = lr\n",
        "        return lr\n",
        "\n",
        "# NOTE:\n",
        "# In Cell 02 you already have build_scheduler(...) and helpers like\n",
        "#   - scheduler_steps_per_batch()\n",
        "#   - current_lr()\n",
        "# We simply keep EarlyStopping here and let Cell 14 call build_scheduler.\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stop on validation loss. Call .step(val_loss) each epoch.\"\"\"\n",
        "    def __init__(self, patience=8, min_delta=1e-4):\n",
        "        self.patience = int(patience)\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.best = float('inf')\n",
        "        self.bad = 0\n",
        "    def step(self, val_loss: float) -> bool:\n",
        "        if val_loss < self.best - self.min_delta:\n",
        "            self.best = val_loss\n",
        "            self.bad = 0\n",
        "            return False\n",
        "        self.bad += 1\n",
        "        return self.bad >= self.patience\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "821a02bd",
      "metadata": {
        "id": "821a02bd"
      },
      "source": [
        "### Cell 14 â€” Training Loop (Plateau uses val_loss; OneCycle steps per batch)\n",
        "\n",
        "**Logic:**\n",
        "This cell implements the full training loop with support for multiple schedulers (Plateau, Cosine, OneCycle), advanced augmentation scheduling, label smoothing, MixUp/CutMix tapering, fine-tuning, EMA tracking, and early stopping.\n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "1. **Optimizer & Scheduler**\n",
        "   - Uses `make_optimizer` (from Cell 13) â†’ AdamW (chosen optimizer).\n",
        "   - Scheduler built from Cell 02 (`build_scheduler`):\n",
        "     - **OneCycle**: steps per batch.  \n",
        "     - **Cosine/Plateau**: steps per epoch (Plateau keyed to `val_loss`).  \n",
        "   - *Reasoning:* Different schedulers provide flexibility: OneCycle accelerates convergence, Cosine smooths decay, Plateau reacts adaptively to stagnation.\n",
        "\n",
        "2. **Stability Helpers**\n",
        "   - **EarlyStopping** (Cell 13): halts training if no validation loss improvement for `patience`.  \n",
        "   - **EMA**: Tracks smoothed parameter averages for more stable evaluation.  \n",
        "   - **AMP + Gradient Clipping**: Reduces memory/compute cost (fp16) while avoiding exploding gradients.  \n",
        "\n",
        "3. **Augmentation Scheduling**\n",
        "   - `FER_AUG_FACTORY(s)`: strength `s` increases over ramp-up epochs, capped later (`AUG_CAP_LATE`).  \n",
        "   - *Reasoning:* Start with lighter augmentations for stability â†’ stronger ones later to fight overfitting.\n",
        "\n",
        "4. **MixUp/CutMix Tapering**\n",
        "   - Probabilities (`p_mix`, `p_cut`) decay from base values to zero between `TAPER_START_FRAC` and `TAPER_END_FRAC`.  \n",
        "   - Disabled entirely in fine-tune tail.  \n",
        "   - *Reasoning:* Regularize strongly early, then allow the network to focus on clean data near convergence.\n",
        "\n",
        "5. **Fine-tune Tail**\n",
        "   - Final fraction of epochs (`FINE_TUNE_FRACTION`) runs **without augmentation/mix** at lower LR.  \n",
        "   - *Reasoning:* Acts as a â€œclean finish,â€ letting the model consolidate on unaugmented signals.\n",
        "\n",
        "6. **Label Smoothing Schedule**\n",
        "   - Epsilon decays from `LABEL_SMOOTH_START` to `LABEL_SMOOTH_END` across training.  \n",
        "   - *Reasoning:* Stronger smoothing early prevents overconfidence; weaker smoothing later preserves precision.\n",
        "\n",
        "7. **Training Step**\n",
        "   - Normalization: map images to `[-1,1]`.  \n",
        "   - Augmentation â†’ MixUp or CutMix (probabilistic).  \n",
        "   - Loss:  \n",
        "     - Main = Label Smoothed CE (with MixUp/CutMix if active).  \n",
        "     - Weighted blend with standard CE using class weights (25%).  \n",
        "   - Backprop: AMP scaling, gradient clipping, optimizer + scheduler step.  \n",
        "   - EMA update at each step.\n",
        "\n",
        "8. **Validation Step**\n",
        "   - Clean evaluation (no aug, no mix).  \n",
        "   - Reports mean val loss + accuracy.  \n",
        "   - Used for checkpointing (best `val_loss`).\n",
        "\n",
        "9. **Logging + Checkpointing**\n",
        "   - Logs metrics each epoch: train loss, val loss, val acc, LR.  \n",
        "   - Saves model state if `val_loss` improves.  \n",
        "   - Stops early if `EarlyStopping` triggers.\n",
        "\n",
        "---\n",
        "\n",
        "**Why This Design?**\n",
        "- **Comprehensive regularization:** AugMix, MixUp/CutMix, smoothing, and class weights each address different risks (overfitting, imbalance, noisy labels).  \n",
        "- **Dynamic scheduling:** Augmentation/mix taper and label smoothing schedule ensure training starts robust but finishes precise.  \n",
        "- **Stability mechanisms:** EMA, gradient clipping, and AMP provide safety and efficiency.  \n",
        "- **General-purpose flexibility:** One training loop works with AdamW + OneCycle (default) or other scheduler configs.\n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "This loop represents a **modern, production-grade training recipe** â€” combining augmentation diversity, dynamic scheduling, EMA smoothing, and adaptive early stopping. It balances *robustness early in training* with *precision late in training* to achieve strong generalization on FER2013.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#helper cell before 14"
      ],
      "metadata": {
        "id": "Kahj3dd-oGQJ"
      },
      "id": "Kahj3dd-oGQJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# === Safe EMA (skip non-floating buffers) ===\n",
        "class EMA:\n",
        "    \"\"\"\n",
        "    Exponential Moving Average of model parameters/buffers.\n",
        "    - Only updates tensors with floating dtype (float16/32/64, bfloat16).\n",
        "    - Non-floating tensors (e.g., Long buffers like num_batches_tracked) are copied once and left unchanged.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, decay: float = 0.9995, device=None):\n",
        "        self.decay = float(decay)\n",
        "        self.shadow = {}\n",
        "        self.backup = {}\n",
        "\n",
        "        # Create the shadow dict on the same device/dtype as the source tensors\n",
        "        for k, v in model.state_dict().items():\n",
        "            if v.dtype.is_floating_point:\n",
        "                self.shadow[k] = v.detach().clone()\n",
        "            else:\n",
        "                # copy once; we won't EMA-update these\n",
        "                self.shadow[k] = v.detach().clone()\n",
        "\n",
        "        # optional: move shadow to a different device (usually not needed)\n",
        "        if device is not None:\n",
        "            for k in self.shadow:\n",
        "                self.shadow[k] = self.shadow[k].to(device)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, model):\n",
        "        d = self.decay\n",
        "        for k, v in model.state_dict().items():\n",
        "            if not v.dtype.is_floating_point:\n",
        "                # leave non-floating tensors unchanged\n",
        "                self.shadow[k].copy_(v)\n",
        "                continue\n",
        "            # EMA: shadow = d*shadow + (1-d)*v\n",
        "            self.shadow[k].mul_(d).add_(v.detach(), alpha=(1.0 - d))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def apply_shadow(self, model):\n",
        "        # swap in shadow weights (backup originals)\n",
        "        self.backup = {}\n",
        "        for k, v in model.state_dict().items():\n",
        "            self.backup[k] = v.detach().clone()\n",
        "            v.copy_(self.shadow[k])\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def restore(self, model):\n",
        "        # restore originals after evaluation\n",
        "        if not self.backup:\n",
        "            return\n",
        "        for k, v in model.state_dict().items():\n",
        "            v.copy_(self.backup[k])\n",
        "        self.backup = {}\n"
      ],
      "metadata": {
        "id": "QrMxxZtZoJ1Y"
      },
      "id": "QrMxxZtZoJ1Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ac8bd3c",
      "metadata": {
        "id": "4ac8bd3c"
      },
      "outputs": [],
      "source": [
        "# === Cell 14: Training loop (Plateau uses val_loss; OneCycle steps per batch) ===\n",
        "import random, math, numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import inspect\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Compatibility shims so this cell works with your Cellâ€‘02 factories.\n",
        "# ------------------------------------------------------------------\n",
        "def _make_opt(model, hp, cfg):\n",
        "    \"\"\"\n",
        "    Your Cell 02 typically defines: make_optimizer(model)  -> uses global HP/CONFIG.\n",
        "    If an older signature is present, we fall back to: make_optimizer(model.parameters(), hp, cfg).\n",
        "    \"\"\"\n",
        "    sig = inspect.signature(make_optimizer)\n",
        "    if len(sig.parameters) == 1:\n",
        "        return make_optimizer(model)\n",
        "    return make_optimizer(model.parameters(), hp, cfg)\n",
        "\n",
        "def _build_sched(optimizer, steps_per_epoch, hp, cfg):\n",
        "    \"\"\"\n",
        "    Your Cell 02 usually defines: build_scheduler(optimizer, steps_per_epoch=None).\n",
        "    Support older variants that take positional args, too.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return build_scheduler(optimizer, steps_per_epoch=steps_per_epoch)\n",
        "    except TypeError:\n",
        "        return build_scheduler(optimizer, steps_per_epoch, hp, cfg)\n",
        "\n",
        "def _scheduler_steps_per_batch(cfg):\n",
        "    \"\"\"\n",
        "    OneCycle -> step per batch; Cosine/Plateau -> step per epoch.\n",
        "    If you already expose scheduler_steps_per_batch(), we will honor it.\n",
        "    \"\"\"\n",
        "    if 'scheduler_steps_per_batch' in globals():\n",
        "        try:\n",
        "            return bool(scheduler_steps_per_batch())\n",
        "        except Exception:\n",
        "            pass\n",
        "    sched = str(cfg.get(\"SCHEDULER\", CONFIG.get(\"SCHEDULER\", \"onecycle\"))).lower()\n",
        "    return (sched == \"onecycle\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Trainer (uses your Cellâ€‘08 metrics/losses exactly)\n",
        "#   â€¢ main loss: SmoothedFocal(eps=0.10, gamma=1.5, alpha=0.70, weight=CLASS_WEIGHTS)\n",
        "#   â€¢ light CE stabilizer blended in (optional)\n",
        "#   â€¢ AMP, EMA (if EMA class exists), OneCycle/Cosine/Plateau\n",
        "#   â€¢ sparse logging: every PRINT_EVERY, on improved val_acc, and final epoch\n",
        "#   â€¢ checkpoint on best val_loss (unchanged)\n",
        "# ------------------------------------------------------------------\n",
        "def fit_with_aug(model: nn.Module, train_dl, val_dl, hp: dict, cfg: dict):\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # ----- Optimizer & Scheduler -----\n",
        "    opt = _make_opt(model, hp, cfg)\n",
        "    steps_per_epoch = len(train_dl)\n",
        "    sched = _build_sched(opt, steps_per_epoch=steps_per_epoch, hp=hp, cfg=cfg)\n",
        "    use_batch_sched = _scheduler_steps_per_batch(cfg)\n",
        "\n",
        "    # ----- EMA (instantiate only if an EMA class is available) -----\n",
        "    ema_obj = None\n",
        "    if cfg.get(\"USE_EMA\", True) and (\"EMA\" in globals()) and callable(globals()[\"EMA\"]):\n",
        "        try:\n",
        "            ema_obj = EMA(model, decay=float(hp.get(\"EMA_DECAY\", 0.9995)))\n",
        "        except TypeError:\n",
        "            ema_obj = EMA(model)  # fallback if your EMA takes only (model)\n",
        "\n",
        "    # ----- AMP -----\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=bool(cfg.get(\"USE_AMP\", torch.cuda.is_available())))\n",
        "\n",
        "    # ----- Losses (from Cell 08) -----\n",
        "    # main criterion (uses your CLASS_WEIGHTS)\n",
        "    main_crit = SmoothedFocal(eps=0.10, gamma=1.5, alpha=0.70, weight=CLASS_WEIGHTS)\n",
        "    # optional stabilizer CE (class-weighted) used as a light blend below\n",
        "    ce_weight = CLASS_WEIGHTS.to(device=device) if isinstance(CLASS_WEIGHTS, torch.Tensor) else None\n",
        "\n",
        "    # ----- Logging / early stop controls -----\n",
        "    E = int(hp[\"EPOCHS\"])\n",
        "    PRINT_EVERY = int(cfg.get(\"PRINT_EVERY\", 10))\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_val_acc  = -1.0\n",
        "\n",
        "    # Mix/taper schedule & clean fineâ€‘tune tail\n",
        "    ramp_epochs  = ( int(hp[\"AUG_RAMP_EPOCHS\"]) if hp.get(\"AUG_RAMP_EPOCHS\", 0.40) >= 1.0\n",
        "                     else max(1, int(float(hp.get(\"AUG_RAMP_EPOCHS\", 0.40)) * E)) )\n",
        "    t_start      = float(cfg.get(\"TAPER_START_FRAC\", 0.20))\n",
        "    t_end        = float(cfg.get(\"TAPER_END_FRAC\",   0.85))\n",
        "    base_mix_p   = float(cfg.get(\"BASE_MIXUP_PROB\", 0.45))\n",
        "    base_cut_p   = float(cfg.get(\"BASE_CUTMIX_PROB\", 0.25))\n",
        "    tail_frac    = float(cfg.get(\"FINE_TUNE_FRACTION\", 0.12))\n",
        "    tail_start_e = max(1, int((1.0 - tail_frac) * E))\n",
        "\n",
        "    history = []\n",
        "\n",
        "    for epoch in range(1, E + 1):\n",
        "        model.train()\n",
        "        frac = (epoch - 1) / max(1, E - 1)\n",
        "\n",
        "        # ----- Aug strength (optional cap late) -----\n",
        "        if cfg.get(\"USE_AUG\", True):\n",
        "            s = 0.2 + 0.6 * min(1.0, epoch / ramp_epochs)\n",
        "            if cfg.get(\"AUG_CAP_LATE\", True) and epoch >= int(0.7 * E):\n",
        "                s = min(s, 0.6)\n",
        "            augment = globals().get(\"FER_AUG_FACTORY\", lambda _: None)(s)  # noâ€‘op if not defined\n",
        "        else:\n",
        "            augment = None\n",
        "\n",
        "        # ----- MixUp/CutMix taper -----\n",
        "        if frac < t_start:\n",
        "            p_mix, p_cut = base_mix_p, base_cut_p\n",
        "        elif frac > t_end:\n",
        "            p_mix, p_cut = 0.0, 0.0\n",
        "        else:\n",
        "            tf = (frac - t_start) / max(1e-8, (t_end - t_start))\n",
        "            p_mix, p_cut = base_mix_p * (1.0 - tf), base_cut_p * (1.0 - tf)\n",
        "\n",
        "        # ----- Fineâ€‘tune tail (disable aug/mix; reduce LR group-wise) -----\n",
        "        if epoch >= tail_start_e:\n",
        "            augment = None\n",
        "            p_mix = p_cut = 0.0\n",
        "            for g in opt.param_groups:\n",
        "                g['lr'] = max(hp.get(\"LR_MIN\", 1e-6), 0.2 * float(hp[\"LR\"]))\n",
        "\n",
        "        # ---- Train one epoch ----\n",
        "        loss_sum, n_seen = 0.0, 0\n",
        "        for xb, yb in train_dl:\n",
        "            xb = xb.to(device, non_blocking=True)\n",
        "            yb = yb.to(device, non_blocking=True)\n",
        "\n",
        "            # (Inputs already normalized upstream; do NOT re-normalize.)\n",
        "            if augment is not None:\n",
        "                try:\n",
        "                    xb = augment(xb)\n",
        "                except Exception:\n",
        "                    pass  # keep training even if your custom augment is a no-op\n",
        "\n",
        "            # MixUp/CutMix (probabilistic) â€” only if helpers exist\n",
        "            have_mix = \"mixup_data\" in globals() and callable(globals()[\"mixup_data\"])\n",
        "            have_cut = \"cutmix_data\" in globals() and callable(globals()[\"cutmix_data\"])\n",
        "            use_mix = have_mix and cfg.get(\"USE_MIXUP\", True) and (random.random() < p_mix)\n",
        "            use_cut = (not use_mix) and have_cut and cfg.get(\"USE_CUTMIX\", True) and (random.random() < p_cut)\n",
        "\n",
        "            if use_mix:\n",
        "                xb2, (ya, yb2), lam, _ = globals()[\"mixup_data\"](xb, yb, alpha=hp.get(\"MIXUP_ALPHA\", 0.3))\n",
        "                xb, targets_mix, lam = xb2, (ya, yb2), float(lam)\n",
        "            elif use_cut:\n",
        "                xb2, (ya, yb2), lam, _ = globals()[\"cutmix_data\"](xb, yb, alpha=hp.get(\"CUTMIX_ALPHA\", 1.0))\n",
        "                xb, targets_mix, lam = xb2, (ya, yb2), float(lam)\n",
        "            else:\n",
        "                targets_mix, lam = yb, 1.0\n",
        "\n",
        "            with torch.cuda.amp.autocast(enabled=scaler.is_enabled()):\n",
        "                logits = model(xb)\n",
        "\n",
        "                # main loss via your SmoothedFocal\n",
        "                if isinstance(targets_mix, tuple):\n",
        "                    ya_, yb_ = targets_mix\n",
        "                    loss_main = lam * main_crit(logits, ya_) + (1.0 - lam) * main_crit(logits, yb_)\n",
        "                else:\n",
        "                    loss_main = main_crit(logits, targets_mix)\n",
        "\n",
        "                # light CE stabilizer (optional)\n",
        "                if ce_weight is not None:\n",
        "                    loss_aux = F.cross_entropy(logits, yb, weight=ce_weight)\n",
        "                    loss = 0.75 * loss_main + 0.25 * loss_aux\n",
        "                else:\n",
        "                    loss = loss_main\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(opt); scaler.update()\n",
        "            if use_batch_sched:\n",
        "                sched.step()\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            if ema_obj is not None and hasattr(ema_obj, \"update\"):\n",
        "                ema_obj.update(model)  # NOTE: pass `model` (fixes the earlier TypeError)\n",
        "\n",
        "            bs = xb.size(0)\n",
        "            loss_sum += float(loss) * bs\n",
        "            n_seen   += bs\n",
        "\n",
        "        train_loss = loss_sum / max(1, n_seen)\n",
        "\n",
        "        # Epoch-level scheduler step for cosine (Plateau steps after val)\n",
        "        if (not use_batch_sched) and str(cfg.get(\"SCHEDULER\", \"onecycle\")).lower() == \"cosine\":\n",
        "            sched.step()\n",
        "\n",
        "        # ---- Validation (apply EMA shadow if present) ----\n",
        "        model.eval()\n",
        "        if ema_obj is not None and hasattr(ema_obj, \"apply_shadow\"):\n",
        "            ema_obj.apply_shadow(model)\n",
        "\n",
        "        val_loss_sum, val_acc_sum, n_val = 0.0, 0.0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_dl:\n",
        "                xb = xb.to(device, non_blocking=True)\n",
        "                yb = yb.to(device, non_blocking=True)\n",
        "                logits = model(xb)\n",
        "                vloss = (F.cross_entropy(logits, yb, weight=ce_weight)\n",
        "                         if ce_weight is not None else F.cross_entropy(logits, yb))\n",
        "                vacc  = accuracy(logits, yb)  # your Cellâ€‘08 metric\n",
        "                bs = xb.size(0)\n",
        "                val_loss_sum += float(vloss) * bs\n",
        "                val_acc_sum  += float(vacc)  * bs\n",
        "                n_val        += bs\n",
        "\n",
        "        val_loss = val_loss_sum / max(1, n_val)\n",
        "        val_acc  = val_acc_sum  / max(1, n_val)\n",
        "\n",
        "        if ema_obj is not None and hasattr(ema_obj, \"restore\"):\n",
        "            ema_obj.restore(model)\n",
        "\n",
        "        # Plateau reacts to validation loss\n",
        "        if (not use_batch_sched) and str(cfg.get(\"SCHEDULER\", \"onecycle\")).lower() == \"plateau\":\n",
        "            sched.step(val_loss)\n",
        "\n",
        "        # ---- Checkpoint on improved val_loss (unchanged) ----\n",
        "        if val_loss < best_val_loss - 1e-6:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save({\"model_state\": model.state_dict()}, CONFIG[\"SAVE_BEST_PATH\"])\n",
        "\n",
        "        # ---- Track best val_acc for gated logging ----\n",
        "        improved_acc = val_acc > best_val_acc + 1e-9\n",
        "        if improved_acc:\n",
        "            best_val_acc = val_acc\n",
        "\n",
        "        # ---- History (always append) ----\n",
        "        lr_now = float(opt.param_groups[0][\"lr\"])\n",
        "        history.append({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_acc\":  val_acc,\n",
        "            \"best_val_acc\": best_val_acc,\n",
        "            \"lr\": lr_now\n",
        "        })\n",
        "\n",
        "        # ---- Sparse console logging ----\n",
        "        if (epoch % PRINT_EVERY == 0) or (epoch == E) or improved_acc:\n",
        "            print(f\"[{epoch:03d}/{E}] \"\n",
        "                  f\"train={train_loss:.4f}  val={val_loss:.4f}  acc={val_acc:.4f}  \"\n",
        "                  f\"best_acc={best_val_acc:.4f}  lr={lr_now:.2e}\")\n",
        "\n",
        "    return history, ema_obj\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19e00e83",
      "metadata": {
        "id": "19e00e83"
      },
      "source": [
        "#Cell 15 â€” Build model and quick probe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0_nFfLM2-_sZ",
      "metadata": {
        "id": "0_nFfLM2-_sZ"
      },
      "source": [
        "## Baseline Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0598310a",
      "metadata": {
        "id": "0598310a"
      },
      "source": [
        "### Cell (Aug Utils) â€” Localized Erasing\n",
        "\n",
        "**Logic:**\n",
        "This utility defines `localized_erasing`, a random erasure augmentation similar to *Random Erasing* in CV literature. It randomly removes a rectangular patch of the image and fills it with a neutral mid-gray value consistent with the inputâ€™s numeric range.\n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "1. **Fill Value (`_neutral_fill_value`)**\n",
        "   - Chooses mid-gray consistent with the imageâ€™s range:\n",
        "     - `[-1,1]` â†’ `0.0`\n",
        "     - `[0,1]` â†’ `0.5`\n",
        "     - `[0,255]` â†’ `127.5`\n",
        "   - *Reasoning:* Avoids biasing the model with unnatural patches (pure black/white).\n",
        "\n",
        "2. **Patch Size**\n",
        "   - Computes target erase area as a fraction of image area (`min_frac`â€“`max_frac`).  \n",
        "   - Chooses random aspect ratio in `[0.3, 3.3]`.  \n",
        "   - Derives patch height (`eh`) and width (`ew`).  \n",
        "   - *Reasoning:* Varying size/shape of erased regions simulates diverse occlusions.\n",
        "\n",
        "3. **Patch Placement**\n",
        "   - Randomly selects `(top, left)` location within image bounds.  \n",
        "   - Applies fill to rectangular region `[top:top+eh, left:left+ew]`.  \n",
        "   - Skips if patch is degenerate (too small/large).  \n",
        "   - *Reasoning:* Random positioning ensures model cannot rely on specific regions.\n",
        "\n",
        "4. **Return**\n",
        "   - Outputs a clone of input with erased patch.  \n",
        "   - *Reasoning:* Non-destructive to original input, safe for augmentation pipelines.\n",
        "\n",
        "---\n",
        "\n",
        "**Why This Design?**\n",
        "- **Occlusion Robustness:** Trains the model to recognize expressions even when part of the face is missing (hands, glasses, masks, shadows).  \n",
        "- **Regularization:** Forces reliance on distributed cues, reducing overfitting to small regions (e.g., only mouth or only eyes).  \n",
        "- **Consistency:** Range-aware filling prevents unrealistic artifacts.  \n",
        "- **Compatibility:** Works with all common image scales used in preprocessing.\n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "`localized_erasing` simulates real-world partial occlusions by removing random patches of the face. It acts as a *strong regularizer* in the augmentation bank, improving robustness and generalization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1542643e",
      "metadata": {
        "id": "1542643e"
      },
      "source": [
        "Cell 16 â€” Train (main run)\n",
        "# === Cell 16: Train main run ==="
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 15: EMA (safe) + single-model training wrapper ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# --- Safe EMA: skips non-floating tensors; dtype/device match per param ---\n",
        "class EMA:\n",
        "    def __init__(self, model: nn.Module, decay: float = 0.9995):\n",
        "        self.decay = float(decay)\n",
        "        self.shadow = {}\n",
        "        # initialise shadow with a copy of the current (floating) weights\n",
        "        for k, v in model.state_dict().items():\n",
        "            if torch.is_floating_point(v):\n",
        "                self.shadow[k] = v.detach().clone()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, model: nn.Module):\n",
        "        d = self.decay\n",
        "        for k, v in model.state_dict().items():\n",
        "            if not torch.is_floating_point(v):\n",
        "                # integers/bool buffers (e.g., num_batches_tracked) are copied raw\n",
        "                continue\n",
        "            if k not in self.shadow:\n",
        "                self.shadow[k] = v.detach().clone()\n",
        "            # keep shadow on the same device/dtype as the live param\n",
        "            if self.shadow[k].device != v.device or self.shadow[k].dtype != v.dtype:\n",
        "                self.shadow[k] = self.shadow[k].to(device=v.device, dtype=v.dtype)\n",
        "            # shadow = d*shadow + (1-d)*v\n",
        "            self.shadow[k].mul_(d).add_(v.detach(), alpha=(1.0 - d))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def apply_shadow(self, model: nn.Module):\n",
        "        self._backup = {}\n",
        "        for k, v in model.state_dict().items():\n",
        "            if k in self.shadow and torch.is_floating_point(v):\n",
        "                self._backup[k] = v.detach().clone()\n",
        "                model.state_dict()[k].copy_(self.shadow[k])\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def restore(self, model: nn.Module):\n",
        "        # restore only those we backed up\n",
        "        if not hasattr(self, \"_backup\"):\n",
        "            return\n",
        "        for k, v in model.state_dict().items():\n",
        "            if k in self._backup and torch.is_floating_point(v):\n",
        "                model.state_dict()[k].copy_(self._backup[k])\n",
        "        self._backup = {}\n",
        "\n",
        "# --- Single-model trainer wrapper used by the batch loop ---\n",
        "from pathlib import Path\n",
        "\n",
        "def train_one(backbone_tag: str):\n",
        "    \"\"\"\n",
        "    Builds the model for `backbone_tag`, trains it with your Cell 14 `fit_with_aug`,\n",
        "    saves a unique checkpoint, and returns (model, history, ema_obj).\n",
        "    \"\"\"\n",
        "    # build model\n",
        "    model = build_model(backbone_tag)\n",
        "\n",
        "    # route best-checkpoint path so Cell 14 can write without clashes\n",
        "    save_path = ckpt_path_for(backbone_tag)\n",
        "    CONFIG[\"SAVE_BEST_PATH\"] = str(save_path)\n",
        "\n",
        "    print(f\"\\n=== TRAINING {backbone_tag.upper()} MODEL ===\")\n",
        "    history, ema_obj = fit_with_aug(model, train_dl, val_dl, HP, CONFIG)\n",
        "\n",
        "    # always save the final weights too (in case best == last is not true)\n",
        "    torch.save({\"model_state\": model.state_dict()}, save_path)\n",
        "    print(f\"[{backbone_tag}] checkpoint saved â†’ {save_path}\")\n",
        "\n",
        "    return model, history, ema_obj\n"
      ],
      "metadata": {
        "id": "JpexMeqxv86b"
      },
      "id": "JpexMeqxv86b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16f302e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685,
          "referenced_widgets": [
            "870146efa1da47b695e2c73011f0160c",
            "453a903497d347599c749c8e4b16715f",
            "8642576775984af18266bf7076d2da2a",
            "9cadcc229b4c47dd9581be483bfdcc37",
            "c3faa1a2802245b6ae5939ebe878f99a",
            "a4c39e2e28ef47e2aa8a06548395d50a",
            "49a75aa201394996851c676b93fa7a5b",
            "1dc2b7a2a4c6415591de96e0226e67f9",
            "8ffec57b32be441a9dd3984d4b55fd78",
            "4c40cc2da4744e868ff9ac999dd78bf8",
            "3fd8fd786e8543148ca04cf046bf237a"
          ]
        },
        "id": "16f302e7",
        "outputId": "9fe34b4e-f42c-4d3e-c508-1d88192c320c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.5M/20.5M [00:00<00:00, 169MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TRAINING EFFNET_B0 MODEL ===\n",
            "[001/11] train=nan  val=nan  acc=0.1301  best_acc=0.1301  lr=6.97e-04\n",
            "[010/11] train=nan  val=nan  acc=0.1301  best_acc=0.1301  lr=3.14e-05\n",
            "[011/11] train=nan  val=nan  acc=0.1301  best_acc=0.1301  lr=4.17e-06\n",
            "[effnet_b0] checkpoint saved â†’ /content/project/checkpoints/best_effnet_b0_fer.pth\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.83M/9.83M [00:00<00:00, 119MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TRAINING MOBILENET_V3 MODEL ===\n",
            "[001/11] train=nan  val=nan  acc=0.1301  best_acc=0.1301  lr=6.97e-04\n",
            "[010/11] train=nan  val=nan  acc=0.1301  best_acc=0.1301  lr=3.14e-05\n",
            "[011/11] train=nan  val=nan  acc=0.1301  best_acc=0.1301  lr=4.17e-06\n",
            "[mobilenet_v3] checkpoint saved â†’ /content/project/checkpoints/best_mobilenet_v3_fer.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/25.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "870146efa1da47b695e2c73011f0160c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 960, 1, 1])",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3218336162.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrained_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMODELS_TO_RUN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mema_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtrained_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ema\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mema_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"history\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3925927901.py\u001b[0m in \u001b[0;36mtrain_one\u001b[0;34m(backbone_tag)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \"\"\"\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# build model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# route best-checkpoint path so Cell 14 can write without clashes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4228768506.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(backbone_tag)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFER_MobileNetV3S_CBAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ghostnet_v2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFER_GhostNetV2_CBAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unknown backbone: {backbone_tag}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-652336778.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes, classifier_dropout, use_cbam)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mdummy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"IMG_SIZE\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"IMG_SIZE\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msobel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbam\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mCBAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cbam\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdaptiveAvgPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/timm/models/ghostnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/timm/models/ghostnet.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/timm/models/ghostnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;31m# 1st ghost bottleneck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mghost1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Depth-wise convolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/timm/models/ghostnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheap_operation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2813\u001b[0m         )\n\u001b[1;32m   2814\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2815\u001b[0;31m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2817\u001b[0m     return torch.batch_norm(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2779\u001b[0m         \u001b[0msize_prods\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2780\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_prods\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2781\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2782\u001b[0m             \u001b[0;34mf\"Expected more than 1 value per channel when training, got input size {size}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2783\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 960, 1, 1])"
          ]
        }
      ],
      "source": [
        "# === Cell 16: Batch train selected models (full run) ===\n",
        "trained_models = {}\n",
        "for tag in MODELS_TO_RUN:\n",
        "    model, history, ema_obj = train_one(tag)\n",
        "    trained_models[tag] = {\"model\": model, \"ema\": ema_obj, \"history\": history}\n",
        "\n",
        "print(\"\\n[Train] Completed models:\", \", \".join(trained_models.keys()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c03e38",
      "metadata": {
        "id": "13c03e38"
      },
      "source": [
        "# === Cell 18: Evaluation (val clean; test clean + optional TTA) ==="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d9e4b88",
      "metadata": {
        "id": "3d9e4b88"
      },
      "source": [
        "### Cell pre-18 â€” BatchNorm Recalibration & Clean Evaluation\n",
        "\n",
        "**Logic:**\n",
        "This cell recalibrates **BatchNorm (BN) running statistics** at the end of training, ensuring they reflect *clean (non-augmented)* data distribution. It then performs a quick validation/test evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "1. **BN Presence Check**\n",
        "   - `has_batchnorm(model)`: scans model for BatchNorm layers.\n",
        "   - *Reasoning:* Recalibration only makes sense if BN layers exist.\n",
        "\n",
        "2. **Evaluation Transform**\n",
        "   - `_get_eval_transform()`: if the validation dataset has a defined transform, reuse it as the \"clean\" transform.  \n",
        "   - *Reasoning:* Ensures recalibration uses the same normalization pipeline as evaluation.\n",
        "\n",
        "3. **Clean Train Loader**\n",
        "   - `_TransformView`: wraps training dataset with evaluation transform (disables augmentation).  \n",
        "   - Constructs `train_dl_clean` (same batch size and worker config as `val_dl`).  \n",
        "   - *Reasoning:* BN recalibration requires streaming **realistic clean data**, not augmented samples.\n",
        "\n",
        "4. **BN Recalibration**\n",
        "   - `update_bn`: feeds clean data through the model in training mode without weight updates.  \n",
        "   - Updates BN running means and variances.  \n",
        "   - Preserves original `train/eval` mode after recalibration.  \n",
        "   - *Reasoning:* During heavy augmentation training, BN stats may drift; recalibration realigns them to the clean test distribution.\n",
        "\n",
        "5. **Optional EMA Integration**\n",
        "   - Commented block suggests applying EMA weights before recalibration, then restoring after.  \n",
        "   - *Reasoning:* Ensures the final BN stats are aligned with the weights used for evaluation.\n",
        "\n",
        "6. **Quick Evaluation**\n",
        "   - `_eval_quick`: Runs validation and test sets with clean normalization.  \n",
        "   - Reports post-BN recalibration accuracy (`val_post`, `test_post`).  \n",
        "   - *Reasoning:* Confirms recalibration improves or stabilizes final evaluation metrics.\n",
        "\n",
        "7. **Fallback**\n",
        "   - If no BN layers are present, skips recalibration entirely.  \n",
        "   - *Reasoning:* Models without BN (e.g., LayerNorm, GroupNorm) donâ€™t require this.\n",
        "\n",
        "---\n",
        "\n",
        "**Why This Design?**\n",
        "- **BN sensitivity:** BN layers depend on running statistics; heavy augmentation corrupts them.  \n",
        "- **Recalibration step:** Provides *true distribution statistics* from clean data before final evaluation.  \n",
        "- **Lightweight:** No training updates, only a forward pass through clean data.  \n",
        "- **Robust final eval:** Prevents mismatch between training (augmented) and inference (clean) distributions.  \n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "BN recalibration acts as a **post-training correction step**, ensuring BatchNorm statistics reflect the clean test distribution. This yields more reliable and often higher validation/test accuracy without retraining.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZNyRyb5kYMFY",
      "metadata": {
        "id": "ZNyRyb5kYMFY"
      },
      "outputs": [],
      "source": [
        "# === Cell 18: Evaluation helpers (BN recal + clean eval)\n",
        "# Purpose:\n",
        "#   â€¢ eval_loader(): uses the PDFâ€™s `accuracy(logits, yb)` helper (no extra normalization here).\n",
        "#   â€¢ recalibrate_bn(): updates BN running stats on a *clean* (no-aug, normalized) stream.\n",
        "\n",
        "from torch.optim.swa_utils import update_bn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def _dev_of(model):\n",
        "    return next(model.parameters()).device\n",
        "\n",
        "def has_batchnorm(m: nn.Module) -> bool:\n",
        "    return any(isinstance(x, nn.modules.batchnorm._BatchNorm) for x in m.modules())\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_loader(model, loader):\n",
        "    \"\"\"\n",
        "    IMPORTANT:\n",
        "      - Assumes the loader already yields normalized tensors, as in the PDF (ToTensor + Normalize([0.5],[0.5])).\n",
        "      - Uses the provided `accuracy(logits, yb)` helper from your exam/PDF code.\n",
        "    \"\"\"\n",
        "    # Safety: ensure the helper exists, so failures are obvious.\n",
        "    assert 'accuracy' in globals(), \"Missing `accuracy` helper expected by eval_loader().\"\n",
        "    model.eval()\n",
        "    dev = _dev_of(model)\n",
        "    acc_sum, n_batches = 0.0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(dev, non_blocking=True)\n",
        "        yb = yb.to(dev, non_blocking=True)\n",
        "        # DO NOT reâ€‘normalize here; rely on dataset transforms.\n",
        "        logits = model(xb)\n",
        "        acc_sum += accuracy(logits, yb).item()\n",
        "        n_batches += 1\n",
        "    return acc_sum / max(1, n_batches)\n",
        "\n",
        "# ---- Normalizationâ€‘aware BN recalibration -----------------------------------\n",
        "def _get_eval_transform():\n",
        "    \"\"\"Inherit the evaluation transform from val loader if present.\"\"\"\n",
        "    if 'val_dl' in globals() and hasattr(val_dl, 'dataset') and hasattr(val_dl.dataset, 'transform'):\n",
        "        return val_dl.dataset.transform\n",
        "    return None\n",
        "\n",
        "EVAL_TF = _get_eval_transform()\n",
        "\n",
        "class _TransformView(Dataset):\n",
        "    \"\"\"\n",
        "    Wrap a Dataset and override its transform so we can feed *clean*,\n",
        "    normalized batches to update_bn(). If the base dataset is already\n",
        "    normalized by its transform, we simply return x as is.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_ds, transform):\n",
        "        self.base = base_ds\n",
        "        self.transform = transform\n",
        "    def __len__(self):  return len(self.base)\n",
        "    def __getitem__(self, i):\n",
        "        x, y = self.base[i]\n",
        "        # If original dataset already applies Normalize, leave `x` unchanged.\n",
        "        # Otherwise (rare) youâ€™d apply explicit normalization here.\n",
        "        return x, y\n",
        "\n",
        "# Build a clean (no aug) train-like stream for BN recalibration\n",
        "if EVAL_TF is not None:\n",
        "    train_ds_clean = _TransformView(train_dl.dataset, EVAL_TF)\n",
        "    train_dl_clean = DataLoader(\n",
        "        train_ds_clean,\n",
        "        batch_size=val_dl.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=val_dl.num_workers,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=getattr(val_dl, 'persistent_workers', False)\n",
        "    )\n",
        "else:\n",
        "    print(\"[BN][WARN] No eval transform detected; reusing train_dl (assumed normalized).\")\n",
        "    train_dl_clean = train_dl\n",
        "\n",
        "def recalibrate_bn(model, train_like_loader=train_dl_clean):\n",
        "    \"\"\"\n",
        "    Recompute BN running stats on a clean, normalized stream.\n",
        "    No extra normalization is applied here; we trust the datasetâ€™s transform.\n",
        "    \"\"\"\n",
        "    if not has_batchnorm(model):\n",
        "        print(\"[BN] No BN layers; skipping recalibration.\")\n",
        "        return\n",
        "    dev = _dev_of(model)\n",
        "    was_training = model.training\n",
        "    model.train()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _clean_iter(dloader):\n",
        "        for xb, _ in dloader:\n",
        "            yield xb.to(dev, non_blocking=True)  # already normalized by dataset transforms\n",
        "\n",
        "    update_bn(_clean_iter(train_like_loader), model)\n",
        "    model.train(was_training); model.eval()\n",
        "    print(\"[BN] Done.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "019ad855",
      "metadata": {
        "id": "019ad855"
      },
      "source": [
        "#=== Cell 19: Reload + evaluate each model (val/test) â€” using exam accuracy ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74d24fd0",
      "metadata": {
        "id": "74d24fd0"
      },
      "outputs": [],
      "source": [
        "# === Cell 17: Reload + evaluate each model (val/test) ===\n",
        "results = {}\n",
        "for tag in MODELS_TO_RUN:\n",
        "    path = ckpt_path_for(tag)\n",
        "    print(f\"\\n[Eval] Loading {tag} from {path}\")\n",
        "    model = build_model(tag)\n",
        "    ckpt  = torch.load(path, map_location=device)\n",
        "    state = ckpt.get(\"model_state\", ckpt)  # tolerate raw state_dict or wrapped\n",
        "    msg = model.load_state_dict(state, strict=False)\n",
        "    if getattr(msg, \"missing_keys\", None) or getattr(msg, \"unexpected_keys\", None):\n",
        "        print(\"[warn] load_state_dict mismatches:\", msg)\n",
        "\n",
        "    # Recalibrate BN on a clean, normalized stream (no aug)\n",
        "    recalibrate_bn(model, train_dl_clean if 'train_dl_clean' in globals() else train_dl)\n",
        "\n",
        "    # PDF-logic eval (no explicit normalization here)\n",
        "    val_acc  = eval_loader(model, val_dl)\n",
        "    test_acc = eval_loader(model, test_dl)\n",
        "    print(f\"[{tag}] val={val_acc:.4f}  test={test_acc:.4f}\")\n",
        "    results[tag] = {\"val\": val_acc, \"test\": test_acc}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30d0d0d3",
      "metadata": {
        "id": "30d0d0d3"
      },
      "source": [
        "#=== Cell 20: Sideâ€‘byâ€‘side summary (tabular) ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dff455b",
      "metadata": {
        "id": "1dff455b"
      },
      "outputs": [],
      "source": [
        "#=== Cell 20: Sideâ€‘byâ€‘side summary (tabular) ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a89eb770",
      "metadata": {
        "id": "a89eb770"
      },
      "outputs": [],
      "source": [
        "# === Cell 18: Sideâ€‘byâ€‘side summary ===\n",
        "import pandas as pd\n",
        "df_results = pd.DataFrame.from_dict(\n",
        "    {k: {**v, \"ckpt\": str(ckpt_path_for(k))} for k, v in results.items()},\n",
        "    orient=\"index\"\n",
        ")[[\"val\", \"test\", \"ckpt\"]]\n",
        "print(df_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "608eb229",
      "metadata": {
        "id": "608eb229"
      },
      "source": [
        "# === Cell 19: Confusion matrix + per-class accuracy (test) ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36653a3e",
      "metadata": {
        "id": "36653a3e"
      },
      "outputs": [],
      "source": [
        "# === Cell 19: Confusion matrix + perâ€‘class accuracy (for any model) ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "IDX2EMO = {0:'Angry',1:'Disgust',2:'Fear',3:'Happy',4:'Sad',5:'Surprise',6:'Neutral'}\n",
        "\n",
        "@torch.no_grad()\n",
        "def confusion_matrix_and_report(model: nn.Module,\n",
        "                                loader,\n",
        "                                num_classes: int = 7,\n",
        "                                normalize_inputs: bool = True,\n",
        "                                title: str | None = None):\n",
        "    \"\"\"\n",
        "    Computes a confusion matrix on `loader`, plots it, and prints perâ€‘class accuracy.\n",
        "    Normalization matches the notebook's eval path: (x/255 - 0.5)*2 when `normalize_inputs=True`.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    dev = next(model.parameters()).device\n",
        "\n",
        "    cm = torch.zeros(num_classes, num_classes, dtype=torch.int64, device=dev)\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(dev, non_blocking=True)\n",
        "        yb = yb.to(dev, non_blocking=True)\n",
        "        if normalize_inputs:\n",
        "            xb = ((xb / 255.) - 0.5) * 2.0\n",
        "        pred = model(xb).argmax(1)\n",
        "        for t, p in zip(yb.view(-1), pred.view(-1)):\n",
        "            cm[t.long(), p.long()] += 1\n",
        "\n",
        "    cm_cpu = cm.cpu()\n",
        "    totals = cm_cpu.sum(1).clamp(min=1)\n",
        "    per_class = (cm_cpu.diag() / totals).numpy() * 100.0\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.imshow(cm_cpu, interpolation='nearest')\n",
        "    plt.title(title or \"Confusion (rows=true, cols=pred)\")\n",
        "    plt.colorbar(fraction=0.046, pad=0.04)\n",
        "    plt.xticks(range(num_classes), [IDX2EMO[i] for i in range(num_classes)], rotation=45, ha='right')\n",
        "    plt.yticks(range(num_classes), [IDX2EMO[i] for i in range(num_classes)])\n",
        "    for i, j in itertools.product(range(num_classes), range(num_classes)):\n",
        "        n = cm_cpu[i, j].item()\n",
        "        if n > 0:\n",
        "            plt.text(j, i, n, ha='center', va='center', fontsize=7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    for i, acc in enumerate(per_class):\n",
        "        print(f\"{IDX2EMO[i]:>8s}: {acc:.2f}%\")\n",
        "    return cm_cpu, per_class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3b467f9",
      "metadata": {
        "id": "c3b467f9"
      },
      "source": [
        "#=== Cell 22: Reload â†’ BN recalibration â†’ evaluate all models (val/test) ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff14a927",
      "metadata": {
        "id": "ff14a927"
      },
      "outputs": [],
      "source": [
        "# === Cell 20: Reload â†’ BN recalibration â†’ evaluate all models (val/test) ===\n",
        "from torch.optim.swa_utils import update_bn\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "def has_batchnorm(m: nn.Module) -> bool:\n",
        "    return any(isinstance(x, nn.modules.batchnorm._BatchNorm) for x in m.modules())\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_loader(model: nn.Module, loader) -> float:\n",
        "    \"\"\"\n",
        "    Returns mean topâ€‘1 accuracy (fraction) across the loader.\n",
        "    Uses the same evaluation normalization as your earlier cells: (x/255 - 0.5)*2.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    dev = next(model.parameters()).device\n",
        "    acc_sum, n_batches = 0.0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(dev, non_blocking=True)\n",
        "        yb = yb.to(dev, non_blocking=True)\n",
        "        xb = ((xb / 255.) - 0.5) * 2.0\n",
        "        logits = model(xb)\n",
        "        acc_sum += (logits.argmax(1) == yb).float().mean().item()\n",
        "        n_batches += 1\n",
        "    return acc_sum / max(1, n_batches)\n",
        "\n",
        "def _clean_stream_from(train_dl_like):\n",
        "    \"\"\"\n",
        "    Iterator that yields only normalized inputs for BN runningâ€‘stat recalibration.\n",
        "    Matches eval normalization: (x/255 - 0.5)*2.\n",
        "    \"\"\"\n",
        "    dev = next(iter(train_dl_like))[0].device if hasattr(train_dl_like, '__iter__') else device\n",
        "    @torch.no_grad()\n",
        "    def _gen():\n",
        "        for xb, _ in train_dl_like:\n",
        "            xb = xb.to(dev, non_blocking=True)\n",
        "            xb = ((xb / 255.) - 0.5) * 2.0\n",
        "            yield xb\n",
        "    return _gen()\n",
        "\n",
        "def recalibrate_bn(model: nn.Module, train_dl_like):\n",
        "    \"\"\"Recomputes BN running stats on a clean (noâ€‘aug) stream.\"\"\"\n",
        "    if not has_batchnorm(model):\n",
        "        return\n",
        "    was_training = model.training\n",
        "    model.train()\n",
        "    update_bn(_clean_stream_from(train_dl_like), model)\n",
        "    model.train(was_training); model.eval()\n",
        "\n",
        "# Evaluate all requested models (MODELS_TO_RUN should be defined earlier)\n",
        "results = {}\n",
        "loaded_models = {}   # keep the constructed model objects for later cells (plots, FLOPs, CM)\n",
        "for tag in MODELS_TO_RUN:\n",
        "    path = ckpt_path_for(tag)\n",
        "    print(f\"\\n[Eval] Loading {tag} from {path}\")\n",
        "    m = build_model(tag)\n",
        "\n",
        "    # Load state dict (we saved {\"model_state\": ...})\n",
        "    ckpt = torch.load(path, map_location=device)\n",
        "    state = ckpt.get(\"model_state\", ckpt)\n",
        "    msg = m.load_state_dict(state, strict=False)\n",
        "    if getattr(msg, \"missing_keys\", None) or getattr(msg, \"unexpected_keys\", None):\n",
        "        print(\"[warn] load_state_dict mismatches:\", msg)\n",
        "\n",
        "    # BN recalibration on clean training stream (your train_dl is already DeviceDataLoader)\n",
        "    recalibrate_bn(m, train_dl)\n",
        "\n",
        "    # Clean evaluation\n",
        "    val_acc  = eval_loader(m, val_dl)\n",
        "    test_acc = eval_loader(m, test_dl)\n",
        "    print(f\"[{tag}] val={val_acc:.4f}  test={test_acc:.4f}\")\n",
        "    results[tag] = {\"val\": val_acc, \"test\": test_acc}\n",
        "    loaded_models[tag] = m\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8da69e38",
      "metadata": {
        "id": "8da69e38"
      },
      "source": [
        "#=== Cell 25: Accuracy/Loss curves (handles multiple histories) ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HIwXyFezq_Sf",
      "metadata": {
        "id": "HIwXyFezq_Sf"
      },
      "outputs": [],
      "source": [
        "# === Cell 23: Accuracy/Loss curves (handles multiple histories) ===\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def _plot_history(history, label_prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Expects history to be a list[dict] with keys: epoch, train_loss, val_loss, train_acc, val_acc, (optional) test_acc.\n",
        "    The trainer youâ€™re using should already populate a similar structure.\n",
        "    \"\"\"\n",
        "    if not history or not isinstance(history, list):\n",
        "        return False\n",
        "\n",
        "    epochs     = [m.get('epoch', i+1) for i, m in enumerate(history)]\n",
        "    train_loss = [m.get('train_loss') for m in history]\n",
        "    val_loss   = [m.get('val_loss')   for m in history]\n",
        "    train_acc  = [m.get('train_acc')  for m in history]\n",
        "    val_acc    = [m.get('val_acc')    for m in history]\n",
        "    test_acc   = [m.get('test_acc')   for m in history] if history and 'test_acc' in history[0] else None\n",
        "\n",
        "    plt.figure(figsize=(12,5))\n",
        "\n",
        "    # 1) Accuracy\n",
        "    plt.subplot(1,2,1)\n",
        "    if train_acc[0] is not None:\n",
        "        plt.plot(epochs, train_acc, marker='o', label=f\"{label_prefix}Train Acc\")\n",
        "    plt.plot(epochs, val_acc, marker='o', label=f\"{label_prefix}Val Acc\")\n",
        "    if test_acc is not None and any(v is not None for v in test_acc):\n",
        "        plt.plot(epochs, test_acc, marker='x', label=f\"{label_prefix}Test Acc\")\n",
        "    plt.title(f\"Accuracy â€” {label_prefix}\".strip())\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Topâ€‘1 Acc\"); plt.grid(True); plt.legend()\n",
        "\n",
        "    # 2) Loss\n",
        "    plt.subplot(1,2,2)\n",
        "    if train_loss[0] is not None:\n",
        "        plt.plot(epochs, train_loss, marker='o', label=f\"{label_prefix}Train Loss\")\n",
        "    plt.plot(epochs, val_loss, marker='o', label=f\"{label_prefix}Val Loss\")\n",
        "    plt.title(f\"Loss â€” {label_prefix}\".strip())\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.grid(True); plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return True\n",
        "\n",
        "# If you trained in this session via Cell 15, `trained_models[tag]['history']` will exist.\n",
        "for tag in MODELS_TO_RUN:\n",
        "    hist = trained_models.get(tag, {}).get(\"history\", None)\n",
        "    if hist:\n",
        "        _ = _plot_history(hist, label_prefix=f\"{tag} \")\n",
        "    else:\n",
        "        print(f\"[Curves] No history found for '{tag}' (skipping).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a898e2e",
      "metadata": {
        "id": "1a898e2e"
      },
      "source": [
        "### Analysis of Training Curves\n",
        "\n",
        "#### Accuracy Curves (left plot)\n",
        "- Validation accuracy starts around **35%** and rises steadily.  \n",
        "- By **epoch ~10**, accuracy is already **60%+**, showing rapid early learning.  \n",
        "- It then **plateaus between 66â€“69%**, with small oscillations (expected under augmentation/regularization).  \n",
        "- By **epoch 55â€“60**, it stabilizes close to **70%**.  \n",
        "\n",
        "**Interpretation:**  \n",
        "- The model converges well and generalizes up to ~70% validation accuracy.  \n",
        "- The plateau after ~20 epochs shows **diminishing returns** â€” most learning happens early, and fine-tuning/taper schedules carry it to ~70%.  \n",
        "- The small oscillations suggest some sensitivity to augmentations or learning rate scheduling, but the trend is stable.  \n",
        "\n",
        "---\n",
        "\n",
        "#### Loss Curves (right plot)\n",
        "- **Train Loss** decreases smoothly from ~2.0 to <0.25.  \n",
        "- **Validation Loss** decreases sharply at first, bottoms out around **epoch 15â€“20**, then starts fluctuating and slowly increasing.  \n",
        "\n",
        "**Interpretation:**  \n",
        "- Training loss continues to fall â†’ the model fits training data strongly.  \n",
        "- Validation loss diverges slightly after ~20 epochs while validation accuracy stays flat.  \n",
        "- This indicates **mild overfitting**: the model memorizes training patterns but does not gain further generalization.  \n",
        "- Augmentation + label smoothing prevented collapse, but the upward trend in validation loss shows the network is at its **effective capacity**.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d19e1269",
      "metadata": {
        "id": "d19e1269"
      },
      "source": [
        "# === Cell 20: FLOPs (fvcore) + Accuracy/GFLOP ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DuMfrwctVKMr",
      "metadata": {
        "id": "DuMfrwctVKMr"
      },
      "outputs": [],
      "source": [
        "%pip install fvcore\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dd8f669",
      "metadata": {
        "id": "2dd8f669"
      },
      "source": [
        "#=== Cell 26: FLOPs + efficiency (acc% / GFLOP) for all models ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "050a90f1",
      "metadata": {
        "id": "050a90f1"
      },
      "outputs": [],
      "source": [
        "# === Cell 24: FLOPs + efficiency (acc% / GFLOP) for all models ===\n",
        "try:\n",
        "    from fvcore.nn import FlopCountAnalysis\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"fvcore\"])\n",
        "    from fvcore.nn import FlopCountAnalysis\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def _probe_in_channels(m: nn.Module, img_sz: int, dev, dtype) -> int:\n",
        "    \"\"\"\n",
        "    Our backbones accept 1â€‘ch inputs (Sobel expands to 3 inside), but some\n",
        "    wrappers may accept 3 directly. Probe 1 then 3 safely.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        for c in (1, 3):\n",
        "            try:\n",
        "                _ = m(torch.zeros(1, c, img_sz, img_sz, device=dev, dtype=dtype))\n",
        "                return c\n",
        "            except Exception:\n",
        "                pass\n",
        "    # Fallback to first convâ€™s channel count\n",
        "    for mod in m.modules():\n",
        "        if isinstance(mod, nn.Conv2d):\n",
        "            return int(mod.weight.shape[1])\n",
        "    return 1\n",
        "\n",
        "def _accuracy_for_efficiency(tag: str) -> float | None:\n",
        "    # Prefer the measured test accuracy from Cell 20\n",
        "    rec = results.get(tag, None)\n",
        "    if rec and isinstance(rec.get(\"test\", None), (int, float)):\n",
        "        return float(rec[\"test\"])\n",
        "    # Otherwise attempt a quick pass on test set (rare)\n",
        "    m = loaded_models.get(tag, None)\n",
        "    if m is None:\n",
        "        return None\n",
        "    return eval_loader(m, test_dl)\n",
        "\n",
        "eff_table = []\n",
        "IMG_SIZE = int(CONFIG.get(\"IMG_SIZE\", 96))\n",
        "\n",
        "for tag, model in loaded_models.items():\n",
        "    model.eval()\n",
        "    dev   = next(model.parameters()).device\n",
        "    dtype = next(model.parameters()).dtype\n",
        "    in_ch = _probe_in_channels(model, IMG_SIZE, dev, dtype)\n",
        "\n",
        "    dummy = torch.zeros(1, in_ch, IMG_SIZE, IMG_SIZE, device=dev, dtype=dtype)\n",
        "    flops = float(FlopCountAnalysis(model, dummy).total())   # operations\n",
        "    gflops = flops / 1e9\n",
        "\n",
        "    acc = _accuracy_for_efficiency(tag)  # fraction\n",
        "    eff = (acc * 100.0) / gflops if (isinstance(acc, (int, float)) and gflops > 0) else None\n",
        "\n",
        "    eff_table.append({\"model\": tag, \"GFLOPs\": gflops, \"acc_test\": acc, \"efficiency_%/GFLOP\": eff})\n",
        "\n",
        "eff_df = pd.DataFrame(eff_table).set_index(\"model\").sort_values(\"efficiency_%/GFLOP\", ascending=False)\n",
        "print(eff_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OFUc1dZ5vcIz",
      "metadata": {
        "id": "OFUc1dZ5vcIz"
      },
      "outputs": [],
      "source": [
        "# === Cell: FLOPs, GFLOPs, and Efficiency (CONFIG-aware, no required IN_CHANNELS) ===\n",
        "# Uses CONFIG[\"IMG_SIZE\"] if set; otherwise falls back to 48.\n",
        "# If CONFIG[\"IN_CHANNELS\"] is not set, it probes the model (1â†’3) safely.\n",
        "\n",
        "# fvcore import\n",
        "try:\n",
        "    from fvcore.nn import FlopCountAnalysis\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"fvcore\"])\n",
        "    from fvcore.nn import FlopCountAnalysis\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ---------- Model / device ----------\n",
        "model.eval()\n",
        "_dev   = next(model.parameters()).device\n",
        "_dtype = next(model.parameters()).dtype\n",
        "\n",
        "# ---------- Read from CONFIG (no hardcoding) ----------\n",
        "_cfg = globals().get(\"CONFIG\", {})\n",
        "IMG_SIZE    = int(_cfg.get(\"IMG_SIZE\", 48))              # you set 98; this will use it\n",
        "IN_CHANNELS = _cfg.get(\"IN_CHANNELS\", None)              # optional\n",
        "\n",
        "# Optional eval normalisation config; default matches your eval path\n",
        "NORM_SPEC = _cfg.get(\"EVAL_NORMALIZE\", {\"mode\": \"minus1to1\", \"scale_255\": True})\n",
        "LOADER_PREF = _cfg.get(\"EVAL_LOADER_PRIORITY\",\n",
        "                       [\"test_dl_hybrid\",\"test_dl\",\"val_dl_hybrid\",\"val_dl\"])\n",
        "\n",
        "def _apply_eval_norm(x: torch.Tensor) -> torch.Tensor:\n",
        "    mode = str(NORM_SPEC.get(\"mode\", \"minus1to1\")).lower()\n",
        "    scale_255 = bool(NORM_SPEC.get(\"scale_255\", True))\n",
        "    if mode == \"meanstd\":\n",
        "        mean = torch.as_tensor(NORM_SPEC[\"mean\"], device=x.device, dtype=x.dtype).view(1, -1, 1, 1)\n",
        "        std  = torch.as_tensor(NORM_SPEC[\"std\"],  device=x.device, dtype=x.dtype).view(1, -1, 1, 1)\n",
        "        x = x / 255.0 if scale_255 else x\n",
        "        return (x - mean) / std\n",
        "    if mode == \"minus1to1\":\n",
        "        x = x / 255.0 if scale_255 else x\n",
        "        return (x - 0.5) * 2.0\n",
        "    return x  # \"none\"\n",
        "\n",
        "# ---------- Decide input channels (probe if not provided) ----------\n",
        "def _resolve_in_ch(m: nn.Module, img_sz: int, device, dtype, cfg_ch):\n",
        "    if isinstance(cfg_ch, (int, float)) and int(cfg_ch) in (1, 3):\n",
        "        return int(cfg_ch)\n",
        "    with torch.no_grad():\n",
        "        for c in (1, 3):\n",
        "            try:\n",
        "                _ = m(torch.zeros(1, c, img_sz, img_sz, device=device, dtype=dtype))\n",
        "                return c\n",
        "            except Exception:\n",
        "                pass\n",
        "    for mod in m.modules():                 # fallback: infer from first Conv2d\n",
        "        if isinstance(mod, nn.Conv2d):\n",
        "            return int(mod.weight.shape[1])\n",
        "    return 1\n",
        "\n",
        "IN_CH = _resolve_in_ch(model, IMG_SIZE, _dev, _dtype, IN_CHANNELS)\n",
        "\n",
        "# ---------- FLOPs (reference-compatible print) ----------\n",
        "_dummy = torch.zeros(1, IN_CH, IMG_SIZE, IMG_SIZE, device=_dev, dtype=_dtype)\n",
        "_total_flops = float(FlopCountAnalysis(model, _dummy).total())\n",
        "_gflops = _total_flops / 1e9\n",
        "\n",
        "print(f\"FLOPs: {_gflops:.5f} GFLOPs\")          # exact reference format\n",
        "print(f\"Total FLOPs (ops): {_total_flops:,.0f}\")\n",
        "\n",
        "# ---------- Accuracy for efficiency ----------\n",
        "def _best_recorded_acc():\n",
        "    for k in (\"test_acc_tta\",\"test_acc_ema\",\"test_acc_base\",\"val_acc_ema\",\"val_acc_base\"):\n",
        "        v = globals().get(k, None)\n",
        "        if isinstance(v, (int, float)):\n",
        "            return float(v)\n",
        "    hist = globals().get(\"history\", None)\n",
        "    if isinstance(hist, list) and hist and isinstance(hist[-1].get(\"val_acc\", None), (int, float)):\n",
        "        return float(hist[-1][\"val_acc\"])\n",
        "    return None\n",
        "\n",
        "@torch.no_grad()\n",
        "def _eval_top1(m, loader):\n",
        "    if loader is None:\n",
        "        return None\n",
        "    m.eval()\n",
        "    tot = cor = 0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(_dev, non_blocking=True)\n",
        "        yb = yb.to(_dev, non_blocking=True)\n",
        "        xb = _apply_eval_norm(xb)\n",
        "        logits = m(xb)\n",
        "        cor += (logits.argmax(1) == yb).sum().item()\n",
        "        tot += yb.size(0)\n",
        "    return (cor / tot) if tot > 0 else None\n",
        "\n",
        "acc_frac = _best_recorded_acc()\n",
        "if acc_frac is None:\n",
        "    # Try loaders in the order given by CONFIG (or the default list)\n",
        "    for name in LOADER_PREF:\n",
        "        loader = globals().get(name, None)\n",
        "        acc_frac = _eval_top1(model, loader)\n",
        "        if isinstance(acc_frac, (int, float)):\n",
        "            break\n",
        "\n",
        "# ---------- Efficiency = accuracy(%) / GFLOPs ----------\n",
        "if isinstance(acc_frac, (int, float)) and _gflops > 0:\n",
        "    efficiency = (acc_frac * 100.0) / _gflops\n",
        "    print(f\"Efficiency (acc% / GFLOPs): {efficiency:.2f}\")\n",
        "else:\n",
        "    print(\"Efficiency not computed: missing accuracy and/or GFLOPs==0.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a604866",
      "metadata": {
        "id": "8a604866"
      },
      "source": [
        "### Final Evaluation Results\n",
        "\n",
        "| Metric                         | Value        |\n",
        "|--------------------------------|--------------|\n",
        "| **FLOPs** (GFLOPs)             | 0.07465      |\n",
        "| **Total FLOPs (ops)**          | 74,650,482   |\n",
        "| **Efficiency** (Acc% / GFLOPs) | 934.64       |\n",
        "| **Validation Accuracy**        | 0.6952 (~69.5%) |\n",
        "| **Test Accuracy**              | 0.7040 (~70.4%) |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0S0aIp-riFim",
      "metadata": {
        "id": "0S0aIp-riFim"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# TESTING: accuracy + image predictions\n",
        "# =========================\n",
        "import torch, numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as VF\n",
        "\n",
        "# --- Class names for readability ---\n",
        "CLASS_NAMES = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']\n",
        "\n",
        "# --- Normalisation used for eval ---\n",
        "def _to_m11(x: torch.Tensor) -> torch.Tensor:\n",
        "    # x in [0,255] (uint8/float) -> float in [-1,1]\n",
        "    return ((x.float() / 255.0) - 0.5) * 2.0\n",
        "\n",
        "# --- 1) Load the trained checkpoint ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "ckpt_path = str(CONFIG.get(\"SAVE_BEST_PATH\", \"\"))  # e.g., \"project/checkpoints/best_fer.pth\"\n",
        "assert ckpt_path, \"CONFIG['SAVE_BEST_PATH'] is empty; set the checkpoint path first.\"\n",
        "\n",
        "# Rebuild model skeleton exactly as trained\n",
        "model = HybridEffNet(num_classes=7, classifier_dropout=0.30, use_cbam=True).to(device)\n",
        "ckpt = torch.load(ckpt_path, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model_state\"], strict=True)\n",
        "model.eval()\n",
        "\n",
        "# --- 2) Compute and print test accuracy (PrivateTest) ---\n",
        "@torch.no_grad()\n",
        "def eval_accuracy(model, loader) -> float:\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb = _to_m11(xb.to(device, non_blocking=True))\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "        logits = model(xb)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += (pred == yb).sum().item()\n",
        "        total   += yb.numel()\n",
        "    return correct / max(1, total)\n",
        "\n",
        "test_acc = eval_accuracy(model, test_dl)\n",
        "print(f\"[Test] accuracy (PrivateTest) = {test_acc:.4f}\")\n",
        "\n",
        "# --- 3a) Predict a few samples from the FER2013 test loader ---\n",
        "@torch.no_grad()\n",
        "def preview_test_predictions(model, loader, n=12):\n",
        "    model.eval()\n",
        "    xb, yb = next(iter(loader))                 # one batch\n",
        "    xb = xb[:n]                                 # first n images\n",
        "    gt = yb[:n].cpu().numpy()\n",
        "    xb_dev = _to_m11(xb.to(device, non_blocking=True))\n",
        "    logits = model(xb_dev)\n",
        "    probs  = logits.softmax(1).cpu().numpy()\n",
        "    pred   = probs.argmax(1)\n",
        "    # print a small table\n",
        "    print(\"\\n[Index]  Pred (pmax)     |  GT\")\n",
        "    for i in range(n):\n",
        "        pclass = int(pred[i]); gclass = int(gt[i])\n",
        "        pmax   = float(probs[i, pclass])\n",
        "        print(f\"{i:>6d}  {CLASS_NAMES[pclass]:<12s} ({pmax:0.3f}) |  {CLASS_NAMES[gclass]}\")\n",
        "    return pred, gt\n",
        "\n",
        "_ = preview_test_predictions(model, test_dl, n=12)\n",
        "\n",
        "# --- 3b) Predict arbitrary external images (file paths) ---\n",
        "@torch.no_grad()\n",
        "def predict_images(model, image_paths):\n",
        "    \"\"\"\n",
        "    image_paths: List[str] to arbitrary images.\n",
        "    Preprocessing: grayscale -> resize to 96x96 -> tensor [1,H,W] -> [-1,1].\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    batch = []\n",
        "    for path in image_paths:\n",
        "        img = Image.open(path).convert(\"L\")            # force grayscale\n",
        "        img = img.resize((CONFIG[\"IMG_SIZE\"], CONFIG[\"IMG_SIZE\"]), Image.BILINEAR)\n",
        "        x = torch.from_numpy(np.array(img, dtype=np.uint8))[None, ...]  # [1,H,W] uint8\n",
        "        batch.append(x)\n",
        "    xb = torch.stack(batch, dim=0)                     # [B,1,H,W]\n",
        "    xb = _to_m11(xb).to(device, non_blocking=True)\n",
        "    logits = model(xb)\n",
        "    probs  = logits.softmax(1).cpu().numpy()\n",
        "    preds  = probs.argmax(1)\n",
        "    # pretty-print\n",
        "    print(\"\\n[External Image Predictions]\")\n",
        "    for path, p in zip(image_paths, preds):\n",
        "        print(f\"{path}  ->  {CLASS_NAMES[int(p)]} (p={probs[list(preds).index(p), int(p)]:.3f})\")\n",
        "    return preds, probs\n",
        "\n",
        "# Example usage for external files (uncomment and set your paths):\n",
        "# preds, probs = predict_images(model, [\n",
        "#     \"/content/some_face1.png\",\n",
        "#     \"/content/some_face2.jpg\",\n",
        "# ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rvuKRYewo88q",
      "metadata": {
        "id": "rvuKRYewo88q"
      },
      "outputs": [],
      "source": [
        "# === Cell B: Visualize predictions on Test images (robust / auto-range) ===\n",
        "import torch, numpy as np, matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Class names (adjust if your label order differs)\n",
        "Labels = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']\n",
        "\n",
        "# If you used my BN-recal cell, EVAL_TF may exist. Otherwise this stays None.\n",
        "EVAL_TF = globals().get('EVAL_TF', None)\n",
        "\n",
        "def _transform_includes_normalize(transform) -> bool:\n",
        "    \"\"\"Detects torchvision.transforms.Normalize inside a Compose-like transform.\"\"\"\n",
        "    try:\n",
        "        from torchvision.transforms import Normalize\n",
        "        seq = getattr(transform, 'transforms', None)\n",
        "        return any(isinstance(t, Normalize) for t in (seq or []))\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "@torch.no_grad()\n",
        "def _model_ready_batch(xb: torch.Tensor, dev: torch.device) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Returns a tensor ready for the model:\n",
        "    - If val/test transform already contains Normalize(0.5,0.5), pass through.\n",
        "    - Otherwise apply explicit ((x/255)-0.5)*2 normalization.\n",
        "    \"\"\"\n",
        "    xb = xb.to(dev, non_blocking=True)\n",
        "    need_explicit_norm = True\n",
        "    if EVAL_TF is not None and _transform_includes_normalize(EVAL_TF):\n",
        "        need_explicit_norm = False\n",
        "    if need_explicit_norm:\n",
        "        xb = ((xb / 255.0) - 0.5) * 2.0\n",
        "    return xb\n",
        "\n",
        "def _to_display(img: torch.Tensor) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert a single image tensor to HxW in [0,1] for imshow.\n",
        "    Works for uint8 [0,255], float [0,1], or float [-1,1].\n",
        "    \"\"\"\n",
        "    x = img.detach().cpu()\n",
        "    if x.ndim == 3 and x.size(0) == 1:  # [1,H,W] -> [H,W]\n",
        "        x = x[0]\n",
        "    x = x.float()\n",
        "    m, M = float(x.min()), float(x.max())\n",
        "    if M > 1.5:          # likely uint8 [0,255]\n",
        "        x = x / 255.0\n",
        "    elif m < -0.25:      # likely [-1,1]\n",
        "        x = (x * 0.5) + 0.5\n",
        "    x = x.clamp(0, 1)\n",
        "    return x.numpy()\n",
        "\n",
        "@torch.no_grad()\n",
        "def fetch_batch_and_predict(model, loader: DataLoader):\n",
        "    \"\"\"Fetch first batch from loader, run model, return (xb_raw, yb, pred, conf).\"\"\"\n",
        "    model.eval()\n",
        "    dev = next(model.parameters()).device\n",
        "    xb, yb = next(iter(loader))\n",
        "    xb_for_model = _model_ready_batch(xb.clone(), dev)\n",
        "    logits = model(xb_for_model)\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    pred  = probs.argmax(1)\n",
        "    conf  = probs.max(1).values\n",
        "    return xb, yb, pred.cpu(), conf.cpu()\n",
        "\n",
        "# If you want EMA weights evaluated, uncomment:\n",
        "# try: ema_tail.apply_shadow(model)\n",
        "# except NameError: pass\n",
        "\n",
        "xb, yb, pred, conf = fetch_batch_and_predict(model, test_dl)\n",
        "\n",
        "# --- Grid render ---\n",
        "K = min(25, xb.size(0))   # number of images to show\n",
        "cols = 5\n",
        "rows = int(np.ceil(K / cols))\n",
        "plt.figure(figsize=(cols * 3, rows * 3))\n",
        "\n",
        "for i in range(K):\n",
        "    ax = plt.subplot(rows, cols, i + 1)\n",
        "    ax.imshow(_to_display(xb[i]), cmap='gray', interpolation='nearest')\n",
        "    t = Labels[int(yb[i])]\n",
        "    p = Labels[int(pred[i])]\n",
        "    c = float(conf[i])\n",
        "    ax.set_title(f\"P:{p} ({c:.2f})\\nT:{t}\", fontsize=9)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# If you applied EMA above and want to revert to base weights, uncomment:\n",
        "# try: ema_tail.restore(model)\n",
        "# except NameError: pass\n",
        "\n",
        "\"\"\"# === Cell 22: Save final checkpoint & reload sanity ===\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# === Cell 22: Save final checkpoint & reload sanity ===\n",
        "FINAL_PATH = CKPT_DIR / \"final_fer_model.pth\"\n",
        "torch.save({\"model_state\": model.state_dict()}, FINAL_PATH)\n",
        "print(f\"[Save] {FINAL_PATH}\")\n",
        "\n",
        "ckpt = torch.load(FINAL_PATH, map_location=\"cpu\")\n",
        "# ======= BUG FIX START: Fixed model.device attribute error =======\n",
        "# Original: model.to(model.device).eval() - models don't have .device attribute by default\n",
        "# Fixed to use the device variable defined at line 57\n",
        "model.load_state_dict(ckpt[\"model_state\"]); model.to(device).eval()\n",
        "# ======= BUG FIX END =======\n",
        "with torch.no_grad():\n",
        "    xb, yb = next(iter(val_dl))\n",
        "    xb = ((xb/255.) - 0.5) * 2.0\n",
        "    # ======= BUG FIX START: Fixed another model.device reference =======\n",
        "    out = model(xb.to(device))\n",
        "    # ======= BUG FIX END =======\n",
        "print(\"[Reload] Sanity forward OK:\", tuple(out.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "359b37fe",
      "metadata": {
        "id": "359b37fe"
      },
      "source": [
        "# === Cell 22: Save final checkpoint & reload sanity ===\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30b73fe1",
      "metadata": {
        "id": "30b73fe1"
      },
      "outputs": [],
      "source": [
        "# === Cell 22: Save final checkpoint & reload sanity ===\n",
        "FINAL_PATH = CKPT_DIR / \"final_fer_model.pth\"\n",
        "torch.save({\"model_state\": model.state_dict()}, FINAL_PATH)\n",
        "print(f\"[Save] {FINAL_PATH}\")\n",
        "\n",
        "ckpt = torch.load(FINAL_PATH, map_location=\"cpu\")\n",
        "model.load_state_dict(ckpt[\"model_state\"]); model.to(model.device).eval()\n",
        "with torch.no_grad():\n",
        "    xb, yb = next(iter(val_dl))\n",
        "    xb = ((xb/255.) - 0.5) * 2.0\n",
        "    out = model(xb.to(model.device))\n",
        "print(\"[Reload] Sanity forward OK:\", tuple(out.shape))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "first_test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "870146efa1da47b695e2c73011f0160c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_453a903497d347599c749c8e4b16715f",
              "IPY_MODEL_8642576775984af18266bf7076d2da2a",
              "IPY_MODEL_9cadcc229b4c47dd9581be483bfdcc37"
            ],
            "layout": "IPY_MODEL_c3faa1a2802245b6ae5939ebe878f99a"
          }
        },
        "453a903497d347599c749c8e4b16715f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4c39e2e28ef47e2aa8a06548395d50a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_49a75aa201394996851c676b93fa7a5b",
            "value": "model.safetensors:â€‡100%"
          }
        },
        "8642576775984af18266bf7076d2da2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1dc2b7a2a4c6415591de96e0226e67f9",
            "max": 24957808,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ffec57b32be441a9dd3984d4b55fd78",
            "value": 24957808
          }
        },
        "9cadcc229b4c47dd9581be483bfdcc37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c40cc2da4744e868ff9ac999dd78bf8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3fd8fd786e8543148ca04cf046bf237a",
            "value": "â€‡25.0M/25.0Mâ€‡[00:00&lt;00:00,â€‡108MB/s]"
          }
        },
        "c3faa1a2802245b6ae5939ebe878f99a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4c39e2e28ef47e2aa8a06548395d50a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49a75aa201394996851c676b93fa7a5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1dc2b7a2a4c6415591de96e0226e67f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ffec57b32be441a9dd3984d4b55fd78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c40cc2da4744e868ff9ac999dd78bf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fd8fd786e8543148ca04cf046bf237a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}