{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/has9h/com-sci-7315/blob/main/alfie_refactored_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cf6f21f",
      "metadata": {
        "id": "1cf6f21f"
      },
      "source": [
        "#Cell 01 — Environment, paths, reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wUIfIEo7_0O6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUIfIEo7_0O6",
        "outputId": "ddc3eddc-740a-4ed0-9d30-32672eacb76f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GOEyLl7WDmoG",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOEyLl7WDmoG",
        "outputId": "ada58c48-b8d4-4e56-cf3b-f7a5b1279908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from fvcore) (2.0.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from fvcore) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from fvcore) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.12/dist-packages (from fvcore) (3.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from fvcore) (11.3.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from iopath>=0.1.7->fvcore) (4.14.1)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=075e3479a32abf9bec8cce073e7b5f82f2e193a0f5baa84b228e13db2b01683a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/9f/a5/e4f5b27454ccd4596bd8b62432c7d6b1ca9fa22aef9d70a16a\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=b4459cfdae7264b02b290e1d6454f5655c6ecd77e09abd4d46ca3155d1740e7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/96/04/4f5f31ff812f684f69f40cb1634357812220aac58d4698048c\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-3.2.0 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "!pip install fvcore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "599964ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "599964ba",
        "outputId": "9037366b-1e45-438a-96e3-f23e448f7de5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[Env] Google Drive mounted.\n",
            "[Env] torch=2.8.0+cu126, torchvision=0.23.0+cu126, device=cuda\n",
            "[Env] CSV path → /content/drive/MyDrive/fer2013.csv\n"
          ]
        }
      ],
      "source": [
        "# === Cell 01: Environment, paths, reproducibility ===\n",
        "# Purpose: create deterministic environment; define paths used everywhere; pick device.\n",
        "\n",
        "import os, sys, random, warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def in_colab() -> bool:\n",
        "    return \"google.colab\" in sys.modules\n",
        "\n",
        "# (Optional) Mount Drive if running in Colab so FER csv/ckpts can live there.\n",
        "if in_colab():\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "        print(\"[Env] Google Drive mounted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"[Env][WARN] Drive mount failed: {e}\")\n",
        "\n",
        "# --- project folders\n",
        "PROJECT_ROOT = Path(\"./project\"); PROJECT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "CKPT_DIR     = PROJECT_ROOT / \"checkpoints\"; CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LOG_DIR      = PROJECT_ROOT / \"logs\";        LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- seeds for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# --- deterministic cuDNN (slower but stable)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# --- device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"[Env] torch={torch.__version__}, torchvision={torchvision.__version__}, device={device}\")\n",
        "\n",
        "# --- dataset CSV (edit path if needed)\n",
        "FER_CSV_PATH = Path(\"/content/drive/MyDrive/fer2013.csv\") if in_colab() else Path(\"./fer2013.csv\")\n",
        "print(f\"[Env] CSV path → {FER_CSV_PATH.resolve()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36994e40",
      "metadata": {
        "id": "36994e40"
      },
      "source": [
        "#Cell 02 — Global config (single source of truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54257985",
      "metadata": {
        "id": "54257985"
      },
      "source": [
        "### Cell 02 — Global Configuration (Tuned)\n",
        "\n",
        "| **Category**      | **Key**               | **Value (Default in this cell)** | **Description** |\n",
        "|-------------------|-----------------------|----------------------------------|-----------------|\n",
        "| **I/O**           | `FER_CSV_PATH`        | `./fer2013.csv`                  | Path to FER2013 dataset (CSV). |\n",
        "|                   | `SAVE_BEST_PATH`      | `./project/checkpoints/best_fer.pth` | Filepath to save best model checkpoint. |\n",
        "| **Data**          | `IMG_SIZE`            | `96`                             | Input image size (HxW). |\n",
        "|                   | `BATCH_SIZE`          | `300`                            | Training batch size. |\n",
        "|                   | `NUM_WORKERS`         | `max(2, cpu_count - 2)`          | DataLoader workers for parallelism. |\n",
        "| **Compute**       | `USE_AMP`             | Auto (True if CUDA/MPS available) | Enables Automatic Mixed Precision (AMP). |\n",
        "| **Augmentation**  | `USE_AUG`             | `True`                           | Apply standard augmentations. |\n",
        "|                   | `USE_AUG_ADV`         | `True`                           | Use advanced FER augmentation policy. |\n",
        "|                   | `USE_MIXUP`           | `True`                           | Enable MixUp augmentation. |\n",
        "|                   | `USE_CUTMIX`          | `True`                           | Enable CutMix augmentation. |\n",
        "|                   | `USE_EMA`             | `True`                           | Track EMA weights during training. |\n",
        "|                   | `USE_TTA`             | `True`                           | Use Test-Time Augmentation (test only). |\n",
        "| **Late-phase**    | `AUG_CAP_LATE`        | `True`                           | Cap augmentation strength in later epochs. |\n",
        "|                   | `TAPER_MIX_LATE`      | `True`                           | Gradually reduce MixUp/CutMix late training. |\n",
        "| **Optimiser**     | `USE_SGD`             | `False`                          | Optimizer choice: `False=AdamW`, `True=SGD`. |\n",
        "|                   | `SCHEDULER`           | `\"onecycle\"`                     | LR scheduler type (`onecycle`, `cosine`, `plateau`). |\n",
        "| **Mix/Taper**     | `BASE_MIXUP_PROB`     | `0.45`                           | Initial MixUp probability. |\n",
        "|                   | `BASE_CUTMIX_PROB`    | `0.25`                           | Initial CutMix probability. |\n",
        "|                   | `TAPER_START_FRAC`    | `0.20`                           | Fraction of epochs to start taper. |\n",
        "|                   | `TAPER_END_FRAC`      | `0.85`                           | Fraction of epochs to end taper. |\n",
        "| **Label Smooth**  | `LABEL_SMOOTH_START`  | `0.08`                           | Early-phase label smoothing. |\n",
        "|                   | `LABEL_SMOOTH_END`    | `0.02`                           | Late-phase label smoothing. |\n",
        "| **Fine-tune**     | `FINE_TUNE_FRACTION`  | `0.12`                           | Fraction of total epochs reserved for clean fine-tuning. |\n",
        "\n",
        "---\n",
        "\n",
        "### Hyperparameters (HP)\n",
        "\n",
        "| **Key**            | **Value**  | **Description** |\n",
        "|---------------------|-----------|-----------------|\n",
        "| `EPOCHS`            | `70`      | Total training epochs. |\n",
        "| `LR`                | `3e-4`    | Base learning rate. |\n",
        "| `WD`                | `5e-5`    | Weight decay. |\n",
        "| `EMA_DECAY`         | `0.9995`  | EMA decay factor. |\n",
        "| `AUG_RAMP_EPOCHS`   | `0.40`    | Ramp-up duration for augmentation (fraction of epochs). |\n",
        "| `MIXUP_ALPHA`       | `0.30`    | MixUp Beta distribution parameter. |\n",
        "| `CUTMIX_ALPHA`      | `1.00`    | CutMix Beta distribution parameter. |\n",
        "| `PATIENCE`          | `18`      | Early stopping patience. |\n",
        "| `LR_MIN`            | `5e-5`    | Minimum LR for cosine scheduler. |\n",
        "| `WARMUP_EPOCHS`     | `4`       | Cosine warmup length. |\n",
        "| `PLATEAU_FACTOR`    | `0.5`     | Factor for ReduceLROnPlateau. |\n",
        "| `PLATEAU_PATIENCE`  | `5`       | Patience for ReduceLROnPlateau. |\n",
        "| `MIN_LR`            | `1e-6`    | Min LR for ReduceLROnPlateau. |\n",
        "| `SGD_MOMENTUM`      | `0.9`     | Momentum for SGD. |\n",
        "| `SGD_NESTEROV`      | `True`    | Use Nesterov momentum in SGD. |\n",
        "| `LR_MAX`            | `1e-3`    | Peak LR for OneCycle scheduler. |\n",
        "| `OCL_PCT_START`     | `0.15`    | Fraction of cycle for LR rise (OneCycle). |\n",
        "| `OCL_DIV_FACTOR`    | `12.0`    | Initial LR factor for OneCycle. |\n",
        "| `OCL_FINAL_DIV`     | `20.0`    | Final LR decay for OneCycle. |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1223cbb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1223cbb",
        "outputId": "94b6eea5-f092-48ff-b78e-10947b00da4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[CONFIG]\n",
            "  AUG_CAP_LATE        : True\n",
            "  BASE_CUTMIX_PROB    : 0.25\n",
            "  BASE_MIXUP_PROB     : 0.45\n",
            "  BATCH_SIZE          : 300\n",
            "  FER_CSV_PATH        : /content/drive/MyDrive/fer2013.csv\n",
            "  FINE_TUNE_EPOCHS    : 1\n",
            "  FINE_TUNE_FRACTION  : 0.12\n",
            "  IMG_SIZE            : 96\n",
            "  LABEL_SMOOTH_END    : 0.02\n",
            "  LABEL_SMOOTH_START  : 0.08\n",
            "  NUM_WORKERS         : 10\n",
            "  PRINT_EVERY         : 10\n",
            "  SAVE_BEST_PATH      : project/checkpoints/best_fer.pth\n",
            "  SCHEDULER           : onecycle\n",
            "  TAPER_END_EPOCH     : 9\n",
            "  TAPER_END_FRAC      : 0.85\n",
            "  TAPER_MIX_LATE      : True\n",
            "  TAPER_START_EPOCH   : 2\n",
            "  TAPER_START_FRAC    : 0.2\n",
            "  USE_AMP             : True\n",
            "  USE_AUG             : True\n",
            "  USE_AUG_ADV         : True\n",
            "  USE_CUTMIX          : True\n",
            "  USE_EMA             : True\n",
            "  USE_MIXUP           : True\n",
            "  USE_SGD             : False\n",
            "  USE_TTA             : True\n",
            "\n",
            "[HP]\n",
            "  AUG_RAMP_EPOCHS     : 4\n",
            "  CUTMIX_ALPHA        : 1.0\n",
            "  EMA_DECAY           : 0.9995\n",
            "  EPOCHS              : 11\n",
            "  LR                  : 0.0003\n",
            "  LR_MAX              : 0.001\n",
            "  LR_MIN              : 5e-05\n",
            "  MIN_LR              : 1e-06\n",
            "  MIXUP_ALPHA         : 0.3\n",
            "  OCL_DIV_FACTOR      : 12.0\n",
            "  OCL_FINAL_DIV       : 20.0\n",
            "  OCL_PCT_START       : 0.15\n",
            "  PATIENCE            : 18\n",
            "  PLATEAU_FACTOR      : 0.5\n",
            "  PLATEAU_PATIENCE    : 5\n",
            "  SGD_MOMENTUM        : 0.9\n",
            "  SGD_NESTEROV        : True\n",
            "  WARMUP_EPOCHS       : 4\n",
            "  WD                  : 5e-05\n",
            "\n",
            "[DERIVED]\n",
            "  TAPER_START_EPOCH : 2\n",
            "  TAPER_END_EPOCH   : 9\n",
            "  FINE_TUNE_EPOCHS  : 1\n",
            "  AUG_RAMP_EPOCHS   : 4\n"
          ]
        }
      ],
      "source": [
        "# Purpose: one place to control hyper‑params, toggles, scheduler, and derived numbers.\n",
        "# Notes:\n",
        "# - Added CONFIG[\"PRINT_EVERY\"] so you can control logging stride from config.\n",
        "# - Keep the rest identical to your current setup so other cells don’t break.\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "from pathlib import Path\n",
        "from torch.optim import AdamW, SGD\n",
        "from torch.optim.lr_scheduler import (\n",
        "    ReduceLROnPlateau, CosineAnnealingLR, LinearLR, SequentialLR, OneCycleLR\n",
        ")\n",
        "\n",
        "# Respect upstream paths if defined; otherwise use local defaults\n",
        "FER_CSV_PATH = FER_CSV_PATH if 'FER_CSV_PATH' in globals() else Path(\"./fer2013.csv\")\n",
        "CKPT_DIR     = CKPT_DIR     if 'CKPT_DIR'     in globals() else Path(\"./project/checkpoints\")\n",
        "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _amp_available() -> bool:\n",
        "    mps_ok = getattr(torch.backends, \"mps\", None)\n",
        "    return bool(torch.cuda.is_available() or (mps_ok and torch.backends.mps.is_available()))\n",
        "\n",
        "CONFIG = {\n",
        "    # I/O\n",
        "    \"FER_CSV_PATH\": FER_CSV_PATH,\n",
        "    \"SAVE_BEST_PATH\": CKPT_DIR / \"best_fer.pth\",  # legacy single path (still used by baseline)\n",
        "\n",
        "    # Data\n",
        "    \"IMG_SIZE\": 96,                 # 48 also works; 96 helps MBv3/GhostNet\n",
        "    \"BATCH_SIZE\": 300,\n",
        "    \"NUM_WORKERS\": max(2, (os.cpu_count() or 4) - 2),\n",
        "\n",
        "    # Compute\n",
        "    \"USE_AMP\": _amp_available(),\n",
        "\n",
        "    # Augmentation & evaluation toggles\n",
        "    \"USE_AUG\": True,\n",
        "    \"USE_AUG_ADV\": True,\n",
        "    \"USE_MIXUP\": True,\n",
        "    \"USE_CUTMIX\": True,\n",
        "    \"USE_EMA\": True,\n",
        "    \"USE_TTA\": True,\n",
        "\n",
        "    # Late-phase controls\n",
        "    \"AUG_CAP_LATE\": True,\n",
        "    \"TAPER_MIX_LATE\": True,\n",
        "\n",
        "    # Optimiser/scheduler\n",
        "    \"USE_SGD\": False,               # False=AdamW, True=SGD+Nesterov\n",
        "    \"SCHEDULER\": \"onecycle\",        # 'onecycle' | 'cosine' | 'plateau'\n",
        "\n",
        "    # Mix prob base/taper\n",
        "    \"BASE_MIXUP_PROB\": 0.45,\n",
        "    \"BASE_CUTMIX_PROB\": 0.25,\n",
        "    \"TAPER_START_FRAC\": 0.20,\n",
        "    \"TAPER_END_FRAC\":   0.85,\n",
        "\n",
        "    # Label smoothing\n",
        "    \"LABEL_SMOOTH_START\": 0.08,\n",
        "    \"LABEL_SMOOTH_END\":   0.02,\n",
        "\n",
        "    # Clean fine-tune tail (no mix/aug)\n",
        "    \"FINE_TUNE_FRACTION\": 0.12,\n",
        "\n",
        "    # 🔹 NEW: console logging stride\n",
        "    \"PRINT_EVERY\": 10,              # print every 10 epochs (and on improvement / final)\n",
        "}\n",
        "\n",
        "HP = {\n",
        "    \"EPOCHS\": 11,\n",
        "    \"LR\": 3e-4,\n",
        "    \"WD\": 5e-5,\n",
        "\n",
        "    # EMA\n",
        "    \"EMA_DECAY\": 0.9995,\n",
        "\n",
        "    # Aug/mix schedule\n",
        "    \"AUG_RAMP_EPOCHS\": 0.40,   # fraction of total epochs\n",
        "\n",
        "    \"MIXUP_ALPHA\": 0.30,\n",
        "    \"CUTMIX_ALPHA\": 1.00,\n",
        "\n",
        "    # Early stopping\n",
        "    \"PATIENCE\": 18,\n",
        "\n",
        "    # Cosine path\n",
        "    \"LR_MIN\": 5e-5,\n",
        "    \"WARMUP_EPOCHS\": 4,\n",
        "\n",
        "    # Plateau path\n",
        "    \"PLATEAU_FACTOR\": 0.5,\n",
        "    \"PLATEAU_PATIENCE\": 5,\n",
        "    \"MIN_LR\": 1e-6,\n",
        "\n",
        "    # SGD\n",
        "    \"SGD_MOMENTUM\": 0.9,\n",
        "    \"SGD_NESTEROV\": True,\n",
        "\n",
        "    # One-Cycle\n",
        "    \"LR_MAX\": 1.0e-3,\n",
        "    \"OCL_PCT_START\": 0.15,\n",
        "    \"OCL_DIV_FACTOR\": 12.0,\n",
        "    \"OCL_FINAL_DIV\": 20.0,\n",
        "}\n",
        "\n",
        "# ---- Derived numbers (kept from your notebook) ----\n",
        "E = int(HP[\"EPOCHS\"])\n",
        "assert CONFIG[\"SCHEDULER\"] in (\"plateau\", \"cosine\", \"onecycle\")\n",
        "\n",
        "if HP[\"AUG_RAMP_EPOCHS\"] < 1.0:\n",
        "    HP[\"AUG_RAMP_EPOCHS\"] = max(1, int(round(E * HP[\"AUG_RAMP_EPOCHS\"])))\n",
        "else:\n",
        "    HP[\"AUG_RAMP_EPOCHS\"] = int(HP[\"AUG_RAMP_EPOCHS\"])\n",
        "\n",
        "CONFIG[\"TAPER_START_EPOCH\"] = int(round(CONFIG[\"TAPER_START_FRAC\"] * E))\n",
        "CONFIG[\"TAPER_END_EPOCH\"]   = max(CONFIG[\"TAPER_START_EPOCH\"] + 1, int(round(CONFIG[\"TAPER_END_FRAC\"] * E)))\n",
        "CONFIG[\"FINE_TUNE_EPOCHS\"]  = int(round(CONFIG[\"FINE_TUNE_FRACTION\"] * E))\n",
        "CONFIG[\"TAPER_START_EPOCH\"] = min(CONFIG[\"TAPER_START_EPOCH\"], E - 1)\n",
        "CONFIG[\"TAPER_END_EPOCH\"]   = min(CONFIG[\"TAPER_END_EPOCH\"],   E)\n",
        "CONFIG[\"FINE_TUNE_EPOCHS\"]  = min(CONFIG[\"FINE_TUNE_EPOCHS\"],  max(0, E - 1))\n",
        "\n",
        "# ---- Optimizer/scheduler factories (unchanged) ----\n",
        "def make_optimizer(model):\n",
        "    if CONFIG[\"USE_SGD\"]:\n",
        "        return SGD(model.parameters(), lr=HP[\"LR\"], momentum=HP[\"SGD_MOMENTUM\"],\n",
        "                   nesterov=HP[\"SGD_NESTEROV\"], weight_decay=HP[\"WD\"])\n",
        "    return AdamW(model.parameters(), lr=HP[\"LR\"], weight_decay=HP[\"WD\"])\n",
        "\n",
        "def _make_cosine(optimizer):\n",
        "    warmup_e = max(0, int(HP[\"WARMUP_EPOCHS\"]))\n",
        "    warmup = LinearLR(optimizer, start_factor=1.0, end_factor=1.0, total_iters=max(1, warmup_e))\n",
        "    cosine = CosineAnnealingLR(optimizer, T_max=max(1, E - warmup_e), eta_min=HP[\"LR_MIN\"])\n",
        "    return SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_e]) if warmup_e > 0 else cosine\n",
        "\n",
        "def _make_plateau(optimizer):\n",
        "    return ReduceLROnPlateau(optimizer, mode=\"min\", factor=HP[\"PLATEAU_FACTOR\"],\n",
        "                             patience=HP[\"PLATEAU_PATIENCE\"], min_lr=HP[\"MIN_LR\"], verbose=True)\n",
        "\n",
        "def _make_onecycle(optimizer, steps_per_epoch: int):\n",
        "    return OneCycleLR(optimizer, max_lr=float(HP[\"LR_MAX\"]), epochs=E, steps_per_epoch=int(steps_per_epoch),\n",
        "                      pct_start=float(HP[\"OCL_PCT_START\"]), anneal_strategy=\"cos\",\n",
        "                      cycle_momentum=False, div_factor=float(HP[\"OCL_DIV_FACTOR\"]),\n",
        "                      final_div_factor=float(HP[\"OCL_FINAL_DIV\"]))\n",
        "\n",
        "def build_scheduler(optimizer, steps_per_epoch: int | None = None):\n",
        "    sched = CONFIG[\"SCHEDULER\"]\n",
        "    if sched == \"onecycle\":\n",
        "        if steps_per_epoch is None:\n",
        "            raise ValueError(\"OneCycleLR requires steps_per_epoch; pass len(train_dl).\")\n",
        "        return _make_onecycle(optimizer, steps_per_epoch)\n",
        "    if sched == \"cosine\":\n",
        "        return _make_cosine(optimizer)\n",
        "    if sched == \"plateau\":\n",
        "        return _make_plateau(optimizer)\n",
        "    raise ValueError(f\"Unknown scheduler: {sched}\")\n",
        "\n",
        "def current_lr(optimizer) -> float: return float(optimizer.param_groups[0][\"lr\"])\n",
        "def scheduler_steps_per_batch() -> bool: return CONFIG[\"SCHEDULER\"] == \"onecycle\"\n",
        "\n",
        "def print_config():\n",
        "    print(\"\\n[CONFIG]\");   [print(f\"  {k:20s}: {CONFIG[k]}\") for k in sorted(CONFIG.keys())]\n",
        "    print(\"\\n[HP]\");       [print(f\"  {k:20s}: {HP[k]}\") for k in sorted(HP.keys())]\n",
        "    print(\"\\n[DERIVED]\")\n",
        "    print(f\"  TAPER_START_EPOCH : {CONFIG['TAPER_START_EPOCH']}\")\n",
        "    print(f\"  TAPER_END_EPOCH   : {CONFIG['TAPER_END_EPOCH']}\")\n",
        "    print(f\"  FINE_TUNE_EPOCHS  : {CONFIG['FINE_TUNE_EPOCHS']}\")\n",
        "    print(f\"  AUG_RAMP_EPOCHS   : {HP['AUG_RAMP_EPOCHS']}\")\n",
        "\n",
        "print_config()\n",
        "\n",
        "# --- helper to create unique ckpt path for each backbone (no clashes)\n",
        "def ckpt_path_for(backbone_tag: str) -> Path:\n",
        "    return (CKPT_DIR / f\"best_{backbone_tag}_fer.pth\").resolve()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "883ec470",
      "metadata": {
        "id": "883ec470"
      },
      "source": [
        "#Cell 03 — Load FER2013 and split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_cdznT_YnsPh",
      "metadata": {
        "id": "_cdznT_YnsPh"
      },
      "source": [
        "| **Step**                | **Explanation**                                                                                                                                          |\n",
        "| ----------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Import & Path Setup** | Uses `Path` to ensure `FER_CSV_PATH` is correctly resolved from `CONFIG`. Also includes an assertion to stop execution if the file is missing.           |\n",
        "| **CSV Read**            | Loads the FER2013 dataset via `pd.read_csv(FER_CSV_PATH)`.                                                                                               |\n",
        "| **Schema Check**        | Ensures dataset has at least `[\"emotion\", \"pixels\"]` columns, otherwise throws an error.                                                                 |\n",
        "| **Dataset Splits**      | Splits the dataset into: <br>• **Training**: 28,709 samples <br>• **Validation (PublicTest)**: 3,589 samples <br>• **Test (PrivateTest)**: 3,589 samples |\n",
        "| **Index Reset**         | Each split has `.reset_index(drop=True)` so that indices are clean and independent across splits.                                                        |\n",
        "| **Diagnostics**         | Prints the split sizes and confirms counts (`[Split] train=28709, val=3589, test=3589`).                                                                 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc13acbe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc13acbe",
        "outputId": "cf7edf3b-c438-435b-ad87-d34694cf1ffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage\n",
            "Training       28709\n",
            "PublicTest      3589\n",
            "PrivateTest     3589\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# === Cell 03: Load FER2013 and split ===\n",
        "from pathlib import Path\n",
        "FER_CSV_PATH = Path(CONFIG[\"FER_CSV_PATH\"])\n",
        "assert FER_CSV_PATH.exists(), f\"CSV not found: {FER_CSV_PATH}\"\n",
        "\n",
        "df = pd.read_csv(FER_CSV_PATH)\n",
        "assert {\"emotion\",\"pixels\"}.issubset(set(df.columns)), f\"Bad columns: {df.columns.tolist()}\"\n",
        "print(df[\"Usage\"].value_counts())\n",
        "\n",
        "data_df = df  # Alias for compatibility with reference code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3yB_G51lP9tt",
      "metadata": {
        "id": "3yB_G51lP9tt"
      },
      "source": [
        "### Reference Codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rZujksoEOem5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZujksoEOem5",
        "outputId": "3e50897d-2465-45da-de2b-a88287cd3942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   emotion                                             pixels     Usage\n",
            "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
            "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
            "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
            "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
            "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n",
            "       emotion                                             pixels       Usage\n",
            "28709        0  254 254 254 254 254 249 255 160 2 58 53 70 77 ...  PublicTest\n",
            "28710        1  156 184 198 202 204 207 210 212 213 214 215 21...  PublicTest\n",
            "28711        4  69 118 61 60 96 121 103 87 103 88 70 90 115 12...  PublicTest\n",
            "28712        6  205 203 236 157 83 158 120 116 94 86 155 180 2...  PublicTest\n",
            "28713        3  87 79 74 66 74 96 77 80 80 84 83 89 102 91 84 ...  PublicTest\n",
            "...        ...                                                ...         ...\n",
            "32292        3  0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 3 4 21 40 53 65 ...  PublicTest\n",
            "32293        4  178 176 172 173 173 174 176 173 166 166 206 22...  PublicTest\n",
            "32294        3  25 34 42 44 42 47 57 59 59 58 54 51 50 56 63 6...  PublicTest\n",
            "32295        4  255 255 255 255 255 255 255 255 255 255 255 25...  PublicTest\n",
            "32296        4  33 25 31 36 36 42 69 103 132 163 175 183 187 1...  PublicTest\n",
            "\n",
            "[3588 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "# Categorizing the dataset to three categories.\n",
        "# Training: To train the model.\n",
        "# PrivateTest: To test the train model; commonly known as Validation.\n",
        "# PublicTest: To test the final model on Test set to check how your model perfomed. Do not use this data as your validation data.\n",
        "train_df = data_df[data_df['Usage']=='Training']\n",
        "valid_df = data_df[data_df['Usage']=='PublicTest']\n",
        "test_df = data_df[data_df['Usage']=='PrivateTest']\n",
        "print(train_df.head())\n",
        "print(valid_df.head(-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ear8_B0XOlop",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ear8_B0XOlop",
        "outputId": "dd0bea34-3e01-44d4-dd89-dcb6351e38eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Split] train=28709, val=3589, test=3589\n"
          ]
        }
      ],
      "source": [
        "print(f\"[Split] train={len(train_df)}, val={len(valid_df)}, test={len(test_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l4meIfqRP4cR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4meIfqRP4cR",
        "outputId": "563dfd12-c1f3-4476-d98c-5e97feaebc7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   emotion                                             pixels        Usage\n",
            "0        0  170 118 101 88 88 75 78 82 66 74 68 59 63 64 6...  PrivateTest\n",
            "1        5  7 5 8 6 7 3 2 6 5 4 4 5 7 5 5 5 6 7 7 7 10 10 ...  PrivateTest\n",
            "2        6  232 240 241 239 237 235 246 117 24 24 22 13 12...  PrivateTest\n",
            "3        4  200 197 149 139 156 89 111 58 62 95 113 117 11...  PrivateTest\n",
            "4        2  40 28 33 56 45 33 31 78 152 194 200 186 196 20...  PrivateTest\n",
            "   -----   -------    -------    --------     -----    -------\n",
            "   emotion                                             pixels       Usage\n",
            "0        0  254 254 254 254 254 249 255 160 2 58 53 70 77 ...  PublicTest\n",
            "1        1  156 184 198 202 204 207 210 212 213 214 215 21...  PublicTest\n",
            "2        4  69 118 61 60 96 121 103 87 103 88 70 90 115 12...  PublicTest\n",
            "3        6  205 203 236 157 83 158 120 116 94 86 155 180 2...  PublicTest\n",
            "4        3  87 79 74 66 74 96 77 80 80 84 83 89 102 91 84 ...  PublicTest\n"
          ]
        }
      ],
      "source": [
        "# Test-check to see wether usage labels have been allocated to the dataset/not.\n",
        "valid_df = valid_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "print(test_df.head())\n",
        "print('   -----   -------    -------    --------     -----    -------')\n",
        "print(valid_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28703c52",
      "metadata": {
        "id": "28703c52"
      },
      "source": [
        "# — Dataset (48→96), returns tensor in [0..255], 1×H×W"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OynQ5TCQP8Cg",
      "metadata": {
        "id": "OynQ5TCQP8Cg"
      },
      "outputs": [],
      "source": [
        "# Normalization of the train and validation data.\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "\n",
        "class expressions(Dataset):\n",
        "    def __init__(self, df, transforms=None):\n",
        "        self.df = df\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.loc[index]\n",
        "        image, label = np.array([x.split() for x in self.df.loc[index, ['pixels']]]), row['emotion']\n",
        "        #image = image.reshape(1,48,48)\n",
        "        image = np.asarray(image).astype(np.uint8).reshape(48,48,1)\n",
        "        #image = np.reshape(image,(1,48,48))\n",
        "\n",
        "\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return image.clone().detach(), label\n",
        "\n",
        "#import albumentations as A\n",
        "stats = ([0.5],[0.5])\n",
        "\n",
        "Labels = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']\n",
        "\n",
        "train_tsfm = T.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.Grayscale(num_output_channels=1),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(*stats,inplace=True),\n",
        "])\n",
        "valid_tsfm = T.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.Grayscale(num_output_channels=1),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(*stats,inplace=True)\n",
        "])\n",
        "\n",
        "train_ds = expressions(train_df, train_tsfm)\n",
        "valid_ds = expressions(valid_df, valid_tsfm)\n",
        "test_ds = expressions(test_df, valid_tsfm)\n",
        "val_ds = valid_ds  # Alias for existing code that expects val_ds\n",
        "\n",
        "batch_size = 400\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True,\n",
        "                      num_workers=2, pin_memory=True)\n",
        "valid_dl = DataLoader(valid_ds, batch_size*2,\n",
        "                    num_workers=2, pin_memory=True)\n",
        "test_dl = DataLoader(test_ds, batch_size*2,\n",
        "                    num_workers=2, pin_memory=True)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "input_size = 48*48\n",
        "output_size = len(Labels)\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    predictions, preds = torch.max(output, dim=1)\n",
        "    return torch.tensor(torch.sum(preds==labels).item()/len(preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21641599",
      "metadata": {
        "id": "21641599"
      },
      "source": [
        "### Cell — Expression Model Class\n",
        "\n",
        "This cell defines the `expression_model` class, a custom subclass of `torch.nn.Module`, which encapsulates the training and validation workflow for facial expression recognition.\n",
        "\n",
        "**Key Responsibilities:**\n",
        "- **`training_step(batch)`**  \n",
        "  - Runs a forward pass on a training batch.  \n",
        "  - Computes and returns the **cross-entropy loss** for optimization.  \n",
        "\n",
        "- **`validation_step(batch)`**  \n",
        "  - Evaluates the model on a validation batch.  \n",
        "  - Returns both **validation loss** and **accuracy** as a dictionary.  \n",
        "\n",
        "- **`validation_epoch_end(outputs)`**  \n",
        "  - Aggregates validation metrics across all batches in an epoch.  \n",
        "  - Computes the **mean loss** and **mean accuracy** for the full epoch.  \n",
        "\n",
        "- **`epoch_end(epoch, result)`**  \n",
        "  - Logs the epoch number, validation loss, and validation accuracy in a formatted string.  \n",
        "\n",
        "**Why this structure?**  \n",
        "- Separates training and validation logic cleanly.  \n",
        "- Makes the training loop simpler, since loss computation, metric calculation, and logging are encapsulated in the class.  \n",
        "- Ensures validation metrics are detached from gradients to save memory.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JvyBvhPpTASX",
      "metadata": {
        "id": "JvyBvhPpTASX"
      },
      "outputs": [],
      "source": [
        "# Expression model class for training and validation purpose.\n",
        "\n",
        "class expression_model(nn.Module):\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)\n",
        "        loss = F.cross_entropy(out, labels)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)\n",
        "        loss = F.cross_entropy(out, labels)\n",
        "        acc = accuracy(out, labels)\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()\n",
        "        batch_acc = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_acc).mean()\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "\n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch[{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7TBBlepTA8K",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7TBBlepTA8K",
        "outputId": "db9ed33a-1cf5-41c7-9348-4f78a99dfeb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are training on: cuda.\n"
          ]
        }
      ],
      "source": [
        "# To check wether Google Colab GPU has been assigned/not.\n",
        "torch.cuda.is_available()\n",
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "device = get_default_device()\n",
        "print(f'You are training on: {device}.')\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nyyv0X9ATFsR",
      "metadata": {
        "id": "nyyv0X9ATFsR"
      },
      "outputs": [],
      "source": [
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "valid_dl = DeviceDataLoader(valid_dl, device)\n",
        "test_dl = DeviceDataLoader(test_dl, device)\n",
        "\n",
        "val_dl = valid_dl  # Alias for existing code that expects val_dl\n",
        "\n",
        "baseline_train_dl = train_dl  # Reference code training DataLoader (48x48, DeviceDataLoader wrapped)\n",
        "baseline_valid_dl = valid_dl  # Reference code validation DataLoader (48x48, DeviceDataLoader wrapped)\n",
        "baseline_test_dl = test_dl    # Reference code test DataLoader (48x48, DeviceDataLoader wrapped)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9676d27f",
      "metadata": {
        "id": "9676d27f"
      },
      "source": [
        "#BASE LAYER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zM_d5oNZTGUD",
      "metadata": {
        "id": "zM_d5oNZTGUD"
      },
      "outputs": [],
      "source": [
        "# Model - 7 layer\n",
        "class expression(expression_model):\n",
        "    def __init__(self,classes):\n",
        "        super().__init__()\n",
        "        self.num_classes = classes\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=3, padding=1),  #(input channels, output channels)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(8, 32, kernel_size=3, padding=1),  #(input channels, output channels)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 64 x 24 x 24\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 128 x 12 x 12\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 256 x 6 x 6\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256*6*6, 2304),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2304, 1152),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1152, 576),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(576,288),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(288,144),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(144,self.num_classes))\n",
        "\n",
        "    def forward(self, xb):\n",
        "        return self.network(xb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zpm7mwdI_N8i",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zpm7mwdI_N8i",
        "outputId": "438e9533-9a31-49a8-9a7a-84d850140eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== BASELINE MODEL EXPERIMENT (Reference Code) ===\n",
            "[Baseline Probe] logits=(800, 7), loss=1.9523\n",
            "=== END BASELINE MODEL EXPERIMENT ===\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== BASELINE MODEL EXPERIMENT (Reference Code) ===\")\n",
        "baseline_model = expression(7)\n",
        "baseline_model.train()\n",
        "baseline_model = to_device(baseline_model, device)\n",
        "# Use reference code data loaders for baseline model\n",
        "xb_baseline, yb_baseline = next(iter(valid_dl))  # Use reference code DataLoader\n",
        "with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=CONFIG[\"USE_AMP\"]):\n",
        "    logits_baseline = baseline_model(xb_baseline)\n",
        "    loss_baseline = F.cross_entropy(logits_baseline, yb_baseline)\n",
        "loss_baseline.backward(); baseline_model.zero_grad(set_to_none=True)\n",
        "print(f\"[Baseline Probe] logits={tuple(logits_baseline.shape)}, loss={loss_baseline.item():.4f}\")\n",
        "print(\"=== END BASELINE MODEL EXPERIMENT ===\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yrTm7wQIArSu",
      "metadata": {
        "id": "yrTm7wQIArSu"
      },
      "source": [
        "#======baseline example given in the exam notebook few epoch trial run===================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Tztupbng-9Jr",
      "metadata": {
        "id": "Tztupbng-9Jr"
      },
      "outputs": [],
      "source": [
        "# RUN_BASELINE_EXPERIMENT = True\n",
        "\n",
        "# if RUN_BASELINE_EXPERIMENT:\n",
        "#     print(\"=== TRAINING REFERENCE CODE BASELINE MODEL ===\")\n",
        "#     baseline_model = expression(7)\n",
        "#     baseline_model = to_device(baseline_model, device)\n",
        "\n",
        "#     # Create a simple training function for baseline model using reference code methods\n",
        "#     def train_baseline_model(model, train_loader, valid_loader, epochs=10):\n",
        "#         optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "#         best_acc = 0.0\n",
        "\n",
        "#         for epoch in range(epochs):\n",
        "#             # ---------- Training ----------\n",
        "#             model.train()\n",
        "#             train_loss = 0.0\n",
        "#             n_batches = 0\n",
        "#             for batch in train_loader:\n",
        "#                 loss = model.training_step(batch)\n",
        "#                 optimizer.zero_grad()\n",
        "#                 loss.backward()\n",
        "#                 optimizer.step()\n",
        "#                 train_loss += loss.item()\n",
        "#                 n_batches += 1\n",
        "\n",
        "#             avg_train_loss = train_loss / max(1, n_batches)\n",
        "\n",
        "#             # ---------- Validation ----------\n",
        "#             model.eval()\n",
        "#             val_outputs = []\n",
        "#             with torch.no_grad():\n",
        "#                 for batch in valid_loader:\n",
        "#                     val_out = model.validation_step(batch)\n",
        "#                     val_outputs.append(val_out)\n",
        "\n",
        "#             val_result = model.validation_epoch_end(val_outputs)\n",
        "\n",
        "#             # Epoch summary (Epoch i/N style)\n",
        "#             print(f\"Epoch {epoch+1}/{epochs}, \"\n",
        "#                   f\"train_loss: {avg_train_loss:.4f}, \"\n",
        "#                   f\"val_loss: {val_result['val_loss']:.4f}, \"\n",
        "#                   f\"val_acc: {val_result['val_acc']:.4f}\")\n",
        "\n",
        "#             # Keep original per-epoch hook\n",
        "#             model.epoch_end(epoch, val_result)\n",
        "\n",
        "#             # Save best baseline model\n",
        "#             if val_result['val_acc'] > best_acc:\n",
        "#                 best_acc = val_result['val_acc']\n",
        "#                 torch.save(\n",
        "#                     {\"model_state\": model.state_dict()},\n",
        "#                     CONFIG[\"SAVE_BEST_PATH\"].parent / \"best_baseline_model.pth\"\n",
        "#                 )\n",
        "\n",
        "#         return best_acc\n",
        "\n",
        "#     print(f\"[Baseline] Using reference code DataLoaders - \"\n",
        "#           f\"train batches: {len(baseline_train_dl)}, valid batches: {len(baseline_valid_dl)}\")\n",
        "\n",
        "#     # --- Run for 10 full epochs (Epoch 0..9) ---\n",
        "#     best_baseline_acc = train_baseline_model(\n",
        "#         baseline_model, baseline_train_dl, baseline_valid_dl, epochs=10\n",
        "#     )\n",
        "\n",
        "#     print(f\"Best baseline model accuracy: {best_baseline_acc:.4f}\")\n",
        "#     print(\"=== BASELINE MODEL TRAINING COMPLETE ===\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08lTUI2OAYhb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08lTUI2OAYhb",
        "outputId": "6ad7da98-7815-4869-ded6-16e7bef9a735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 3 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs: 0.32751 GFLOPs\n"
          ]
        }
      ],
      "source": [
        "from fvcore.nn import FlopCountAnalysis\n",
        "input = torch.randn(1, 1, 48, 48) # The input size should be the same as the size that you put into your model\n",
        "#Get the network and its FLOPs\n",
        "num_classes = 7\n",
        "model = expression(num_classes)\n",
        "flops = FlopCountAnalysis(model, input)\n",
        "print(f\"FLOPs: {flops.total()/1e9:.5f} GFLOPs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kNEV3fCI-IEo"
      },
      "id": "kNEV3fCI-IEo"
    },
    {
      "cell_type": "markdown",
      "id": "fQSWoD1__R0c",
      "metadata": {
        "id": "fQSWoD1__R0c"
      },
      "source": [
        "#=========ANYTHING TO BASELINE ENDED====================="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2NWP8JxzIVm7",
      "metadata": {
        "id": "2NWP8JxzIVm7"
      },
      "source": [
        "### End of Reference Code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae77ae7c",
      "metadata": {
        "id": "ae77ae7c"
      },
      "source": [
        "### Cell 04 — Dataset Logic and Rationale\n",
        "\n",
        "**Logic Flow:**\n",
        "1. **Custom Dataset Class (`FER2013Dataset`)**\n",
        "   - Inherits from `torch.utils.data.Dataset`.\n",
        "   - Reads the FER2013 CSV rows:  \n",
        "     - `pixels`: a string of 2304 numbers → converted to a NumPy array → reshaped into `[48,48]`.\n",
        "     - `emotion`: integer label (0–6).\n",
        "   - Converts to a tensor `[1,48,48]` and rescales to the desired resolution `[1,H,W]` using bilinear interpolation.\n",
        "   - Returns `(image, label)` pairs compatible with PyTorch training.\n",
        "\n",
        "2. **Hybrid vs. Reference Datasets**\n",
        "   - **HybridEffNet datasets**: Resized to **96×96** for better alignment with EfficientNet backbone (higher receptive field, more representational power).\n",
        "   - **Reference code datasets**: Kept at **48×48** as a baseline (original FER2013 format, lighter compute).\n",
        "\n",
        "3. **Default Assignment**\n",
        "   - The default datasets (`train_ds`, `val_ds`, `test_ds`) are mapped to the **96×96 HybridEffNet versions**, so the rest of the training pipeline runs on the stronger configuration.\n",
        "   - The original 48×48 versions are still available for controlled comparisons and sanity checks.\n",
        "\n",
        "---\n",
        "\n",
        "**Why This Design?**\n",
        "- **Resizing 48→96:** EfficientNet architectures were designed for larger input resolutions; upscaling preserves pipeline compatibility and generally boosts accuracy (at the cost of more FLOPs).\n",
        "- **Keeping Both Versions:** Retaining 48×48 alongside 96×96 allows quick ablation studies (accuracy vs. efficiency trade-off).\n",
        "- **Encapsulation in Class:** All preprocessing logic is encapsulated inside `FER2013Dataset`, keeping the training loop clean and flexible.\n",
        "- **Future-proofing:** Switching resolution is controlled via a single parameter (`img_size`), making experiments reproducible and less error-prone.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c93cf74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c93cf74",
        "outputId": "2b5628fa-1ae4-48f1-f6bf-6c9a45cd52c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Dataset] HybridEffNet datasets (96x96) and reference code datasets (48x48) both ready.\n"
          ]
        }
      ],
      "source": [
        "# === Cell 04: Dataset (48→96), returns [1,H,W] in 0..255 float ===\n",
        "import torchvision.transforms.functional as VF\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class FER2013Dataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, img_size: int = 96):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_size = int(img_size)\n",
        "        if len(self.df) > 0:\n",
        "            _ = self._get_x(0)\n",
        "\n",
        "    def _get_x(self, i: int) -> torch.Tensor:\n",
        "        px = self.df.iloc[i][\"pixels\"]\n",
        "        arr = np.fromstring(str(px), sep=\" \", dtype=np.float32)\n",
        "        arr = np.array(str(px).split(), dtype=np.float32)\n",
        "        assert arr.size == 48*48, f\"Row {i}: expected 2304 pixels, got {arr.size}\"\n",
        "        x = torch.from_numpy(arr.reshape(48, 48)).unsqueeze(0)  # [1,48,48], float32 in 0..255\n",
        "        x = VF.resize(\n",
        "            x,\n",
        "            [self.img_size, self.img_size],\n",
        "            interpolation=torchvision.transforms.InterpolationMode.BILINEAR,\n",
        "            antialias=True,\n",
        "        )\n",
        "        return x\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        x = self._get_x(i).contiguous()  # [1,H,W], float32 0..255\n",
        "        y = int(self.df.iloc[i][\"emotion\"])\n",
        "        return x, y\n",
        "\n",
        "# Create HybridEffNet datasets (96x96)\n",
        "IMG_SIZE = int(CONFIG[\"IMG_SIZE\"])\n",
        "train_ds_hybrid = FER2013Dataset(train_df, IMG_SIZE)\n",
        "val_ds_hybrid   = FER2013Dataset(valid_df,   IMG_SIZE)\n",
        "test_ds_hybrid  = FER2013Dataset(test_df,  IMG_SIZE)\n",
        "\n",
        "# Keep reference code datasets available for baseline experiments (48x48)\n",
        "# train_ds, valid_ds, test_ds from reference code are still available above\n",
        "\n",
        "# Set default datasets to HybridEffNet versions for main training\n",
        "train_ds = train_ds_hybrid\n",
        "val_ds = val_ds_hybrid\n",
        "test_ds = test_ds_hybrid\n",
        "\n",
        "print(\"[Dataset] HybridEffNet datasets (96x96) and reference code datasets (48x48) both ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fc72637",
      "metadata": {
        "id": "3fc72637"
      },
      "source": [
        "#Cell 05 — DataLoaders"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ywuLnWmpLnz",
      "metadata": {
        "id": "1ywuLnWmpLnz"
      },
      "source": [
        "| **Aspect**             | **Design Choice**             | **Rationale / Benefit**                                                  |\n",
        "| ---------------------- | ----------------------------- | ------------------------------------------------------------------------ |\n",
        "| **Batch size**         | `BATCH` from config           | Flexible tuning; balances GPU memory usage and throughput.               |\n",
        "| **Workers**            | `NUM_WORKERS` from config     | Parallel data loading → faster pipeline.                                 |\n",
        "| **Pin memory**         | Enabled if CUDA available     | Faster CPU→GPU transfer, avoids page faults.                             |\n",
        "| **Training loader**    | Shuffle=True, batch=BATCH     | Randomized sampling prevents bias, improves generalization.              |\n",
        "| **Validation loader**  | Shuffle=False, batch=2×BATCH  | Deterministic eval + speedup with larger batches.                        |\n",
        "| **Test loader**        | Same as validation            | Ensures reproducible benchmarking, efficient evaluation.                 |\n",
        "| **Persistent workers** | Kept alive if workers>0       | Reduces overhead of re-spawning processes per epoch.                     |\n",
        "| **Sanity check**       | Print val batch shape + range | Debug safeguard: verifies `[N,1,96,96]` shape and pixel range `[0,255]`. |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed12317c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed12317c",
        "outputId": "b8d60c81-ba7d-4e3d-9650-8d55db18ff75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DataLoaders] HybridEffNet DataLoaders (96x96) and reference code DataLoaders (48x48) both ready.\n",
            "[Check] val batch: torch.Size([600, 1, 96, 96]), range [0.0,255.0]\n"
          ]
        }
      ],
      "source": [
        "# === Cell 05: DataLoaders ===\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH = int(CONFIG[\"BATCH_SIZE\"])\n",
        "NUM_WORKERS = int(CONFIG[\"NUM_WORKERS\"])\n",
        "PIN = bool(torch.cuda.is_available())  # safer on CPU-only runs\n",
        "\n",
        "train_dl_hybrid = DataLoader(\n",
        "    train_ds, batch_size=BATCH, shuffle=True,\n",
        "    num_workers=NUM_WORKERS, pin_memory=PIN, persistent_workers=(NUM_WORKERS>0)\n",
        ")\n",
        "val_dl_hybrid   = DataLoader(\n",
        "    val_ds,   batch_size=BATCH*2, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=PIN, persistent_workers=(NUM_WORKERS>0)\n",
        ")\n",
        "test_dl_hybrid  = DataLoader(\n",
        "    test_ds,  batch_size=BATCH*2, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=PIN, persistent_workers=(NUM_WORKERS>0)\n",
        ")\n",
        "\n",
        "# Set default DataLoaders to HybridEffNet versions for main training\n",
        "train_dl = train_dl_hybrid\n",
        "val_dl = val_dl_hybrid\n",
        "test_dl = test_dl_hybrid\n",
        "\n",
        "# Reference code DataLoaders (train_dl, valid_dl, test_dl from DeviceDataLoader) still available for baseline experiments\n",
        "\n",
        "print(\"[DataLoaders] HybridEffNet DataLoaders (96x96) and reference code DataLoaders (48x48) both ready.\")\n",
        "\n",
        "xb, yb = next(iter(val_dl))\n",
        "print(f\"[Check] val batch: {xb.shape}, range [{xb.min():.1f},{xb.max():.1f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb43e1aa",
      "metadata": {
        "id": "bb43e1aa"
      },
      "source": [
        "#Cell 06 — Advanced augmentation primitives (photometric, geometric, occlusion, elastic)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7860cfeb",
      "metadata": {
        "id": "7860cfeb"
      },
      "source": [
        "### Cell 06 — Advanced Augmentation Primitives (Grayscale)\n",
        "\n",
        "**Logic:**\n",
        "- The FER2013 dataset is grayscale (single channel, 48×48 upscaled to 96×96).  \n",
        "- Standard color augmentations (hue, saturation, color jitter) do not apply.  \n",
        "- Instead, this cell builds a **custom augmentation library** specialized for grayscale images.  \n",
        "- Functions are organized into **photometric**, **geometric**, and **elastic/occlusion** categories, all operating directly on `[1,H,W]` tensors in the range `[0..255]`.\n",
        "\n",
        "---\n",
        "\n",
        "**Reasoning Behind Each Category:**\n",
        "\n",
        "1. **Photometric (intensity-based changes)**  \n",
        "   - `gauss_noise`: Adds Gaussian noise to simulate sensor noise.  \n",
        "   - `rand_gamma`: Adjusts brightness curve (mimics lighting changes).  \n",
        "   - `rand_contrast`: Scales contrast around image mean.  \n",
        "   - `rand_equalize`: Histogram equalization for balanced brightness.  \n",
        "   - `rand_jpeg`: Simulates compression artifacts (robustness to real-world data).  \n",
        "   - `rand_vignette`: Darkens edges, mimicking lens/camera imperfections.  \n",
        "   - `rand_blur`: Blurs images in a range-safe way (helps model learn robustness to soft focus).\n",
        "\n",
        "   *Why:* Facial expression datasets are sensitive to lighting/contrast variations — simulating these improves generalization.\n",
        "\n",
        "2. **Geometric (spatial distortions)**  \n",
        "   - `rand_affine_small`: Small random rotations, translations, scaling, and shears.  \n",
        "   - `rand_pad_crop`: Random shifts via reflective padding and cropping.  \n",
        "   - `rand_hflip`: Horizontal flip with 50% chance (mimics left/right symmetry of faces).  \n",
        "\n",
        "   *Why:* Expressions should be invariant to slight geometric shifts (head tilt, position in frame).\n",
        "\n",
        "3. **Elastic (local warps and occlusion)**  \n",
        "   - `rand_elastic`: Smooth, random pixel-level displacements, mimicking distortions in facial tissue/expressions.  \n",
        "   - **(occlusion placeholder):** Later additions might cover partial masking (hands, glasses, etc.).\n",
        "\n",
        "   *Why:* Faces in the wild often have small deformations or obstructions; elastic deformations help the model handle them.\n",
        "\n",
        "---\n",
        "\n",
        "**Overall Reasoning:**\n",
        "- **Diversity without color bias:** Since images are grayscale, augmentations target intensity and shape rather than color channels.  \n",
        "- **Robustness to real-world variance:** JPEG artifacts, vignettes, noise, and blur simulate common low-quality capture scenarios.  \n",
        "- **Balanced difficulty:** Small ranges are chosen (e.g., `max_rot=12°`) so augmentations improve generalization but don’t destroy key expression features.  \n",
        "- **Modularity:** Each function can be called independently or assembled into augmentation pipelines (used later in Cell 07).\n",
        "\n",
        "**Key Principle:**  \n",
        "These augmentations make the model *less brittle* by forcing it to learn invariant features of expressions, rather than overfitting to perfect, centered, noise-free 48×48 FER images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cba5bd2",
      "metadata": {
        "id": "0cba5bd2"
      },
      "outputs": [],
      "source": [
        "# === Cell 06: Advanced augmentation primitives (grayscale) ===\n",
        "import io\n",
        "from PIL import Image, ImageOps\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def _to_pil_gray(x255: torch.Tensor) -> Image.Image:\n",
        "    x = x255.clamp(0,255).to(torch.uint8).squeeze(0).cpu().numpy()\n",
        "    return Image.fromarray(x, mode='L')\n",
        "\n",
        "def _from_pil_gray(img: Image.Image) -> torch.Tensor:\n",
        "    return torch.tensor(np.array(img, dtype=np.uint8), dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "def _clip(x): return x.clamp(0.0, 255.0)\n",
        "\n",
        "# photometric\n",
        "def gauss_noise(x, sigma=0.02): return _clip(x + torch.randn_like(x)*(sigma*255.))\n",
        "def rand_gamma(x, gmin=0.85, gmax=1.25):\n",
        "    g = random.uniform(gmin,gmax); x01=(x/255.).clamp(0,1); return (x01**g)*255.\n",
        "def rand_contrast(x, scale=0.25):\n",
        "    c = 1.0+random.uniform(-scale,scale); m=x.mean(dim=(1,2),keepdim=True); return _clip((x-m)*c+m)\n",
        "def rand_equalize(x):\n",
        "    img=_to_pil_gray(x); img=ImageOps.equalize(img); return _from_pil_gray(img).to(x.dtype).to(x.device)\n",
        "def rand_jpeg(x, qmin=55, qmax=85):\n",
        "    img=_to_pil_gray(x); buf=io.BytesIO(); img.save(buf,format='JPEG',quality=random.randint(qmin,qmax))\n",
        "    buf.seek(0); img2=Image.open(buf).convert('L'); return _from_pil_gray(img2).to(x.dtype).to(x.device)\n",
        "def rand_vignette(x, strength=0.25):\n",
        "    _,H,W=x.shape; yy,xx=torch.meshgrid(torch.linspace(-1,1,H,device=x.device),\n",
        "                                        torch.linspace(-1,1,W,device=x.device),indexing='ij')\n",
        "    r=torch.sqrt(xx**2+yy**2); mask=1.0-strength*(r/r.max()).clamp(0,1)\n",
        "    return _clip(x*mask.unsqueeze(0))\n",
        "\n",
        "# Range-safe blur: convert to [0,1] → blur → back to [0,255]\n",
        "def rand_blur(x, k=3):\n",
        "    x01 = (x/255.).clamp(0,1)\n",
        "    y01 = torchvision.transforms.functional.gaussian_blur(x01, kernel_size=k)\n",
        "    return (y01 * 255.0).clamp(0,255)\n",
        "\n",
        "# geometric\n",
        "def rand_affine_small(x, max_rot=12, max_trans=0.08, max_shear=8.0, max_scale=0.08):\n",
        "    H,W=x.shape[-2:]\n",
        "    angle=random.uniform(-max_rot,max_rot)\n",
        "    trans=[int(random.uniform(-max_trans,max_trans)*W),int(random.uniform(-max_trans,max_trans)*H)]\n",
        "    scale=1.0+random.uniform(-max_scale,max_scale)\n",
        "    shear=[random.uniform(-max_shear,max_shear),0.0]\n",
        "    return torchvision.transforms.functional.affine(x, angle=angle, translate=trans, scale=scale, shear=shear)\n",
        "\n",
        "def rand_pad_crop(x, pad=3):\n",
        "    _,H,W=x.shape; xpad=F.pad(x,(pad,pad,pad,pad),mode='reflect'); i=random.randint(0,2*pad); j=random.randint(0,2*pad)\n",
        "    return xpad[:,i:i+H, j:j+W]\n",
        "\n",
        "def rand_hflip(x, p=0.5): return torchvision.transforms.functional.hflip(x) if random.random()<p else x\n",
        "\n",
        "# elastic\n",
        "def rand_elastic(x, alpha=1.0, sigma=4.0):\n",
        "    _,H,W=x.shape\n",
        "    def _gkern(k=21,s=sigma):\n",
        "        ax=torch.arange(k,device=x.device)-(k-1)/2; ker=torch.exp(-(ax**2)/(2*s*s)); ker/=ker.sum(); return ker\n",
        "    k=21; gx=_gkern(k).view(1,1,1,k); gy=_gkern(k).view(1,1,k,1)\n",
        "    dx=F.conv2d(F.conv2d(torch.randn(1,1,H,W,device=x.device),gx,padding=(0,k//2)),gy,padding=(k//2,0)).squeeze()*alpha\n",
        "    dy=F.conv2d(F.conv2d(torch.randn(1,1,H,W,device=x.device),gx,padding=(0,k//2)),gy,padding=(k//2,0)).squeeze()*alpha\n",
        "    yy,xx=torch.meshgrid(torch.linspace(-1,1,H,device=x.device),\n",
        "                         torch.linspace(-1,1,W,device=x.device),indexing='ij')\n",
        "    xx=(xx+dx/(W/2)).clamp(-1,1); yy=(yy+dy/(H/2)).clamp(-1,1)\n",
        "    grid=torch.stack([xx,yy],dim=-1).unsqueeze(0)\n",
        "    return F.grid_sample(x.unsqueeze(0), grid, mode='bilinear', padding_mode='border', align_corners=True).squeeze(0)\n",
        "\n",
        "# occlusio\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65ce3ae2",
      "metadata": {
        "id": "65ce3ae2"
      },
      "source": [
        "### Cell 06.x — Augmentation Utility: Band Occlusion\n",
        "\n",
        "**Logic:**\n",
        "- The function creates **horizontal band occlusions** across specific facial regions (eyes, mouth, top, bottom, or middle).\n",
        "- It modifies a clone of the input tensor `[C,H,W]` by replacing the chosen band with a neutral fill value that matches the input range.\n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "1. **Range Awareness**\n",
        "   - `_neutral_fill_value_range_aware(x)` ensures the fill matches the input image’s scale:\n",
        "     - `[-1,1]` → fill = `0.0` (neutral gray).\n",
        "     - `[0,1]` → fill = `0.5`.\n",
        "     - `[0,255]` → fill = `127.5`.\n",
        "   - This avoids introducing artificial biases (too bright or too dark occlusions).\n",
        "\n",
        "2. **Band Placement**\n",
        "   - Band height = `frac × H` (e.g., `0.18 × H` → ~18% of image height).\n",
        "   - Vertical anchor depends on `mode`:\n",
        "     - **eyes**: ~30% down from top.\n",
        "     - **mouth**: ~72% down.\n",
        "     - **mid**: centered at 50%.\n",
        "     - **top**: near forehead region (~10%).\n",
        "     - **bottom**: near chin (~85%).\n",
        "     - **default**: random vertical placement.\n",
        "\n",
        "3. **Application**\n",
        "   - The chosen band is overwritten with the neutral fill.\n",
        "   - Output = occluded tensor, same shape as input.\n",
        "\n",
        "---\n",
        "\n",
        "**Reasoning:**\n",
        "- **Why band occlusion?**  \n",
        "  Facial expressions are often localized (eyes, mouth). By masking these regions, the model is forced to rely on *global context* and not overfit to single regions.\n",
        "  \n",
        "- **Why multiple modes?**  \n",
        "  - `eyes` occlusion: tests robustness to missing eye cues.  \n",
        "  - `mouth` occlusion: tests robustness to missing mouth cues.  \n",
        "  - `mid/top/bottom`: adds variability, simulating accessories (scarves, masks, hats).  \n",
        "\n",
        "- **Why neutral fill?**  \n",
        "  Keeps occlusion consistent and range-safe, so the augmentation doesn’t introduce unrealistic artifacts (e.g., black bars that act as shortcuts).\n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "This augmentation simulates real-world occlusions (glasses glare, masks, hands) and forces the model to learn **distributed representations** of emotion, improving robustness and generalization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pBRdqZrNfOQO",
      "metadata": {
        "id": "pBRdqZrNfOQO"
      },
      "outputs": [],
      "source": [
        "# === Cell 06.x: Aug Utils — band_occlusion  ===\n",
        "import torch\n",
        "import random\n",
        "\n",
        "def _neutral_fill_value_range_aware(x: torch.Tensor) -> float:\n",
        "    \"\"\"\n",
        "    Mid-gray fill compatible with [-1,1], [0,1], or [0,255] ranges.\n",
        "    \"\"\"\n",
        "    xmin = float(x.min())\n",
        "    xmax = float(x.max())\n",
        "    if xmin >= -1.0 and xmax <= 1.0:\n",
        "        return 0.0\n",
        "    if xmin >= 0.0 and xmax <= 1.0:\n",
        "        return 0.5\n",
        "    if xmax > 1.0:\n",
        "        return 127.5\n",
        "    return 0.0\n",
        "\n",
        "def band_occlusion(img: torch.Tensor, mode: str = 'eyes', frac: float = 0.18) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Draw a horizontal band occlusion corresponding to a face region.\n",
        "    Args:\n",
        "        img: [C,H,W] tensor (float). Accepted ranges: [-1,1], [0,1], [0,255].\n",
        "        mode: 'eyes' | 'mouth' | 'top' | 'bottom' | 'mid'\n",
        "        frac: vertical band height as fraction of H.\n",
        "    Returns:\n",
        "        Tensor with an occluded band (on a clone).\n",
        "    \"\"\"\n",
        "    if not (torch.is_tensor(img) and img.ndim == 3):\n",
        "        raise TypeError(\"band_occlusion expects a tensor of shape [C,H,W].\")\n",
        "\n",
        "    C, H, W = img.shape\n",
        "    band_h = max(1, int(H * float(frac)))\n",
        "    fill = _neutral_fill_value_range_aware(img)\n",
        "    out = img.clone()\n",
        "\n",
        "    # Default anchors (approximate facial landmarks for FER crops)\n",
        "    if mode == 'eyes':\n",
        "        top = int(0.30 * H) - band_h // 2\n",
        "    elif mode == 'mouth':\n",
        "        top = int(0.72 * H) - band_h // 2\n",
        "    elif mode == 'mid':\n",
        "        top = int(0.50 * H) - band_h // 2\n",
        "    elif mode == 'top':\n",
        "        top = int(0.10 * H)\n",
        "    elif mode == 'bottom':\n",
        "        top = int(0.85 * H) - band_h\n",
        "    else:\n",
        "        # Fallback: random vertical placement\n",
        "        top = random.randint(0, max(0, H - band_h))\n",
        "\n",
        "    top = max(0, min(top, H - band_h))\n",
        "    out[:, top:top + band_h, :] = fill\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6602b98",
      "metadata": {
        "id": "a6602b98"
      },
      "source": [
        "#Cell 07 — AugMix‑lite and advanced augmentation builder (returns [-1,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c7c9d63",
      "metadata": {
        "id": "3c7c9d63"
      },
      "source": [
        "### Cell 07 — AugMix-lite and Advanced FER Augmentation (→ [-1,1])\n",
        "\n",
        "**Logic:**\n",
        "- Builds a *configurable augmentation pipeline* for FER2013 images, combining **photometric, geometric, and occlusion** augmentations with **AugMix-lite** for diversity.\n",
        "- Normalizes final outputs into the `[-1,1]` range for model input compatibility.\n",
        "- Uses a strength parameter (`0.0–1.0`) to scale probabilities and magnitudes of augmentations.\n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "1. **AugMix-lite core**\n",
        "   - `augmix_lite`: Blends the original image with multiple augmentation “branches.”\n",
        "   - Each branch applies random augmentations from different banks, repeated `depth` times.\n",
        "   - The mixed result = weighted average of original + augmented images → prevents overfitting to one distorted view.\n",
        "\n",
        "2. **Probability scheduling (`build_advanced_fer_augment`)**\n",
        "   - Probabilities (chance of applying a transform) scale with `strength`:\n",
        "     - Photometric (`p_photo`): up to 70% chance.\n",
        "     - Geometric (`p_geom`): up to 60%.\n",
        "     - Occlusion (`p_occl`): up to 40%.\n",
        "     - Histogram equalization (`p_equal`): up to 20%.\n",
        "     - Blur (`p_blur`): up to 15%.\n",
        "   - Magnitudes also scale: gamma range, contrast, JPEG quality, vignette strength, elastic deformation amplitude, rotation/translation/shear/scale bounds.\n",
        "\n",
        "3. **Augmentation banks**\n",
        "   - **Photometric bank**: noise, gamma correction, contrast shift, JPEG artifacts, vignette.\n",
        "   - **Geometric bank**: affine transforms, pad+crop jitter, horizontal flip, elastic warps.\n",
        "   - **Occlusion bank**: band occlusion (eyes, mouth, top) and localized erasing.\n",
        "\n",
        "4. **Final augment function**\n",
        "   - Applies a *probabilistic sequence*:\n",
        "     1. Pad+crop (jitter).\n",
        "     2. Optional blur.\n",
        "     3. Random photometric transform.\n",
        "     4. AugMix-lite (mix across banks).\n",
        "     5. Optional geometric or occlusion.\n",
        "     6. Optional equalization.\n",
        "   - Normalizes from `[0..255]` → `[0..1]` → `[-1,1]`.\n",
        "\n",
        "---\n",
        "\n",
        "**Reasoning:**\n",
        "- **Why AugMix-lite?**  \n",
        "  Blending multiple randomized views stabilizes training, improves robustness, and prevents reliance on a single augmentation type.\n",
        "  \n",
        "- **Why banks?**  \n",
        "  Grouping augmentations by type ensures diversity while controlling probabilities independently (e.g., geometric distortions shouldn’t dominate over photometric ones).\n",
        "\n",
        "- **Why strength scaling?**  \n",
        "  Allows adaptive control: weaker augmentations early (to avoid destabilizing convergence), stronger augmentations later (to prevent overfitting).\n",
        "\n",
        "- **Why normalization to [-1,1]?**  \n",
        "  Matches model input conventions (EfficientNet and many pretrained backbones expect normalized input).\n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "This cell provides a **flexible augmentation factory** (`FER_AUG_FACTORY`) that dynamically assembles augmentations, mixes them with AugMix-lite, and normalizes the results. It balances **diversity**, **realism**, and **training stability** — critical for FER, where overfitting to lighting, pose, or occlusion artifacts is common.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d7bef03",
      "metadata": {
        "id": "8d7bef03"
      },
      "outputs": [],
      "source": [
        "# === Cell 07: AugMix-lite + advanced augmentation builder (→ [-1,1]) ===\n",
        "def _apply_bank(x, bank, k=2):\n",
        "    y=x.clone()\n",
        "    for _ in range(k):\n",
        "        y = random.choice(bank)(y)\n",
        "    return y\n",
        "\n",
        "def augmix_lite(x, banks, alpha=0.65, branches=2, depth=2):\n",
        "    mix=x.clone()\n",
        "    for _ in range(branches):\n",
        "        y=_apply_bank(x, random.choice(banks), k=depth)\n",
        "        mix=mix+y\n",
        "    mix = mix / (branches+1.0)\n",
        "    return (1-alpha)*x + alpha*mix\n",
        "\n",
        "def build_advanced_fer_augment(strength: float):\n",
        "    s=float(max(0.0,min(1.0,strength)))\n",
        "    # probabilities\n",
        "    p_photo=0.7*(0.5+0.5*s); p_geom=0.6*(0.5+0.5*s); p_occl=0.40*(0.5+0.5*s)\n",
        "    p_equal=0.20*s; p_blur=0.15*s\n",
        "    # magnitudes\n",
        "    gamma_rng=(0.85-0.15*s, 1.20+0.05*s)\n",
        "    contrast=0.20+0.10*s\n",
        "    jpeg_q=(55-int(10*s), 85)\n",
        "    vignette=0.15+0.20*s\n",
        "    elastic_a=0.6+0.8*s\n",
        "    rot=10+5*s; shear=6+4*s; trans=0.06+0.03*s; scale=0.06+0.04*s\n",
        "\n",
        "    photometric_bank = [\n",
        "        lambda z: gauss_noise(z, sigma=0.015+0.02*s),\n",
        "        lambda z: rand_gamma(z, *gamma_rng),\n",
        "        lambda z: rand_contrast(z, scale=contrast),\n",
        "        lambda z: rand_jpeg(z, qmin=jpeg_q[0], qmax=jpeg_q[1]),\n",
        "        lambda z: rand_vignette(z, strength=vignette),\n",
        "    ]\n",
        "    geometric_bank = [\n",
        "        lambda z: rand_affine_small(z, max_rot=rot, max_trans=trans, max_shear=shear, max_scale=scale),\n",
        "        lambda z: rand_pad_crop(z, pad=3),\n",
        "        lambda z: rand_hflip(z, p=0.5),\n",
        "        lambda z: rand_elastic(z, alpha=elastic_a, sigma=4.0),\n",
        "    ]\n",
        "    occlusion_bank = [\n",
        "        lambda z: band_occlusion(z, mode='eyes',  frac=0.16+0.06*s),\n",
        "        lambda z: band_occlusion(z, mode='mouth', frac=0.16+0.06*s),\n",
        "        lambda z: band_occlusion(z, mode='top',   frac=0.14+0.06*s),\n",
        "        lambda z: localized_erasing(z, min_frac=0.01, max_frac=0.05),\n",
        "    ]\n",
        "    banks=[photometric_bank, geometric_bank, occlusion_bank]\n",
        "\n",
        "    def _norm_to_m11(x255):\n",
        "        x01=(x255/255.).clamp(0,1)\n",
        "        return (x01 - 0.5) * 2.0\n",
        "\n",
        "    def _augment(x):\n",
        "        if random.random() < p_geom:  x = rand_pad_crop(x, pad=3)\n",
        "        if random.random() < p_blur:  x = rand_blur(x, k=3)\n",
        "        if random.random() < p_photo: x = random.choice(photometric_bank)(x)\n",
        "        x = augmix_lite(x, banks=banks, alpha=CONFIG.get(\"AUG_ALPHA\",0.65), branches=2, depth=2)\n",
        "        if random.random() < p_geom:  x = random.choice(geometric_bank)(x)\n",
        "        if random.random() < p_occl:  x = random.choice(occlusion_bank)(x)\n",
        "        if random.random() < p_equal: x = rand_equalize(x)\n",
        "        return _norm_to_m11(x)\n",
        "\n",
        "    return _augment\n",
        "\n",
        "FER_AUG_FACTORY = build_advanced_fer_augment if CONFIG.get(\"USE_AUG_ADV\", False) else build_advanced_fer_augment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f016c61",
      "metadata": {
        "id": "9f016c61"
      },
      "source": [
        "#Cell 08 — Metrics, class weights, losses (Label‑Smoothing + Focal composite)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4744386d",
      "metadata": {
        "id": "4744386d"
      },
      "source": [
        "### Cell 08 — Metrics, Class Weights, and Composite Loss\n",
        "\n",
        "**Logic:**\n",
        "This cell defines the key **evaluation metric** (accuracy), computes **class weights** for imbalanced data, and implements a family of **loss functions** (Label Smoothing, Focal, and a Composite Smoothed-Focal loss).\n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "1. **Metric**\n",
        "   - `accuracy(logits, targets)`: Compares predicted class (argmax of logits) with ground truth and averages over the batch.\n",
        "   - *Reasoning:* Simple, interpretable metric for classification tasks; complements loss-based monitoring.\n",
        "\n",
        "2. **Class Weights**\n",
        "   - `compute_class_weights(df)`:  \n",
        "     - Counts class frequencies in training set.  \n",
        "     - Computes inverse-frequency weights (`total / count[c]`) to counter class imbalance.  \n",
        "     - Normalizes so the mean weight = 1.  \n",
        "   - *Reasoning:* FER2013 has skewed distributions (e.g., many more \"happy\" than \"disgust\"), so weighting helps the model not ignore minority classes.\n",
        "\n",
        "3. **Loss Functions**\n",
        "   - **LabelSmoothingCE**  \n",
        "     - Softens hard labels by distributing a fraction `eps` of probability mass across other classes.  \n",
        "     - Reduces overconfidence, improves generalization.  \n",
        "\n",
        "   - **FocalLoss**  \n",
        "     - Reweights cross-entropy by `(1 - pt)^γ` where `pt` = predicted prob.  \n",
        "     - Down-weights easy examples, focuses learning on harder/misclassified ones.  \n",
        "     - `γ=1.5` chosen as a balanced setting.  \n",
        "\n",
        "   - **SmoothedFocal** (composite)  \n",
        "     - Weighted combination:  \n",
        "       - `α * LabelSmoothingCE + (1-α) * FocalLoss`.  \n",
        "       - If `weight` is provided, incorporates **class weights** inside the focal loss.  \n",
        "     - *Reasoning:* Combines benefits — label smoothing for regularization, focal loss for hard-class mining, and class weights for imbalance.  \n",
        "     - `α=0.70` biases slightly more toward label smoothing stability.\n",
        "\n",
        "---\n",
        "\n",
        "**Why this design?**\n",
        "- **Accuracy**: clear and interpretable baseline metric.  \n",
        "- **Class Weights**: handle imbalanced FER2013 classes.  \n",
        "- **Label Smoothing**: prevents overfitting and overconfidence.  \n",
        "- **Focal Loss**: emphasizes difficult examples.  \n",
        "- **Smoothed Focal (composite)**: integrates all three principles into one loss, tuned for robust performance on noisy, imbalanced facial expression data.\n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "This cell balances **robustness** (label smoothing), **focus** (focal loss), and **fairness** (class weights), ensuring the model learns generalizable and equitable decision boundaries across all facial expression categories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c35ae68",
      "metadata": {
        "id": "2c35ae68"
      },
      "outputs": [],
      "source": [
        "# === Cell 08: Metrics, class weights, composite loss ===\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "\n",
        "def accuracy(logits, targets): return (logits.argmax(1) == targets).float().mean()\n",
        "\n",
        "def compute_class_weights(df) -> torch.Tensor:\n",
        "    counts = Counter(int(e) for e in df[\"emotion\"].tolist())\n",
        "    total = sum(counts.values())\n",
        "    w = torch.tensor([total / max(1, counts.get(c,1)) for c in range(7)], dtype=torch.float32)\n",
        "    return w / w.mean()\n",
        "\n",
        "CLASS_WEIGHTS = compute_class_weights(train_df)\n",
        "\n",
        "class LabelSmoothingCE(nn.Module):\n",
        "    def __init__(self, eps=0.10, reduction='mean'):\n",
        "        super().__init__(); self.eps=eps; self.reduction=reduction\n",
        "    def forward(self, logits, targets):\n",
        "        n = logits.size(-1); logp = F.log_softmax(logits, dim=-1)\n",
        "        with torch.no_grad():\n",
        "            true = torch.zeros_like(logp).fill_(self.eps/(n-1))\n",
        "            true.scatter_(1, targets.unsqueeze(1), 1.0 - self.eps)\n",
        "        loss = -(true * logp).sum(dim=1)\n",
        "        return loss.mean() if self.reduction=='mean' else loss\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=1.5, reduction='mean'):\n",
        "        super().__init__(); self.g=gamma; self.reduction=reduction\n",
        "    def forward(self, logits, targets):\n",
        "        ce = F.cross_entropy(logits, targets, reduction='none')\n",
        "        pt = torch.exp(-ce)\n",
        "        fl = (1-pt).pow(self.g) * ce\n",
        "        return fl.mean() if self.reduction=='mean' else fl\n",
        "\n",
        "class SmoothedFocal(nn.Module):\n",
        "    def __init__(self, eps=0.10, gamma=1.5, alpha=0.70, weight=None):\n",
        "        super().__init__(); self.a=alpha; self.w=weight\n",
        "        self.lsce = LabelSmoothingCE(eps); self.focal = FocalLoss(gamma)\n",
        "    def forward(self, logits, targets):\n",
        "        if self.w is not None:\n",
        "            ce = F.cross_entropy(logits, targets, reduction='none', weight=self.w.to(logits.device))\n",
        "            pt = torch.exp(-ce); fl = (1-pt).pow(1.5) * ce\n",
        "            ls = self.lsce(logits, targets)\n",
        "            return self.a*ls + (1-self.a)*fl.mean()\n",
        "        return self.a*self.lsce(logits, targets) + (1-self.a)*self.focal(logits, targets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54368702",
      "metadata": {
        "id": "54368702"
      },
      "source": [
        "#Cell 09 — MixUp / CutMix and mixed criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9111c1b",
      "metadata": {
        "id": "a9111c1b"
      },
      "source": [
        "### Cell 09 — MixUp / CutMix and Mixed Criterion\n",
        "\n",
        "**Logic:**\n",
        "This cell implements two popular *sample-level augmentation* methods, **MixUp** and **CutMix**, plus a unified loss function (`mixed_criterion`) that can handle mixed targets.\n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "1. **MixUp (`mixup_data`)**\n",
        "   - Blends two random images and their labels with ratio `λ ~ Beta(α, α)`.\n",
        "   - Formula:  \n",
        "     - `x_mix = λ*x_i + (1-λ)*x_j`  \n",
        "     - `y_mix = (y_i, y_j, λ)`  \n",
        "   - *Reasoning:* Encourages linear behavior in between samples, smooths decision boundaries, and reduces overfitting.\n",
        "\n",
        "2. **CutMix (`cutmix_data`)**\n",
        "   - Randomly cuts a patch from one image and pastes it into another.  \n",
        "   - Adjusts labels proportionally to the area kept/replaced.  \n",
        "   - `λ` is recomputed as the effective ratio of preserved pixels.  \n",
        "   - *Reasoning:* Preserves local structures (unlike MixUp’s pixel averaging), improves localization robustness, and combats over-reliance on small features.\n",
        "\n",
        "3. **Mixed Criterion (`mixed_criterion`)**\n",
        "   - Generalizes loss computation for mixed targets:  \n",
        "     - If targets are `(y_a, y_b, λ)`, computes weighted sum:  \n",
        "       `λ * loss(y_a) + (1-λ) * loss(y_b)`.  \n",
        "     - If no mix, reduces to standard criterion.  \n",
        "   - *Reasoning:* Ensures compatibility between MixUp/CutMix outputs and arbitrary loss functions (e.g., SmoothedFocal).\n",
        "\n",
        "---\n",
        "\n",
        "**Why This Design?**\n",
        "- **MixUp**: Regularizes by enforcing convex combinations of inputs → smoother decision boundaries.  \n",
        "- **CutMix**: Stronger augmentation that preserves semantic patches, improving robustness to occlusions and positional variance.  \n",
        "- **Shared Criterion**: Simplifies training loop by unifying the loss handling for both augmented and non-augmented batches.  \n",
        "- **Alpha Parameter**: Controls augmentation strength — higher α increases mixing variability.\n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "MixUp and CutMix both force the model to **share representation across samples**, reducing overconfidence, improving calibration, and boosting robustness to noisy labels and occlusions. This cell provides a **plug-and-play augmentation-loss integration**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9aa56ef2",
      "metadata": {
        "id": "9aa56ef2"
      },
      "outputs": [],
      "source": [
        "# === Cell 09: MixUp / CutMix and mixed criterion ===\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    if alpha <= 0.0: return x, y, 1.0, None\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    idx = torch.randperm(x.size(0), device=x.device)\n",
        "    return lam*x + (1-lam)*x[idx], (y, y[idx]), lam, idx\n",
        "\n",
        "def cutmix_data(x, y, alpha=1.0, min_lam=0.3, max_lam=0.7):\n",
        "    if alpha <= 0.0: return x, y, 1.0, None\n",
        "    lam = float(np.clip(np.random.beta(alpha, alpha), min_lam, max_lam))\n",
        "    B,C,H,W = x.size(); idx = torch.randperm(B, device=x.device)\n",
        "    cut_w = int(W * math.sqrt(1 - lam)); cut_h = int(H * math.sqrt(1 - lam))\n",
        "    cx, cy = np.random.randint(W), np.random.randint(H)\n",
        "    x1, x2 = np.clip(cx - cut_w//2, 0, W), np.clip(cx + cut_w//2, 0, W)\n",
        "    y1, y2 = np.clip(cy - cut_h//2, 0, H), np.clip(cy + cut_h//2, 0, H)\n",
        "    x[:, :, y1:y2, x1:x2] = x[idx, :, y1:y2, x1:x2]\n",
        "    lam = 1 - ((x2-x1)*(y2-y1) / (W*H + 1e-9))\n",
        "    return x, (y, y[idx]), lam, idx\n",
        "\n",
        "def mixed_criterion(criterion, logits, targets_mix, lam):\n",
        "    if isinstance(targets_mix, tuple):\n",
        "        y_a, y_b = targets_mix\n",
        "        return lam * criterion(logits, y_a) + (1-lam) * criterion(logits, y_b)\n",
        "    return criterion(logits, targets_mix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b0a3aba",
      "metadata": {
        "id": "3b0a3aba"
      },
      "source": [
        "#Cell 10 — EMA (exponential moving average)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "addd1010",
      "metadata": {
        "id": "addd1010"
      },
      "source": [
        "### Cell 10 — Exponential Moving Average (EMA)\n",
        "\n",
        "**Logic:**\n",
        "Implements an **Exponential Moving Average (EMA)** wrapper for model parameters. EMA maintains a *smoothed copy* of model weights that often generalizes better than the raw, noisy weights trained via SGD/Adam.\n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "1. **Initialization (`__init__`)**\n",
        "   - Takes a model and a decay factor (default `0.999`).  \n",
        "   - Creates a `shadow` dictionary storing clones of each trainable parameter.  \n",
        "   - *Reasoning:* Provides a slow-moving copy of weights that can track long-term stability.\n",
        "\n",
        "2. **Update (`update`)**\n",
        "   - After each training step, updates the shadow weights:  \n",
        "     - `shadow = decay * shadow + (1 - decay) * current_param`  \n",
        "   - *Reasoning:* Smooths fast-changing weights, reducing the impact of noisy gradient steps.\n",
        "\n",
        "3. **Apply Shadow (`apply_shadow`)**\n",
        "   - Temporarily replaces model parameters with their EMA (shadow) versions.  \n",
        "   - Backs up current parameters in `backup`.  \n",
        "   - *Reasoning:* Enables evaluation or checkpointing with stable EMA weights.\n",
        "\n",
        "4. **Restore (`restore`)**\n",
        "   - Restores the model’s original parameters from backup.  \n",
        "   - Clears backup dictionary.  \n",
        "   - *Reasoning:* Allows training to continue with unaltered model weights after EMA evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "**Why This Design?**\n",
        "- **Stability:** EMA parameters are less sensitive to mini-batch noise.  \n",
        "- **Generalization:** Often improves validation/test accuracy because EMA acts like a low-pass filter over updates.  \n",
        "- **Flexibility:** Separation of `apply_shadow` and `restore` makes it easy to switch between EMA and raw parameters during evaluation and training.  \n",
        "- **Decay Factor (`0.999`):** High decay → slow updates, longer memory; low decay → tracks current weights more closely.  \n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "EMA acts as a **teacher model inside training** — it aggregates past versions of the student (raw model), leading to smoother and more generalizable checkpoints, especially in noisy or augmentation-heavy training regimes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40fbb1f9",
      "metadata": {
        "id": "40fbb1f9"
      },
      "outputs": [],
      "source": [
        "# === Cell 10: EMA ===\n",
        "import torch.nn as nn\n",
        "\n",
        "class EMA:\n",
        "    def __init__(self, model: nn.Module, decay: float = 0.999):\n",
        "        self.decay=float(decay); self.shadow={}; self.backup={}\n",
        "        for n,p in model.named_parameters():\n",
        "            if p.requires_grad: self.shadow[n]=p.data.clone()\n",
        "    def update(self, model):\n",
        "        for n,p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.shadow[n]=(1-self.decay)*p.data + self.decay*self.shadow[n]\n",
        "    def apply_shadow(self, model):\n",
        "        self.backup={}\n",
        "        for n,p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.backup[n]=p.data.clone(); p.data=self.shadow[n].clone()\n",
        "    def restore(self, model):\n",
        "        for n,p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                p.data=self.backup[n].clone()\n",
        "        self.backup={}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "718332c2",
      "metadata": {
        "id": "718332c2"
      },
      "source": [
        "### Cell 11 — CBAM (Convolutional Block Attention Module) + Sobel Stem\n",
        "\n",
        "**Logic:**\n",
        "This cell defines two architectural components:\n",
        "1. **CBAM**: A lightweight attention module that improves feature learning by sequentially applying **channel** and **spatial** attention.\n",
        "2. **SobelLayer**: A handcrafted edge-detection stem that enhances the network’s ability to capture facial contours.\n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "#### 1. CBAM\n",
        "- **Channel Attention (CA)**\n",
        "  - Pools features using both average and max pooling (`adaptive_avg_pool2d`, `adaptive_max_pool2d`).\n",
        "  - Passes pooled features through a small MLP (Conv → ReLU → Conv).\n",
        "  - Outputs attention weights via `Sigmoid`.\n",
        "  - *Reasoning:* Helps the model learn **which feature channels are most important** for expression recognition (e.g., mouth edges vs. forehead textures).\n",
        "\n",
        "- **Spatial Attention (SA)**\n",
        "  - Concatenates mean and max across the channel dimension → `[B,2,H,W]`.\n",
        "  - Passes through a 7×7 Conv + Sigmoid to produce a spatial mask.\n",
        "  - *Reasoning:* Highlights **where** in the image important features lie (eyes, mouth corners).\n",
        "\n",
        "- **Forward Pass**\n",
        "  - Multiplies input by CA weights, then by SA mask.\n",
        "  - *Reasoning:* Enhances discriminative features while suppressing irrelevant background.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. Sobel Layer\n",
        "- **Definition**\n",
        "  - Registers horizontal (kx) and vertical (ky) Sobel kernels (edge detectors).\n",
        "  - Applies them via convolution to input grayscale image `[B,1,H,W]`.\n",
        "- **Output**\n",
        "  - Original grayscale channel + horizontal edges + vertical edges → `[B,3,H,W]`.\n",
        "- *Reasoning:* Forces the network to explicitly learn from **edges/gradients**, which are critical for detecting facial muscle movements in expression recognition.\n",
        "\n",
        "---\n",
        "\n",
        "**Why This Design?**\n",
        "- **CBAM**: Provides *dynamic attention* with minimal overhead; improves robustness to irrelevant noise by focusing on salient regions/channels.\n",
        "- **Sobel Stem**: Hardcodes useful inductive bias (edges) into the network’s first layer, accelerating convergence and improving low-level feature learning.\n",
        "- **Combined Effect:** CBAM refines learned deep features, while Sobel strengthens shallow edge cues → together they enhance performance on FER tasks.\n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "- **CBAM** = learnable *what/where to focus*.  \n",
        "- **Sobel** = fixed *edge prior*.  \n",
        "Together, they form a strong, efficient input stem for facial expression recognition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4548e23b",
      "metadata": {
        "id": "4548e23b"
      },
      "outputs": [],
      "source": [
        "# === Cell 11: CBAM + Sobel stem ===\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, ch, r=8):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Conv2d(ch, max(1,ch//r), 1, bias=True), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(max(1,ch//r), ch, 1, bias=True)\n",
        "        )\n",
        "        self.spatial = nn.Sequential(nn.Conv2d(2,1,kernel_size=7,padding=3,bias=False), nn.Sigmoid())\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        ca = self.sigmoid(self.mlp(F.adaptive_avg_pool2d(x,1) + F.adaptive_max_pool2d(x,1)))\n",
        "        x = x * ca\n",
        "        ms = torch.cat([x.mean(1,keepdim=True), x.max(1,keepdim=True)[0]], dim=1)\n",
        "        return x * self.spatial(ms)\n",
        "\n",
        "class SobelLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        kx = torch.tensor([[1,0,-1],[2,0,-2],[1,0,-1]], dtype=torch.float32)\n",
        "        ky = torch.tensor([[1,2,1],[0,0,0],[-1,-2,-1]], dtype=torch.float32)\n",
        "        w  = torch.stack([kx, ky]).unsqueeze(1)   # (2,1,3,3)\n",
        "        self.register_buffer('w', w)\n",
        "    def forward(self, x):                          # x: [B,1,H,W]\n",
        "        edges = F.conv2d(x, self.w, padding=1)     # [B,2,H,W]\n",
        "        return torch.cat([x, edges], dim=1)        # [B,3,H,W]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b3be854",
      "metadata": {
        "id": "8b3be854"
      },
      "source": [
        "#=== Cell 12a: Blocks (Sobel, CBAM) ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34ac5e4b",
      "metadata": {
        "id": "34ac5e4b"
      },
      "outputs": [],
      "source": [
        "# === Cell 12a: Blocks (Sobel, CBAM) ===\n",
        "# Purpose: tiny, dependency‑free “head” modules shared by all backbones.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Sobel stem: take 1‑ch input and expand to 3‑ch using gradients.\n",
        "class SobelLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        kx = torch.tensor([[1,0,-1],[2,0,-2],[1,0,-1]], dtype=torch.float32)\n",
        "        ky = kx.t()\n",
        "        self.register_buffer(\"wx\", kx.view(1,1,3,3))\n",
        "        self.register_buffer(\"wy\", ky.view(1,1,3,3))\n",
        "\n",
        "    def forward(self, x1):  # x1: [B,1,H,W] in [-1,1]\n",
        "        gx = F.conv2d(x1, self.wx, padding=1)\n",
        "        gy = F.conv2d(x1, self.wy, padding=1)\n",
        "        mag = torch.sqrt(gx*gx + gy*gy + 1e-6)\n",
        "        return torch.cat([x1, gx, mag], dim=1)  # → [B,3,H,W]\n",
        "\n",
        "# --- CBAM: lightweight attention (channel + spatial).\n",
        "class ChannelGate(nn.Module):\n",
        "    def __init__(self, ch, r=16):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(ch, max(8, ch//r)), nn.ReLU(inplace=True),\n",
        "            nn.Linear(max(8, ch//r), ch)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.shape\n",
        "        avg = F.adaptive_avg_pool2d(x, 1).view(b, c)\n",
        "        maxp= F.adaptive_max_pool2d(x, 1).view(b, c)\n",
        "        att = torch.sigmoid(self.mlp(avg) + self.mlp(maxp)).view(b,c,1,1)\n",
        "        return x * att\n",
        "\n",
        "class SpatialGate(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size=7, padding=3)\n",
        "    def forward(self, x):\n",
        "        att = torch.sigmoid(self.conv(torch.cat([x.mean(1, True), x.max(1, True).values], dim=1)))\n",
        "        return x * att\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, ch):\n",
        "        super().__init__()\n",
        "        self.c = ChannelGate(ch)\n",
        "        self.s = SpatialGate()\n",
        "    def forward(self, x): return self.s(self.c(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59b6cc56",
      "metadata": {
        "id": "59b6cc56"
      },
      "source": [
        "### Cell 12 — HybridEffNet (EfficientNet-B0 + CBAM + Sobel)\n",
        "\n",
        "**Logic:**\n",
        "This cell defines a custom model architecture `HybridEffNet`, which integrates:\n",
        "- **EfficientNet-B0** backbone (strong general-purpose CNN).  \n",
        "- **SobelLayer** stem (explicit edge priors).  \n",
        "- **CBAM** attention (channel + spatial focus).  \n",
        "- **Custom classifier head** tuned for FER2013’s 7 emotion classes.  \n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "1. **Sobel Stem**\n",
        "   - Input: `[B,1,H,W]` grayscale image normalized to `[-1,1]`.  \n",
        "   - Sobel transforms → `[B,3,H,W]` (original + horizontal + vertical edges).  \n",
        "   - *Reasoning:* Provides explicit edge features that highlight facial muscle contours.\n",
        "\n",
        "2. **EfficientNet-B0 Features**\n",
        "   - Pretrained `efficientnet_b0` feature extractor (`.features` block).  \n",
        "   - Processes 3-channel input to output `[B,1280,h,w]`.  \n",
        "   - *Reasoning:* EfficientNet is a well-scaled architecture with strong accuracy/FLOPs trade-off.\n",
        "\n",
        "3. **CBAM Attention (optional)**\n",
        "   - Applied on `[B,1280,h,w]` feature maps.  \n",
        "   - Learns **what channels to emphasize** and **where spatially to focus**.  \n",
        "   - *Reasoning:* FER requires sensitivity to subtle regions (eyes, mouth); CBAM improves discriminative power.\n",
        "\n",
        "4. **Pooling & Classifier**\n",
        "   - `AdaptiveAvgPool2d(1)` → global pooling to `[B,1280]`.  \n",
        "   - BatchNorm + Dropout (0.3) for regularization.  \n",
        "   - Linear head → `[B,7]` logits.  \n",
        "   - *Reasoning:* Simple, effective classifier design that reduces overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "**Why This Design?**\n",
        "- **Sobel + EfficientNet synergy:** Combines handcrafted low-level edges with pretrained deep features.  \n",
        "- **CBAM:** Adds lightweight attention to refine features without large FLOP overhead.  \n",
        "- **Dropout + BatchNorm:** Controls overfitting, stabilizes training.  \n",
        "- **Pretrained EfficientNet:** Leverages strong ImageNet initialization, improving convergence speed and final accuracy.  \n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "HybridEffNet blends **inductive priors (Sobel edges)** with **modern CNN efficiency (EfficientNet-B0)** and **attention refinement (CBAM)** to create a robust backbone tailored for **facial expression recognition**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6a13161",
      "metadata": {
        "id": "a6a13161"
      },
      "source": [
        "#=== Cell 12b: Backbones (EffNet‑B0, MobileNetV3‑Small, GhostNetV2) ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7efc18e5",
      "metadata": {
        "id": "7efc18e5"
      },
      "outputs": [],
      "source": [
        "# === Cell 12b: Backbones (EffNet‑B0, MobileNetV3‑Small, GhostNetV2) ===\n",
        "# Purpose: define 3 independent model classes with the same forward signature.\n",
        "# All three use Sobel(1→3) + backbone.features + (optional) CBAM + GAP + BN + Dropout + Linear.\n",
        "\n",
        "# Install timm once for GhostNet (no‑op if already installed)\n",
        "try:\n",
        "    import timm\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"timm\"])\n",
        "    import timm\n",
        "\n",
        "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
        "from torchvision.models import mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
        "\n",
        "CLASSIFIER_DROPOUT = 0.30\n",
        "USE_CBAM = True\n",
        "NUM_CLASSES = 7\n",
        "\n",
        "# ----- 1) EfficientNet‑B0 (matches the PDF lineage)\n",
        "class FER_EffNetB0_CBAM(nn.Module):\n",
        "    def __init__(self, num_classes=NUM_CLASSES, classifier_dropout=CLASSIFIER_DROPOUT, use_cbam=USE_CBAM):\n",
        "        super().__init__()\n",
        "        base = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
        "        self.sobel    = SobelLayer()             # 1→3\n",
        "        self.features = base.features            # → [B,1280,h,w]\n",
        "        self.cbam     = CBAM(1280) if use_cbam else None\n",
        "        self.pool     = nn.AdaptiveAvgPool2d(1)\n",
        "        self.bn       = nn.BatchNorm1d(1280)\n",
        "        self.drop     = nn.Dropout(p=classifier_dropout)\n",
        "        self.head     = nn.Linear(1280, num_classes)\n",
        "\n",
        "    def forward(self, x1):\n",
        "        x3 = self.sobel(x1)\n",
        "        f  = self.features(x3)\n",
        "        if self.cbam is not None: f = self.cbam(f)\n",
        "        f  = self.pool(f).flatten(1)\n",
        "        f  = self.bn(f); f = self.drop(f)\n",
        "        return self.head(f)\n",
        "\n",
        "# ----- 2) MobileNetV3‑Small\n",
        "class FER_MobileNetV3S_CBAM(nn.Module):\n",
        "    def __init__(self, num_classes=NUM_CLASSES, classifier_dropout=CLASSIFIER_DROPOUT, use_cbam=USE_CBAM):\n",
        "        super().__init__()\n",
        "        base = mobilenet_v3_small(weights=MobileNet_V3_Small_Weights.DEFAULT)\n",
        "        self.sobel    = SobelLayer()\n",
        "        self.features = base.features            # last channels = 576\n",
        "        last_ch = 576\n",
        "        self.cbam     = CBAM(last_ch) if use_cbam else None\n",
        "        self.pool     = nn.AdaptiveAvgPool2d(1)\n",
        "        self.bn       = nn.BatchNorm1d(last_ch)\n",
        "        self.drop     = nn.Dropout(p=classifier_dropout)\n",
        "        self.head     = nn.Linear(last_ch, num_classes)\n",
        "\n",
        "    def forward(self, x1):\n",
        "        x3 = self.sobel(x1)\n",
        "        f  = self.features(x3)\n",
        "        if self.cbam is not None: f = self.cbam(f)\n",
        "        f  = self.pool(f).flatten(1)\n",
        "        f  = self.bn(f); f = self.drop(f)\n",
        "        return self.head(f)\n",
        "\n",
        "# ----- 3) GhostNetV2‑100 (timm). If you prefer v1: 'ghostnet_100' & adjust channels.\n",
        "class FER_GhostNetV2_CBAM(nn.Module):\n",
        "    def __init__(self, num_classes=NUM_CLASSES, classifier_dropout=CLASSIFIER_DROPOUT, use_cbam=USE_CBAM):\n",
        "        super().__init__()\n",
        "        # num_classes=0 & global_pool='' → raw feature map; we add our own head for consistency.\n",
        "        base = timm.create_model('ghostnetv2_100', pretrained=True, in_chans=3, num_classes=0, global_pool='')\n",
        "        self.sobel    = SobelLayer()\n",
        "        self.features = base                     # returns [B,C,h,w]\n",
        "        # Determine last channel count robustly with a tiny dry run\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1,1,int(CONFIG[\"IMG_SIZE\"]),int(CONFIG[\"IMG_SIZE\"]))\n",
        "            ch = self.features(self.sobel(dummy)).shape[1]\n",
        "        self.cbam     = CBAM(ch) if use_cbam else None\n",
        "        self.pool     = nn.AdaptiveAvgPool2d(1)\n",
        "        self.bn       = nn.BatchNorm1d(ch)\n",
        "        self.drop     = nn.Dropout(p=classifier_dropout)\n",
        "        self.head     = nn.Linear(ch, num_classes)\n",
        "\n",
        "    def forward(self, x1):\n",
        "        x3 = self.sobel(x1)\n",
        "        f  = self.features(x3)\n",
        "        if self.cbam is not None: f = self.cbam(f)\n",
        "        f  = self.pool(f).flatten(1)\n",
        "        f  = self.bn(f); f = self.drop(f)\n",
        "        return self.head(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "051e97c2",
      "metadata": {
        "id": "051e97c2"
      },
      "source": [
        "#=== Cell 13: Builders + run list (comment to skip) ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2f4b89f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2f4b89f",
        "outputId": "2bca0034-7a2f-45f7-aa77-5a1f3bd3b14a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Build] Planning to run: ['effnet_b0', 'mobilenet_v3', 'ghostnet_v2']\n",
            "  effnet_b0    → /content/project/checkpoints/best_effnet_b0_fer.pth\n",
            "  mobilenet_v3 → /content/project/checkpoints/best_mobilenet_v3_fer.pth\n",
            "  ghostnet_v2  → /content/project/checkpoints/best_ghostnet_v2_fer.pth\n"
          ]
        }
      ],
      "source": [
        "# === Cell 13: Builders + unique checkpoint names ===\n",
        "# Purpose: single entry point to instantiate any backbone; define which models to run.\n",
        "\n",
        "def build_model(backbone_tag: str) -> nn.Module:\n",
        "    tag = backbone_tag.lower()\n",
        "    if tag == \"effnet_b0\":\n",
        "        m = FER_EffNetB0_CBAM(NUM_CLASSES)\n",
        "    elif tag == \"mobilenet_v3\":\n",
        "        m = FER_MobileNetV3S_CBAM(NUM_CLASSES)\n",
        "    elif tag == \"ghostnet_v2\":\n",
        "        m = FER_GhostNetV2_CBAM(NUM_CLASSES)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown backbone: {backbone_tag}\")\n",
        "    return m.to(device)\n",
        "\n",
        "# === Toggle here: comment out any model you don't want to run ===\n",
        "MODELS_TO_RUN = [\n",
        "    \"effnet_b0\",\n",
        "    \"mobilenet_v3\",\n",
        "    \"ghostnet_v2\",\n",
        "]\n",
        "\n",
        "print(\"[Build] Planning to run:\", MODELS_TO_RUN)\n",
        "for tag in MODELS_TO_RUN:\n",
        "    print(f\"  {tag:12s} → {ckpt_path_for(tag)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf0d8191",
      "metadata": {
        "id": "bf0d8191"
      },
      "source": [
        "#Cell 13 — Optimizer, scheduler, early stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9a47ae2",
      "metadata": {
        "id": "d9a47ae2"
      },
      "source": [
        "### Cell 13 — Optimizer, Scheduler Helper, and EarlyStopping\n",
        "\n",
        "**Logic:**\n",
        "This cell provides utility components for optimization and training stability:\n",
        "1. **Optimizer Factory (`make_optimizer`)**  \n",
        "2. **Warmup + Cosine Scheduler (back-compat helper)**  \n",
        "3. **EarlyStopping mechanism**  \n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "1. **Optimizer Factory (`make_optimizer`)**\n",
        "   - Chooses optimizer based on `CONFIG[\"USE_SGD\"]`:  \n",
        "     - **AdamW** (default, and the optimizer we are using): Adaptive learning rates with decoupled weight decay.  \n",
        "     - **SGD + Nesterov** (optional): Momentum-driven updates, less memory usage.  \n",
        "   - Uses hyperparameters from `HP`:  \n",
        "     - `LR`, `WD`, `SGD_MOMENTUM`, `SGD_NESTEROV`.  \n",
        "   - *Reasoning:* AdamW is chosen because it provides more stable convergence for this task, handles sparse gradients effectively, and is less sensitive to manual learning rate tuning than SGD.\n",
        "\n",
        "2. **Warmup + Cosine Annealing Scheduler (`WarmupCosine`)**\n",
        "   - Manually implements cosine learning rate schedule with linear warmup:  \n",
        "     - **Warmup phase:** LR rises linearly from `lr_min` to `lr_max` over `warmup_epochs`.  \n",
        "     - **Cosine decay:** LR decreases smoothly from `lr_max` to `lr_min` until `max_epochs`.  \n",
        "   - Each call to `.step()` updates the LR and increments the epoch counter.  \n",
        "   - *Reasoning:* Cosine decay avoids abrupt LR drops, improving convergence stability; warmup prevents divergence in early epochs.\n",
        "\n",
        "   *(Note: In Cell 02 you already have `build_scheduler()`. This class provides backwards compatibility or fine control if needed.)*\n",
        "\n",
        "3. **EarlyStopping**\n",
        "   - Monitors validation loss (`val_loss`).  \n",
        "   - Stops training if no significant improvement (`min_delta`) for `patience` consecutive epochs.  \n",
        "   - Keeps track of best loss so far (`self.best`) and count of bad epochs (`self.bad`).  \n",
        "   - Returns `True` if patience is exceeded → signal to stop training.  \n",
        "   - *Reasoning:* Prevents unnecessary training once the model plateaus or starts overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "**Why This Design?**\n",
        "- **AdamW is the chosen optimizer** because it provides robust convergence and reduces the need for extensive manual hyperparameter tuning.  \n",
        "- **Modularity:** Keeps optimizer/scheduler creation separate from training loop.  \n",
        "- **Flexibility:** Optionally allows SGD, though AdamW is preferred for this FER pipeline.  \n",
        "- **Stability:** EarlyStopping provides a safeguard against wasted compute and overfitting.  \n",
        "- **Best Practices:** Warmup + cosine and early stopping are widely used in modern training pipelines for their balance of performance and stability.\n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "This cell acts as the *training control hub* — selecting **AdamW as the optimizer**, managing learning rate schedules, and preventing overtraining through early stopping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4c368f5",
      "metadata": {
        "id": "a4c368f5"
      },
      "outputs": [],
      "source": [
        "# === Cell 13: Optimizer + Scheduler + EarlyStopping (updated) ===\n",
        "import math\n",
        "import torch\n",
        "\n",
        "def make_optimizer(params, hp, cfg):\n",
        "    \"\"\"\n",
        "    AdamW (default) or SGD+Nesterov via CONFIG['USE_SGD'] toggle.\n",
        "    Uses HP['LR'] and HP['WD'].\n",
        "    \"\"\"\n",
        "    if cfg.get(\"USE_SGD\", False):\n",
        "        return torch.optim.SGD(\n",
        "            params,\n",
        "            lr=hp[\"LR\"],\n",
        "            momentum=hp[\"SGD_MOMENTUM\"],\n",
        "            nesterov=hp[\"SGD_NESTEROV\"],\n",
        "            weight_decay=hp[\"WD\"],\n",
        "        )\n",
        "    return torch.optim.AdamW(params, lr=hp[\"LR\"], weight_decay=hp[\"WD\"])\n",
        "\n",
        "# Back‑compat cosine helper (epoch-level stepping)\n",
        "class WarmupCosine:\n",
        "    def __init__(self, opt, warmup_epochs, max_epochs, lr_min=1e-6, lr_max=None):\n",
        "        self.opt = opt\n",
        "        self.warmup = max(1, int(warmup_epochs))\n",
        "        self.maxe = int(max_epochs)\n",
        "        self.t = 0\n",
        "        if lr_max is None:\n",
        "            lr_max = max(pg['lr'] for pg in opt.param_groups)\n",
        "        self.lr_min, self.lr_max = lr_min, lr_max\n",
        "    def step(self):\n",
        "        self.t += 1\n",
        "        if self.t <= self.warmup:\n",
        "            lr = self.lr_min + (self.lr_max - self.lr_min) * (self.t / self.warmup)\n",
        "        else:\n",
        "            tt = (self.t - self.warmup) / max(1, (self.maxe - self.warmup))\n",
        "            lr = self.lr_min + 0.5 * (self.lr_max - self.lr_min) * (1 + math.cos(math.pi * tt))\n",
        "        for g in self.opt.param_groups: g['lr'] = lr\n",
        "        return lr\n",
        "\n",
        "# NOTE:\n",
        "# In Cell 02 you already have build_scheduler(...) and helpers like\n",
        "#   - scheduler_steps_per_batch()\n",
        "#   - current_lr()\n",
        "# We simply keep EarlyStopping here and let Cell 14 call build_scheduler.\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stop on validation loss. Call .step(val_loss) each epoch.\"\"\"\n",
        "    def __init__(self, patience=8, min_delta=1e-4):\n",
        "        self.patience = int(patience)\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.best = float('inf')\n",
        "        self.bad = 0\n",
        "    def step(self, val_loss: float) -> bool:\n",
        "        if val_loss < self.best - self.min_delta:\n",
        "            self.best = val_loss\n",
        "            self.bad = 0\n",
        "            return False\n",
        "        self.bad += 1\n",
        "        return self.bad >= self.patience\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "821a02bd",
      "metadata": {
        "id": "821a02bd"
      },
      "source": [
        "### Cell 14 — Training Loop (Plateau uses val_loss; OneCycle steps per batch)\n",
        "\n",
        "**Logic:**\n",
        "This cell implements the full training loop with support for multiple schedulers (Plateau, Cosine, OneCycle), advanced augmentation scheduling, label smoothing, MixUp/CutMix tapering, fine-tuning, EMA tracking, and early stopping.\n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "1. **Optimizer & Scheduler**\n",
        "   - Uses `make_optimizer` (from Cell 13) → AdamW (chosen optimizer).\n",
        "   - Scheduler built from Cell 02 (`build_scheduler`):\n",
        "     - **OneCycle**: steps per batch.  \n",
        "     - **Cosine/Plateau**: steps per epoch (Plateau keyed to `val_loss`).  \n",
        "   - *Reasoning:* Different schedulers provide flexibility: OneCycle accelerates convergence, Cosine smooths decay, Plateau reacts adaptively to stagnation.\n",
        "\n",
        "2. **Stability Helpers**\n",
        "   - **EarlyStopping** (Cell 13): halts training if no validation loss improvement for `patience`.  \n",
        "   - **EMA**: Tracks smoothed parameter averages for more stable evaluation.  \n",
        "   - **AMP + Gradient Clipping**: Reduces memory/compute cost (fp16) while avoiding exploding gradients.  \n",
        "\n",
        "3. **Augmentation Scheduling**\n",
        "   - `FER_AUG_FACTORY(s)`: strength `s` increases over ramp-up epochs, capped later (`AUG_CAP_LATE`).  \n",
        "   - *Reasoning:* Start with lighter augmentations for stability → stronger ones later to fight overfitting.\n",
        "\n",
        "4. **MixUp/CutMix Tapering**\n",
        "   - Probabilities (`p_mix`, `p_cut`) decay from base values to zero between `TAPER_START_FRAC` and `TAPER_END_FRAC`.  \n",
        "   - Disabled entirely in fine-tune tail.  \n",
        "   - *Reasoning:* Regularize strongly early, then allow the network to focus on clean data near convergence.\n",
        "\n",
        "5. **Fine-tune Tail**\n",
        "   - Final fraction of epochs (`FINE_TUNE_FRACTION`) runs **without augmentation/mix** at lower LR.  \n",
        "   - *Reasoning:* Acts as a “clean finish,” letting the model consolidate on unaugmented signals.\n",
        "\n",
        "6. **Label Smoothing Schedule**\n",
        "   - Epsilon decays from `LABEL_SMOOTH_START` to `LABEL_SMOOTH_END` across training.  \n",
        "   - *Reasoning:* Stronger smoothing early prevents overconfidence; weaker smoothing later preserves precision.\n",
        "\n",
        "7. **Training Step**\n",
        "   - Normalization: map images to `[-1,1]`.  \n",
        "   - Augmentation → MixUp or CutMix (probabilistic).  \n",
        "   - Loss:  \n",
        "     - Main = Label Smoothed CE (with MixUp/CutMix if active).  \n",
        "     - Weighted blend with standard CE using class weights (25%).  \n",
        "   - Backprop: AMP scaling, gradient clipping, optimizer + scheduler step.  \n",
        "   - EMA update at each step.\n",
        "\n",
        "8. **Validation Step**\n",
        "   - Clean evaluation (no aug, no mix).  \n",
        "   - Reports mean val loss + accuracy.  \n",
        "   - Used for checkpointing (best `val_loss`).\n",
        "\n",
        "9. **Logging + Checkpointing**\n",
        "   - Logs metrics each epoch: train loss, val loss, val acc, LR.  \n",
        "   - Saves model state if `val_loss` improves.  \n",
        "   - Stops early if `EarlyStopping` triggers.\n",
        "\n",
        "---\n",
        "\n",
        "**Why This Design?**\n",
        "- **Comprehensive regularization:** AugMix, MixUp/CutMix, smoothing, and class weights each address different risks (overfitting, imbalance, noisy labels).  \n",
        "- **Dynamic scheduling:** Augmentation/mix taper and label smoothing schedule ensure training starts robust but finishes precise.  \n",
        "- **Stability mechanisms:** EMA, gradient clipping, and AMP provide safety and efficiency.  \n",
        "- **General-purpose flexibility:** One training loop works with AdamW + OneCycle (default) or other scheduler configs.\n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "This loop represents a **modern, production-grade training recipe** — combining augmentation diversity, dynamic scheduling, EMA smoothing, and adaptive early stopping. It balances *robustness early in training* with *precision late in training* to achieve strong generalization on FER2013.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#helper cell before 14"
      ],
      "metadata": {
        "id": "Kahj3dd-oGQJ"
      },
      "id": "Kahj3dd-oGQJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# === Safe EMA (skip non-floating buffers) ===\n",
        "class EMA:\n",
        "    \"\"\"\n",
        "    Exponential Moving Average of model parameters/buffers.\n",
        "    - Only updates tensors with floating dtype (float16/32/64, bfloat16).\n",
        "    - Non-floating tensors (e.g., Long buffers like num_batches_tracked) are copied once and left unchanged.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, decay: float = 0.9995, device=None):\n",
        "        self.decay = float(decay)\n",
        "        self.shadow = {}\n",
        "        self.backup = {}\n",
        "\n",
        "        # Create the shadow dict on the same device/dtype as the source tensors\n",
        "        for k, v in model.state_dict().items():\n",
        "            if v.dtype.is_floating_point:\n",
        "                self.shadow[k] = v.detach().clone()\n",
        "            else:\n",
        "                # copy once; we won't EMA-update these\n",
        "                self.shadow[k] = v.detach().clone()\n",
        "\n",
        "        # optional: move shadow to a different device (usually not needed)\n",
        "        if device is not None:\n",
        "            for k in self.shadow:\n",
        "                self.shadow[k] = self.shadow[k].to(device)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, model):\n",
        "        d = self.decay\n",
        "        for k, v in model.state_dict().items():\n",
        "            if not v.dtype.is_floating_point:\n",
        "                # leave non-floating tensors unchanged\n",
        "                self.shadow[k].copy_(v)\n",
        "                continue\n",
        "            # EMA: shadow = d*shadow + (1-d)*v\n",
        "            self.shadow[k].mul_(d).add_(v.detach(), alpha=(1.0 - d))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def apply_shadow(self, model):\n",
        "        # swap in shadow weights (backup originals)\n",
        "        self.backup = {}\n",
        "        for k, v in model.state_dict().items():\n",
        "            self.backup[k] = v.detach().clone()\n",
        "            v.copy_(self.shadow[k])\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def restore(self, model):\n",
        "        # restore originals after evaluation\n",
        "        if not self.backup:\n",
        "            return\n",
        "        for k, v in model.state_dict().items():\n",
        "            v.copy_(self.backup[k])\n",
        "        self.backup = {}\n"
      ],
      "metadata": {
        "id": "QrMxxZtZoJ1Y"
      },
      "id": "QrMxxZtZoJ1Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ac8bd3c",
      "metadata": {
        "id": "4ac8bd3c"
      },
      "outputs": [],
      "source": [
        "# === Cell 14: Training loop (Plateau uses val_loss; OneCycle steps per batch) ===\n",
        "import random, math, numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import inspect\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Compatibility shims so this cell works with your Cell‑02 factories.\n",
        "# ------------------------------------------------------------------\n",
        "def _make_opt(model, hp, cfg):\n",
        "    \"\"\"\n",
        "    Your Cell 02 typically defines: make_optimizer(model)  -> uses global HP/CONFIG.\n",
        "    If an older signature is present, we fall back to: make_optimizer(model.parameters(), hp, cfg).\n",
        "    \"\"\"\n",
        "    sig = inspect.signature(make_optimizer)\n",
        "    if len(sig.parameters) == 1:\n",
        "        return make_optimizer(model)\n",
        "    return make_optimizer(model.parameters(), hp, cfg)\n",
        "\n",
        "def _build_sched(optimizer, steps_per_epoch, hp, cfg):\n",
        "    \"\"\"\n",
        "    Your Cell 02 usually defines: build_scheduler(optimizer, steps_per_epoch=None).\n",
        "    Support older variants that take positional args, too.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return build_scheduler(optimizer, steps_per_epoch=steps_per_epoch)\n",
        "    except TypeError:\n",
        "        return build_scheduler(optimizer, steps_per_epoch, hp, cfg)\n",
        "\n",
        "def _scheduler_steps_per_batch(cfg):\n",
        "    \"\"\"\n",
        "    OneCycle -> step per batch; Cosine/Plateau -> step per epoch.\n",
        "    If you already expose scheduler_steps_per_batch(), we will honor it.\n",
        "    \"\"\"\n",
        "    if 'scheduler_steps_per_batch' in globals():\n",
        "        try:\n",
        "            return bool(scheduler_steps_per_batch())\n",
        "        except Exception:\n",
        "            pass\n",
        "    sched = str(cfg.get(\"SCHEDULER\", CONFIG.get(\"SCHEDULER\", \"onecycle\"))).lower()\n",
        "    return (sched == \"onecycle\")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Trainer (uses your Cell‑08 metrics/losses exactly)\n",
        "#   • main loss: SmoothedFocal(eps=0.10, gamma=1.5, alpha=0.70, weight=CLASS_WEIGHTS)\n",
        "#   • light CE stabilizer blended in (optional)\n",
        "#   • AMP, EMA (if EMA class exists), OneCycle/Cosine/Plateau\n",
        "#   • sparse logging: every PRINT_EVERY, on improved val_acc, and final epoch\n",
        "#   • checkpoint on best val_loss (unchanged)\n",
        "# ------------------------------------------------------------------\n",
        "def fit_with_aug(model: nn.Module, train_dl, val_dl, hp: dict, cfg: dict):\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # ----- Optimizer & Scheduler -----\n",
        "    opt = _make_opt(model, hp, cfg)\n",
        "    steps_per_epoch = len(train_dl)\n",
        "    sched = _build_sched(opt, steps_per_epoch=steps_per_epoch, hp=hp, cfg=cfg)\n",
        "    use_batch_sched = _scheduler_steps_per_batch(cfg)\n",
        "\n",
        "    # ----- EMA (instantiate only if an EMA class is available) -----\n",
        "    ema_obj = None\n",
        "    if cfg.get(\"USE_EMA\", True) and (\"EMA\" in globals()) and callable(globals()[\"EMA\"]):\n",
        "        try:\n",
        "            ema_obj = EMA(model, decay=float(hp.get(\"EMA_DECAY\", 0.9995)))\n",
        "        except TypeError:\n",
        "            ema_obj = EMA(model)  # fallback if your EMA takes only (model)\n",
        "\n",
        "    # ----- AMP -----\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=bool(cfg.get(\"USE_AMP\", torch.cuda.is_available())))\n",
        "\n",
        "    # ----- Losses (from Cell 08) -----\n",
        "    # main criterion (uses your CLASS_WEIGHTS)\n",
        "    main_crit = SmoothedFocal(eps=0.10, gamma=1.5, alpha=0.70, weight=CLASS_WEIGHTS)\n",
        "    # optional stabilizer CE (class-weighted) used as a light blend below\n",
        "    ce_weight = CLASS_WEIGHTS.to(device=device) if isinstance(CLASS_WEIGHTS, torch.Tensor) else None\n",
        "\n",
        "    # ----- Logging / early stop controls -----\n",
        "    E = int(hp[\"EPOCHS\"])\n",
        "    PRINT_EVERY = int(cfg.get(\"PRINT_EVERY\", 10))\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_val_acc  = -1.0\n",
        "\n",
        "    # Mix/taper schedule & clean fine‑tune tail\n",
        "    ramp_epochs  = ( int(hp[\"AUG_RAMP_EPOCHS\"]) if hp.get(\"AUG_RAMP_EPOCHS\", 0.40) >= 1.0\n",
        "                     else max(1, int(float(hp.get(\"AUG_RAMP_EPOCHS\", 0.40)) * E)) )\n",
        "    t_start      = float(cfg.get(\"TAPER_START_FRAC\", 0.20))\n",
        "    t_end        = float(cfg.get(\"TAPER_END_FRAC\",   0.85))\n",
        "    base_mix_p   = float(cfg.get(\"BASE_MIXUP_PROB\", 0.45))\n",
        "    base_cut_p   = float(cfg.get(\"BASE_CUTMIX_PROB\", 0.25))\n",
        "    tail_frac    = float(cfg.get(\"FINE_TUNE_FRACTION\", 0.12))\n",
        "    tail_start_e = max(1, int((1.0 - tail_frac) * E))\n",
        "\n",
        "    history = []\n",
        "\n",
        "    for epoch in range(1, E + 1):\n",
        "        model.train()\n",
        "        frac = (epoch - 1) / max(1, E - 1)\n",
        "\n",
        "        # ----- Aug strength (optional cap late) -----\n",
        "        if cfg.get(\"USE_AUG\", True):\n",
        "            s = 0.2 + 0.6 * min(1.0, epoch / ramp_epochs)\n",
        "            if cfg.get(\"AUG_CAP_LATE\", True) and epoch >= int(0.7 * E):\n",
        "                s = min(s, 0.6)\n",
        "            augment = globals().get(\"FER_AUG_FACTORY\", lambda _: None)(s)  # no‑op if not defined\n",
        "        else:\n",
        "            augment = None\n",
        "\n",
        "        # ----- MixUp/CutMix taper -----\n",
        "        if frac < t_start:\n",
        "            p_mix, p_cut = base_mix_p, base_cut_p\n",
        "        elif frac > t_end:\n",
        "            p_mix, p_cut = 0.0, 0.0\n",
        "        else:\n",
        "            tf = (frac - t_start) / max(1e-8, (t_end - t_start))\n",
        "            p_mix, p_cut = base_mix_p * (1.0 - tf), base_cut_p * (1.0 - tf)\n",
        "\n",
        "        # ----- Fine‑tune tail (disable aug/mix; reduce LR group-wise) -----\n",
        "        if epoch >= tail_start_e:\n",
        "            augment = None\n",
        "            p_mix = p_cut = 0.0\n",
        "            for g in opt.param_groups:\n",
        "                g['lr'] = max(hp.get(\"LR_MIN\", 1e-6), 0.2 * float(hp[\"LR\"]))\n",
        "\n",
        "        # ---- Train one epoch ----\n",
        "        loss_sum, n_seen = 0.0, 0\n",
        "        for xb, yb in train_dl:\n",
        "            xb = xb.to(device, non_blocking=True)\n",
        "            yb = yb.to(device, non_blocking=True)\n",
        "\n",
        "            # (Inputs already normalized upstream; do NOT re-normalize.)\n",
        "            if augment is not None:\n",
        "                try:\n",
        "                    xb = augment(xb)\n",
        "                except Exception:\n",
        "                    pass  # keep training even if your custom augment is a no-op\n",
        "\n",
        "            # MixUp/CutMix (probabilistic) — only if helpers exist\n",
        "            have_mix = \"mixup_data\" in globals() and callable(globals()[\"mixup_data\"])\n",
        "            have_cut = \"cutmix_data\" in globals() and callable(globals()[\"cutmix_data\"])\n",
        "            use_mix = have_mix and cfg.get(\"USE_MIXUP\", True) and (random.random() < p_mix)\n",
        "            use_cut = (not use_mix) and have_cut and cfg.get(\"USE_CUTMIX\", True) and (random.random() < p_cut)\n",
        "\n",
        "            if use_mix:\n",
        "                xb2, (ya, yb2), lam, _ = globals()[\"mixup_data\"](xb, yb, alpha=hp.get(\"MIXUP_ALPHA\", 0.3))\n",
        "                xb, targets_mix, lam = xb2, (ya, yb2), float(lam)\n",
        "            elif use_cut:\n",
        "                xb2, (ya, yb2), lam, _ = globals()[\"cutmix_data\"](xb, yb, alpha=hp.get(\"CUTMIX_ALPHA\", 1.0))\n",
        "                xb, targets_mix, lam = xb2, (ya, yb2), float(lam)\n",
        "            else:\n",
        "                targets_mix, lam = yb, 1.0\n",
        "\n",
        "            with torch.cuda.amp.autocast(enabled=scaler.is_enabled()):\n",
        "                logits = model(xb)\n",
        "\n",
        "                # main loss via your SmoothedFocal\n",
        "                if isinstance(targets_mix, tuple):\n",
        "                    ya_, yb_ = targets_mix\n",
        "                    loss_main = lam * main_crit(logits, ya_) + (1.0 - lam) * main_crit(logits, yb_)\n",
        "                else:\n",
        "                    loss_main = main_crit(logits, targets_mix)\n",
        "\n",
        "                # light CE stabilizer (optional)\n",
        "                if ce_weight is not None:\n",
        "                    loss_aux = F.cross_entropy(logits, yb, weight=ce_weight)\n",
        "                    loss = 0.75 * loss_main + 0.25 * loss_aux\n",
        "                else:\n",
        "                    loss = loss_main\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            scaler.step(opt); scaler.update()\n",
        "            if use_batch_sched:\n",
        "                sched.step()\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            if ema_obj is not None and hasattr(ema_obj, \"update\"):\n",
        "                ema_obj.update(model)  # NOTE: pass `model` (fixes the earlier TypeError)\n",
        "\n",
        "            bs = xb.size(0)\n",
        "            loss_sum += float(loss) * bs\n",
        "            n_seen   += bs\n",
        "\n",
        "        train_loss = loss_sum / max(1, n_seen)\n",
        "\n",
        "        # Epoch-level scheduler step for cosine (Plateau steps after val)\n",
        "        if (not use_batch_sched) and str(cfg.get(\"SCHEDULER\", \"onecycle\")).lower() == \"cosine\":\n",
        "            sched.step()\n",
        "\n",
        "        # ---- Validation (apply EMA shadow if present) ----\n",
        "        model.eval()\n",
        "        if ema_obj is not None and hasattr(ema_obj, \"apply_shadow\"):\n",
        "            ema_obj.apply_shadow(model)\n",
        "\n",
        "        val_loss_sum, val_acc_sum, n_val = 0.0, 0.0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_dl:\n",
        "                xb = xb.to(device, non_blocking=True)\n",
        "                yb = yb.to(device, non_blocking=True)\n",
        "                logits = model(xb)\n",
        "                vloss = (F.cross_entropy(logits, yb, weight=ce_weight)\n",
        "                         if ce_weight is not None else F.cross_entropy(logits, yb))\n",
        "                vacc  = accuracy(logits, yb)  # your Cell‑08 metric\n",
        "                bs = xb.size(0)\n",
        "                val_loss_sum += float(vloss) * bs\n",
        "                val_acc_sum  += float(vacc)  * bs\n",
        "                n_val        += bs\n",
        "\n",
        "        val_loss = val_loss_sum / max(1, n_val)\n",
        "        val_acc  = val_acc_sum  / max(1, n_val)\n",
        "\n",
        "        if ema_obj is not None and hasattr(ema_obj, \"restore\"):\n",
        "            ema_obj.restore(model)\n",
        "\n",
        "        # Plateau reacts to validation loss\n",
        "        if (not use_batch_sched) and str(cfg.get(\"SCHEDULER\", \"onecycle\")).lower() == \"plateau\":\n",
        "            sched.step(val_loss)\n",
        "\n",
        "        # ---- Checkpoint on improved val_loss (unchanged) ----\n",
        "        if val_loss < best_val_loss - 1e-6:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save({\"model_state\": model.state_dict()}, CONFIG[\"SAVE_BEST_PATH\"])\n",
        "\n",
        "        # ---- Track best val_acc for gated logging ----\n",
        "        improved_acc = val_acc > best_val_acc + 1e-9\n",
        "        if improved_acc:\n",
        "            best_val_acc = val_acc\n",
        "\n",
        "        # ---- History (always append) ----\n",
        "        lr_now = float(opt.param_groups[0][\"lr\"])\n",
        "        history.append({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_acc\":  val_acc,\n",
        "            \"best_val_acc\": best_val_acc,\n",
        "            \"lr\": lr_now\n",
        "        })\n",
        "\n",
        "        # ---- Sparse console logging ----\n",
        "        if (epoch % PRINT_EVERY == 0) or (epoch == E) or improved_acc:\n",
        "            print(f\"[{epoch:03d}/{E}] \"\n",
        "                  f\"train={train_loss:.4f}  val={val_loss:.4f}  acc={val_acc:.4f}  \"\n",
        "                  f\"best_acc={best_val_acc:.4f}  lr={lr_now:.2e}\")\n",
        "\n",
        "    return history, ema_obj\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19e00e83",
      "metadata": {
        "id": "19e00e83"
      },
      "source": [
        "#Cell 15 — Build model and quick probe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0_nFfLM2-_sZ",
      "metadata": {
        "id": "0_nFfLM2-_sZ"
      },
      "source": [
        "## Baseline Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0598310a",
      "metadata": {
        "id": "0598310a"
      },
      "source": [
        "### Cell (Aug Utils) — Localized Erasing\n",
        "\n",
        "**Logic:**\n",
        "This utility defines `localized_erasing`, a random erasure augmentation similar to *Random Erasing* in CV literature. It randomly removes a rectangular patch of the image and fills it with a neutral mid-gray value consistent with the input’s numeric range.\n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "1. **Fill Value (`_neutral_fill_value`)**\n",
        "   - Chooses mid-gray consistent with the image’s range:\n",
        "     - `[-1,1]` → `0.0`\n",
        "     - `[0,1]` → `0.5`\n",
        "     - `[0,255]` → `127.5`\n",
        "   - *Reasoning:* Avoids biasing the model with unnatural patches (pure black/white).\n",
        "\n",
        "2. **Patch Size**\n",
        "   - Computes target erase area as a fraction of image area (`min_frac`–`max_frac`).  \n",
        "   - Chooses random aspect ratio in `[0.3, 3.3]`.  \n",
        "   - Derives patch height (`eh`) and width (`ew`).  \n",
        "   - *Reasoning:* Varying size/shape of erased regions simulates diverse occlusions.\n",
        "\n",
        "3. **Patch Placement**\n",
        "   - Randomly selects `(top, left)` location within image bounds.  \n",
        "   - Applies fill to rectangular region `[top:top+eh, left:left+ew]`.  \n",
        "   - Skips if patch is degenerate (too small/large).  \n",
        "   - *Reasoning:* Random positioning ensures model cannot rely on specific regions.\n",
        "\n",
        "4. **Return**\n",
        "   - Outputs a clone of input with erased patch.  \n",
        "   - *Reasoning:* Non-destructive to original input, safe for augmentation pipelines.\n",
        "\n",
        "---\n",
        "\n",
        "**Why This Design?**\n",
        "- **Occlusion Robustness:** Trains the model to recognize expressions even when part of the face is missing (hands, glasses, masks, shadows).  \n",
        "- **Regularization:** Forces reliance on distributed cues, reducing overfitting to small regions (e.g., only mouth or only eyes).  \n",
        "- **Consistency:** Range-aware filling prevents unrealistic artifacts.  \n",
        "- **Compatibility:** Works with all common image scales used in preprocessing.\n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "`localized_erasing` simulates real-world partial occlusions by removing random patches of the face. It acts as a *strong regularizer* in the augmentation bank, improving robustness and generalization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1542643e",
      "metadata": {
        "id": "1542643e"
      },
      "source": [
        "Cell 16 — Train (main run)\n",
        "# === Cell 16: Train main run ==="
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 15: EMA (safe) + single-model training wrapper ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# --- Safe EMA: skips non-floating tensors; dtype/device match per param ---\n",
        "class EMA:\n",
        "    def __init__(self, model: nn.Module, decay: float = 0.9995):\n",
        "        self.decay = float(decay)\n",
        "        self.shadow = {}\n",
        "        # initialise shadow with a copy of the current (floating) weights\n",
        "        for k, v in model.state_dict().items():\n",
        "            if torch.is_floating_point(v):\n",
        "                self.shadow[k] = v.detach().clone()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, model: nn.Module):\n",
        "        d = self.decay\n",
        "        for k, v in model.state_dict().items():\n",
        "            if not torch.is_floating_point(v):\n",
        "                # integers/bool buffers (e.g., num_batches_tracked) are copied raw\n",
        "                continue\n",
        "            if k not in self.shadow:\n",
        "                self.shadow[k] = v.detach().clone()\n",
        "            # keep shadow on the same device/dtype as the live param\n",
        "            if self.shadow[k].device != v.device or self.shadow[k].dtype != v.dtype:\n",
        "                self.shadow[k] = self.shadow[k].to(device=v.device, dtype=v.dtype)\n",
        "            # shadow = d*shadow + (1-d)*v\n",
        "            self.shadow[k].mul_(d).add_(v.detach(), alpha=(1.0 - d))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def apply_shadow(self, model: nn.Module):\n",
        "        self._backup = {}\n",
        "        for k, v in model.state_dict().items():\n",
        "            if k in self.shadow and torch.is_floating_point(v):\n",
        "                self._backup[k] = v.detach().clone()\n",
        "                model.state_dict()[k].copy_(self.shadow[k])\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def restore(self, model: nn.Module):\n",
        "        # restore only those we backed up\n",
        "        if not hasattr(self, \"_backup\"):\n",
        "            return\n",
        "        for k, v in model.state_dict().items():\n",
        "            if k in self._backup and torch.is_floating_point(v):\n",
        "                model.state_dict()[k].copy_(self._backup[k])\n",
        "        self._backup = {}\n",
        "\n",
        "# --- Single-model trainer wrapper used by the batch loop ---\n",
        "from pathlib import Path\n",
        "\n",
        "def train_one(backbone_tag: str):\n",
        "    \"\"\"\n",
        "    Builds the model for `backbone_tag`, trains it with your Cell 14 `fit_with_aug`,\n",
        "    saves a unique checkpoint, and returns (model, history, ema_obj).\n",
        "    \"\"\"\n",
        "    # build model\n",
        "    model = build_model(backbone_tag)\n",
        "\n",
        "    # route best-checkpoint path so Cell 14 can write without clashes\n",
        "    save_path = ckpt_path_for(backbone_tag)\n",
        "    CONFIG[\"SAVE_BEST_PATH\"] = str(save_path)\n",
        "\n",
        "    print(f\"\\n=== TRAINING {backbone_tag.upper()} MODEL ===\")\n",
        "    history, ema_obj = fit_with_aug(model, train_dl, val_dl, HP, CONFIG)\n",
        "\n",
        "    # always save the final weights too (in case best == last is not true)\n",
        "    torch.save({\"model_state\": model.state_dict()}, save_path)\n",
        "    print(f\"[{backbone_tag}] checkpoint saved → {save_path}\")\n",
        "\n",
        "    return model, history, ema_obj\n"
      ],
      "metadata": {
        "id": "JpexMeqxv86b"
      },
      "id": "JpexMeqxv86b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16f302e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685,
          "referenced_widgets": [
            "870146efa1da47b695e2c73011f0160c",
            "453a903497d347599c749c8e4b16715f",
            "8642576775984af18266bf7076d2da2a",
            "9cadcc229b4c47dd9581be483bfdcc37",
            "c3faa1a2802245b6ae5939ebe878f99a",
            "a4c39e2e28ef47e2aa8a06548395d50a",
            "49a75aa201394996851c676b93fa7a5b",
            "1dc2b7a2a4c6415591de96e0226e67f9",
            "8ffec57b32be441a9dd3984d4b55fd78",
            "4c40cc2da4744e868ff9ac999dd78bf8",
            "3fd8fd786e8543148ca04cf046bf237a"
          ]
        },
        "id": "16f302e7",
        "outputId": "9fe34b4e-f42c-4d3e-c508-1d88192c320c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 169MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TRAINING EFFNET_B0 MODEL ===\n",
            "[001/11] train=nan  val=nan  acc=0.1301  best_acc=0.1301  lr=6.97e-04\n",
            "[010/11] train=nan  val=nan  acc=0.1301  best_acc=0.1301  lr=3.14e-05\n",
            "[011/11] train=nan  val=nan  acc=0.1301  best_acc=0.1301  lr=4.17e-06\n",
            "[effnet_b0] checkpoint saved → /content/project/checkpoints/best_effnet_b0_fer.pth\n",
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_small-047dcff4.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.83M/9.83M [00:00<00:00, 119MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== TRAINING MOBILENET_V3 MODEL ===\n",
            "[001/11] train=nan  val=nan  acc=0.1301  best_acc=0.1301  lr=6.97e-04\n",
            "[010/11] train=nan  val=nan  acc=0.1301  best_acc=0.1301  lr=3.14e-05\n",
            "[011/11] train=nan  val=nan  acc=0.1301  best_acc=0.1301  lr=4.17e-06\n",
            "[mobilenet_v3] checkpoint saved → /content/project/checkpoints/best_mobilenet_v3_fer.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/25.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "870146efa1da47b695e2c73011f0160c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 960, 1, 1])",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3218336162.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrained_models\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMODELS_TO_RUN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mema_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtrained_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ema\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mema_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"history\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3925927901.py\u001b[0m in \u001b[0;36mtrain_one\u001b[0;34m(backbone_tag)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \"\"\"\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# build model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# route best-checkpoint path so Cell 14 can write without clashes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4228768506.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(backbone_tag)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFER_MobileNetV3S_CBAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ghostnet_v2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFER_GhostNetV2_CBAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unknown backbone: {backbone_tag}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-652336778.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes, classifier_dropout, use_cbam)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mdummy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"IMG_SIZE\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"IMG_SIZE\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msobel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcbam\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mCBAM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cbam\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdaptiveAvgPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/timm/models/ghostnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/timm/models/ghostnet.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/timm/models/ghostnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;31m# 1st ghost bottleneck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mghost1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Depth-wise convolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/timm/models/ghostnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheap_operation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \"\"\"\n\u001b[0;32m--> 193\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2813\u001b[0m         )\n\u001b[1;32m   2814\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2815\u001b[0;31m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2817\u001b[0m     return torch.batch_norm(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2779\u001b[0m         \u001b[0msize_prods\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2780\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_prods\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2781\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2782\u001b[0m             \u001b[0;34mf\"Expected more than 1 value per channel when training, got input size {size}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2783\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 960, 1, 1])"
          ]
        }
      ],
      "source": [
        "# === Cell 16: Batch train selected models (full run) ===\n",
        "trained_models = {}\n",
        "for tag in MODELS_TO_RUN:\n",
        "    model, history, ema_obj = train_one(tag)\n",
        "    trained_models[tag] = {\"model\": model, \"ema\": ema_obj, \"history\": history}\n",
        "\n",
        "print(\"\\n[Train] Completed models:\", \", \".join(trained_models.keys()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c03e38",
      "metadata": {
        "id": "13c03e38"
      },
      "source": [
        "# === Cell 18: Evaluation (val clean; test clean + optional TTA) ==="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d9e4b88",
      "metadata": {
        "id": "3d9e4b88"
      },
      "source": [
        "### Cell pre-18 — BatchNorm Recalibration & Clean Evaluation\n",
        "\n",
        "**Logic:**\n",
        "This cell recalibrates **BatchNorm (BN) running statistics** at the end of training, ensuring they reflect *clean (non-augmented)* data distribution. It then performs a quick validation/test evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "**Step-by-Step Reasoning:**\n",
        "\n",
        "1. **BN Presence Check**\n",
        "   - `has_batchnorm(model)`: scans model for BatchNorm layers.\n",
        "   - *Reasoning:* Recalibration only makes sense if BN layers exist.\n",
        "\n",
        "2. **Evaluation Transform**\n",
        "   - `_get_eval_transform()`: if the validation dataset has a defined transform, reuse it as the \"clean\" transform.  \n",
        "   - *Reasoning:* Ensures recalibration uses the same normalization pipeline as evaluation.\n",
        "\n",
        "3. **Clean Train Loader**\n",
        "   - `_TransformView`: wraps training dataset with evaluation transform (disables augmentation).  \n",
        "   - Constructs `train_dl_clean` (same batch size and worker config as `val_dl`).  \n",
        "   - *Reasoning:* BN recalibration requires streaming **realistic clean data**, not augmented samples.\n",
        "\n",
        "4. **BN Recalibration**\n",
        "   - `update_bn`: feeds clean data through the model in training mode without weight updates.  \n",
        "   - Updates BN running means and variances.  \n",
        "   - Preserves original `train/eval` mode after recalibration.  \n",
        "   - *Reasoning:* During heavy augmentation training, BN stats may drift; recalibration realigns them to the clean test distribution.\n",
        "\n",
        "5. **Optional EMA Integration**\n",
        "   - Commented block suggests applying EMA weights before recalibration, then restoring after.  \n",
        "   - *Reasoning:* Ensures the final BN stats are aligned with the weights used for evaluation.\n",
        "\n",
        "6. **Quick Evaluation**\n",
        "   - `_eval_quick`: Runs validation and test sets with clean normalization.  \n",
        "   - Reports post-BN recalibration accuracy (`val_post`, `test_post`).  \n",
        "   - *Reasoning:* Confirms recalibration improves or stabilizes final evaluation metrics.\n",
        "\n",
        "7. **Fallback**\n",
        "   - If no BN layers are present, skips recalibration entirely.  \n",
        "   - *Reasoning:* Models without BN (e.g., LayerNorm, GroupNorm) don’t require this.\n",
        "\n",
        "---\n",
        "\n",
        "**Why This Design?**\n",
        "- **BN sensitivity:** BN layers depend on running statistics; heavy augmentation corrupts them.  \n",
        "- **Recalibration step:** Provides *true distribution statistics* from clean data before final evaluation.  \n",
        "- **Lightweight:** No training updates, only a forward pass through clean data.  \n",
        "- **Robust final eval:** Prevents mismatch between training (augmented) and inference (clean) distributions.  \n",
        "\n",
        "---\n",
        "\n",
        "**Key Idea:**  \n",
        "BN recalibration acts as a **post-training correction step**, ensuring BatchNorm statistics reflect the clean test distribution. This yields more reliable and often higher validation/test accuracy without retraining.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZNyRyb5kYMFY",
      "metadata": {
        "id": "ZNyRyb5kYMFY"
      },
      "outputs": [],
      "source": [
        "# === Cell 18: Evaluation helpers (BN recal + clean eval)\n",
        "# Purpose:\n",
        "#   • eval_loader(): uses the PDF’s `accuracy(logits, yb)` helper (no extra normalization here).\n",
        "#   • recalibrate_bn(): updates BN running stats on a *clean* (no-aug, normalized) stream.\n",
        "\n",
        "from torch.optim.swa_utils import update_bn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def _dev_of(model):\n",
        "    return next(model.parameters()).device\n",
        "\n",
        "def has_batchnorm(m: nn.Module) -> bool:\n",
        "    return any(isinstance(x, nn.modules.batchnorm._BatchNorm) for x in m.modules())\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_loader(model, loader):\n",
        "    \"\"\"\n",
        "    IMPORTANT:\n",
        "      - Assumes the loader already yields normalized tensors, as in the PDF (ToTensor + Normalize([0.5],[0.5])).\n",
        "      - Uses the provided `accuracy(logits, yb)` helper from your exam/PDF code.\n",
        "    \"\"\"\n",
        "    # Safety: ensure the helper exists, so failures are obvious.\n",
        "    assert 'accuracy' in globals(), \"Missing `accuracy` helper expected by eval_loader().\"\n",
        "    model.eval()\n",
        "    dev = _dev_of(model)\n",
        "    acc_sum, n_batches = 0.0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(dev, non_blocking=True)\n",
        "        yb = yb.to(dev, non_blocking=True)\n",
        "        # DO NOT re‑normalize here; rely on dataset transforms.\n",
        "        logits = model(xb)\n",
        "        acc_sum += accuracy(logits, yb).item()\n",
        "        n_batches += 1\n",
        "    return acc_sum / max(1, n_batches)\n",
        "\n",
        "# ---- Normalization‑aware BN recalibration -----------------------------------\n",
        "def _get_eval_transform():\n",
        "    \"\"\"Inherit the evaluation transform from val loader if present.\"\"\"\n",
        "    if 'val_dl' in globals() and hasattr(val_dl, 'dataset') and hasattr(val_dl.dataset, 'transform'):\n",
        "        return val_dl.dataset.transform\n",
        "    return None\n",
        "\n",
        "EVAL_TF = _get_eval_transform()\n",
        "\n",
        "class _TransformView(Dataset):\n",
        "    \"\"\"\n",
        "    Wrap a Dataset and override its transform so we can feed *clean*,\n",
        "    normalized batches to update_bn(). If the base dataset is already\n",
        "    normalized by its transform, we simply return x as is.\n",
        "    \"\"\"\n",
        "    def __init__(self, base_ds, transform):\n",
        "        self.base = base_ds\n",
        "        self.transform = transform\n",
        "    def __len__(self):  return len(self.base)\n",
        "    def __getitem__(self, i):\n",
        "        x, y = self.base[i]\n",
        "        # If original dataset already applies Normalize, leave `x` unchanged.\n",
        "        # Otherwise (rare) you’d apply explicit normalization here.\n",
        "        return x, y\n",
        "\n",
        "# Build a clean (no aug) train-like stream for BN recalibration\n",
        "if EVAL_TF is not None:\n",
        "    train_ds_clean = _TransformView(train_dl.dataset, EVAL_TF)\n",
        "    train_dl_clean = DataLoader(\n",
        "        train_ds_clean,\n",
        "        batch_size=val_dl.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=val_dl.num_workers,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=getattr(val_dl, 'persistent_workers', False)\n",
        "    )\n",
        "else:\n",
        "    print(\"[BN][WARN] No eval transform detected; reusing train_dl (assumed normalized).\")\n",
        "    train_dl_clean = train_dl\n",
        "\n",
        "def recalibrate_bn(model, train_like_loader=train_dl_clean):\n",
        "    \"\"\"\n",
        "    Recompute BN running stats on a clean, normalized stream.\n",
        "    No extra normalization is applied here; we trust the dataset’s transform.\n",
        "    \"\"\"\n",
        "    if not has_batchnorm(model):\n",
        "        print(\"[BN] No BN layers; skipping recalibration.\")\n",
        "        return\n",
        "    dev = _dev_of(model)\n",
        "    was_training = model.training\n",
        "    model.train()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _clean_iter(dloader):\n",
        "        for xb, _ in dloader:\n",
        "            yield xb.to(dev, non_blocking=True)  # already normalized by dataset transforms\n",
        "\n",
        "    update_bn(_clean_iter(train_like_loader), model)\n",
        "    model.train(was_training); model.eval()\n",
        "    print(\"[BN] Done.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "019ad855",
      "metadata": {
        "id": "019ad855"
      },
      "source": [
        "#=== Cell 19: Reload + evaluate each model (val/test) — using exam accuracy ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74d24fd0",
      "metadata": {
        "id": "74d24fd0"
      },
      "outputs": [],
      "source": [
        "# === Cell 17: Reload + evaluate each model (val/test) ===\n",
        "results = {}\n",
        "for tag in MODELS_TO_RUN:\n",
        "    path = ckpt_path_for(tag)\n",
        "    print(f\"\\n[Eval] Loading {tag} from {path}\")\n",
        "    model = build_model(tag)\n",
        "    ckpt  = torch.load(path, map_location=device)\n",
        "    state = ckpt.get(\"model_state\", ckpt)  # tolerate raw state_dict or wrapped\n",
        "    msg = model.load_state_dict(state, strict=False)\n",
        "    if getattr(msg, \"missing_keys\", None) or getattr(msg, \"unexpected_keys\", None):\n",
        "        print(\"[warn] load_state_dict mismatches:\", msg)\n",
        "\n",
        "    # Recalibrate BN on a clean, normalized stream (no aug)\n",
        "    recalibrate_bn(model, train_dl_clean if 'train_dl_clean' in globals() else train_dl)\n",
        "\n",
        "    # PDF-logic eval (no explicit normalization here)\n",
        "    val_acc  = eval_loader(model, val_dl)\n",
        "    test_acc = eval_loader(model, test_dl)\n",
        "    print(f\"[{tag}] val={val_acc:.4f}  test={test_acc:.4f}\")\n",
        "    results[tag] = {\"val\": val_acc, \"test\": test_acc}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30d0d0d3",
      "metadata": {
        "id": "30d0d0d3"
      },
      "source": [
        "#=== Cell 20: Side‑by‑side summary (tabular) ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dff455b",
      "metadata": {
        "id": "1dff455b"
      },
      "outputs": [],
      "source": [
        "#=== Cell 20: Side‑by‑side summary (tabular) ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a89eb770",
      "metadata": {
        "id": "a89eb770"
      },
      "outputs": [],
      "source": [
        "# === Cell 18: Side‑by‑side summary ===\n",
        "import pandas as pd\n",
        "df_results = pd.DataFrame.from_dict(\n",
        "    {k: {**v, \"ckpt\": str(ckpt_path_for(k))} for k, v in results.items()},\n",
        "    orient=\"index\"\n",
        ")[[\"val\", \"test\", \"ckpt\"]]\n",
        "print(df_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "608eb229",
      "metadata": {
        "id": "608eb229"
      },
      "source": [
        "# === Cell 19: Confusion matrix + per-class accuracy (test) ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36653a3e",
      "metadata": {
        "id": "36653a3e"
      },
      "outputs": [],
      "source": [
        "# === Cell 19: Confusion matrix + per‑class accuracy (for any model) ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "IDX2EMO = {0:'Angry',1:'Disgust',2:'Fear',3:'Happy',4:'Sad',5:'Surprise',6:'Neutral'}\n",
        "\n",
        "@torch.no_grad()\n",
        "def confusion_matrix_and_report(model: nn.Module,\n",
        "                                loader,\n",
        "                                num_classes: int = 7,\n",
        "                                normalize_inputs: bool = True,\n",
        "                                title: str | None = None):\n",
        "    \"\"\"\n",
        "    Computes a confusion matrix on `loader`, plots it, and prints per‑class accuracy.\n",
        "    Normalization matches the notebook's eval path: (x/255 - 0.5)*2 when `normalize_inputs=True`.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    dev = next(model.parameters()).device\n",
        "\n",
        "    cm = torch.zeros(num_classes, num_classes, dtype=torch.int64, device=dev)\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(dev, non_blocking=True)\n",
        "        yb = yb.to(dev, non_blocking=True)\n",
        "        if normalize_inputs:\n",
        "            xb = ((xb / 255.) - 0.5) * 2.0\n",
        "        pred = model(xb).argmax(1)\n",
        "        for t, p in zip(yb.view(-1), pred.view(-1)):\n",
        "            cm[t.long(), p.long()] += 1\n",
        "\n",
        "    cm_cpu = cm.cpu()\n",
        "    totals = cm_cpu.sum(1).clamp(min=1)\n",
        "    per_class = (cm_cpu.diag() / totals).numpy() * 100.0\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.imshow(cm_cpu, interpolation='nearest')\n",
        "    plt.title(title or \"Confusion (rows=true, cols=pred)\")\n",
        "    plt.colorbar(fraction=0.046, pad=0.04)\n",
        "    plt.xticks(range(num_classes), [IDX2EMO[i] for i in range(num_classes)], rotation=45, ha='right')\n",
        "    plt.yticks(range(num_classes), [IDX2EMO[i] for i in range(num_classes)])\n",
        "    for i, j in itertools.product(range(num_classes), range(num_classes)):\n",
        "        n = cm_cpu[i, j].item()\n",
        "        if n > 0:\n",
        "            plt.text(j, i, n, ha='center', va='center', fontsize=7)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    for i, acc in enumerate(per_class):\n",
        "        print(f\"{IDX2EMO[i]:>8s}: {acc:.2f}%\")\n",
        "    return cm_cpu, per_class\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3b467f9",
      "metadata": {
        "id": "c3b467f9"
      },
      "source": [
        "#=== Cell 22: Reload → BN recalibration → evaluate all models (val/test) ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff14a927",
      "metadata": {
        "id": "ff14a927"
      },
      "outputs": [],
      "source": [
        "# === Cell 20: Reload → BN recalibration → evaluate all models (val/test) ===\n",
        "from torch.optim.swa_utils import update_bn\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "def has_batchnorm(m: nn.Module) -> bool:\n",
        "    return any(isinstance(x, nn.modules.batchnorm._BatchNorm) for x in m.modules())\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_loader(model: nn.Module, loader) -> float:\n",
        "    \"\"\"\n",
        "    Returns mean top‑1 accuracy (fraction) across the loader.\n",
        "    Uses the same evaluation normalization as your earlier cells: (x/255 - 0.5)*2.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    dev = next(model.parameters()).device\n",
        "    acc_sum, n_batches = 0.0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(dev, non_blocking=True)\n",
        "        yb = yb.to(dev, non_blocking=True)\n",
        "        xb = ((xb / 255.) - 0.5) * 2.0\n",
        "        logits = model(xb)\n",
        "        acc_sum += (logits.argmax(1) == yb).float().mean().item()\n",
        "        n_batches += 1\n",
        "    return acc_sum / max(1, n_batches)\n",
        "\n",
        "def _clean_stream_from(train_dl_like):\n",
        "    \"\"\"\n",
        "    Iterator that yields only normalized inputs for BN running‑stat recalibration.\n",
        "    Matches eval normalization: (x/255 - 0.5)*2.\n",
        "    \"\"\"\n",
        "    dev = next(iter(train_dl_like))[0].device if hasattr(train_dl_like, '__iter__') else device\n",
        "    @torch.no_grad()\n",
        "    def _gen():\n",
        "        for xb, _ in train_dl_like:\n",
        "            xb = xb.to(dev, non_blocking=True)\n",
        "            xb = ((xb / 255.) - 0.5) * 2.0\n",
        "            yield xb\n",
        "    return _gen()\n",
        "\n",
        "def recalibrate_bn(model: nn.Module, train_dl_like):\n",
        "    \"\"\"Recomputes BN running stats on a clean (no‑aug) stream.\"\"\"\n",
        "    if not has_batchnorm(model):\n",
        "        return\n",
        "    was_training = model.training\n",
        "    model.train()\n",
        "    update_bn(_clean_stream_from(train_dl_like), model)\n",
        "    model.train(was_training); model.eval()\n",
        "\n",
        "# Evaluate all requested models (MODELS_TO_RUN should be defined earlier)\n",
        "results = {}\n",
        "loaded_models = {}   # keep the constructed model objects for later cells (plots, FLOPs, CM)\n",
        "for tag in MODELS_TO_RUN:\n",
        "    path = ckpt_path_for(tag)\n",
        "    print(f\"\\n[Eval] Loading {tag} from {path}\")\n",
        "    m = build_model(tag)\n",
        "\n",
        "    # Load state dict (we saved {\"model_state\": ...})\n",
        "    ckpt = torch.load(path, map_location=device)\n",
        "    state = ckpt.get(\"model_state\", ckpt)\n",
        "    msg = m.load_state_dict(state, strict=False)\n",
        "    if getattr(msg, \"missing_keys\", None) or getattr(msg, \"unexpected_keys\", None):\n",
        "        print(\"[warn] load_state_dict mismatches:\", msg)\n",
        "\n",
        "    # BN recalibration on clean training stream (your train_dl is already DeviceDataLoader)\n",
        "    recalibrate_bn(m, train_dl)\n",
        "\n",
        "    # Clean evaluation\n",
        "    val_acc  = eval_loader(m, val_dl)\n",
        "    test_acc = eval_loader(m, test_dl)\n",
        "    print(f\"[{tag}] val={val_acc:.4f}  test={test_acc:.4f}\")\n",
        "    results[tag] = {\"val\": val_acc, \"test\": test_acc}\n",
        "    loaded_models[tag] = m\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8da69e38",
      "metadata": {
        "id": "8da69e38"
      },
      "source": [
        "#=== Cell 25: Accuracy/Loss curves (handles multiple histories) ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HIwXyFezq_Sf",
      "metadata": {
        "id": "HIwXyFezq_Sf"
      },
      "outputs": [],
      "source": [
        "# === Cell 23: Accuracy/Loss curves (handles multiple histories) ===\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def _plot_history(history, label_prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Expects history to be a list[dict] with keys: epoch, train_loss, val_loss, train_acc, val_acc, (optional) test_acc.\n",
        "    The trainer you’re using should already populate a similar structure.\n",
        "    \"\"\"\n",
        "    if not history or not isinstance(history, list):\n",
        "        return False\n",
        "\n",
        "    epochs     = [m.get('epoch', i+1) for i, m in enumerate(history)]\n",
        "    train_loss = [m.get('train_loss') for m in history]\n",
        "    val_loss   = [m.get('val_loss')   for m in history]\n",
        "    train_acc  = [m.get('train_acc')  for m in history]\n",
        "    val_acc    = [m.get('val_acc')    for m in history]\n",
        "    test_acc   = [m.get('test_acc')   for m in history] if history and 'test_acc' in history[0] else None\n",
        "\n",
        "    plt.figure(figsize=(12,5))\n",
        "\n",
        "    # 1) Accuracy\n",
        "    plt.subplot(1,2,1)\n",
        "    if train_acc[0] is not None:\n",
        "        plt.plot(epochs, train_acc, marker='o', label=f\"{label_prefix}Train Acc\")\n",
        "    plt.plot(epochs, val_acc, marker='o', label=f\"{label_prefix}Val Acc\")\n",
        "    if test_acc is not None and any(v is not None for v in test_acc):\n",
        "        plt.plot(epochs, test_acc, marker='x', label=f\"{label_prefix}Test Acc\")\n",
        "    plt.title(f\"Accuracy — {label_prefix}\".strip())\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Top‑1 Acc\"); plt.grid(True); plt.legend()\n",
        "\n",
        "    # 2) Loss\n",
        "    plt.subplot(1,2,2)\n",
        "    if train_loss[0] is not None:\n",
        "        plt.plot(epochs, train_loss, marker='o', label=f\"{label_prefix}Train Loss\")\n",
        "    plt.plot(epochs, val_loss, marker='o', label=f\"{label_prefix}Val Loss\")\n",
        "    plt.title(f\"Loss — {label_prefix}\".strip())\n",
        "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.grid(True); plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return True\n",
        "\n",
        "# If you trained in this session via Cell 15, `trained_models[tag]['history']` will exist.\n",
        "for tag in MODELS_TO_RUN:\n",
        "    hist = trained_models.get(tag, {}).get(\"history\", None)\n",
        "    if hist:\n",
        "        _ = _plot_history(hist, label_prefix=f\"{tag} \")\n",
        "    else:\n",
        "        print(f\"[Curves] No history found for '{tag}' (skipping).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a898e2e",
      "metadata": {
        "id": "1a898e2e"
      },
      "source": [
        "### Analysis of Training Curves\n",
        "\n",
        "#### Accuracy Curves (left plot)\n",
        "- Validation accuracy starts around **35%** and rises steadily.  \n",
        "- By **epoch ~10**, accuracy is already **60%+**, showing rapid early learning.  \n",
        "- It then **plateaus between 66–69%**, with small oscillations (expected under augmentation/regularization).  \n",
        "- By **epoch 55–60**, it stabilizes close to **70%**.  \n",
        "\n",
        "**Interpretation:**  \n",
        "- The model converges well and generalizes up to ~70% validation accuracy.  \n",
        "- The plateau after ~20 epochs shows **diminishing returns** — most learning happens early, and fine-tuning/taper schedules carry it to ~70%.  \n",
        "- The small oscillations suggest some sensitivity to augmentations or learning rate scheduling, but the trend is stable.  \n",
        "\n",
        "---\n",
        "\n",
        "#### Loss Curves (right plot)\n",
        "- **Train Loss** decreases smoothly from ~2.0 to <0.25.  \n",
        "- **Validation Loss** decreases sharply at first, bottoms out around **epoch 15–20**, then starts fluctuating and slowly increasing.  \n",
        "\n",
        "**Interpretation:**  \n",
        "- Training loss continues to fall → the model fits training data strongly.  \n",
        "- Validation loss diverges slightly after ~20 epochs while validation accuracy stays flat.  \n",
        "- This indicates **mild overfitting**: the model memorizes training patterns but does not gain further generalization.  \n",
        "- Augmentation + label smoothing prevented collapse, but the upward trend in validation loss shows the network is at its **effective capacity**.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d19e1269",
      "metadata": {
        "id": "d19e1269"
      },
      "source": [
        "# === Cell 20: FLOPs (fvcore) + Accuracy/GFLOP ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DuMfrwctVKMr",
      "metadata": {
        "id": "DuMfrwctVKMr"
      },
      "outputs": [],
      "source": [
        "%pip install fvcore\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dd8f669",
      "metadata": {
        "id": "2dd8f669"
      },
      "source": [
        "#=== Cell 26: FLOPs + efficiency (acc% / GFLOP) for all models ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "050a90f1",
      "metadata": {
        "id": "050a90f1"
      },
      "outputs": [],
      "source": [
        "# === Cell 24: FLOPs + efficiency (acc% / GFLOP) for all models ===\n",
        "try:\n",
        "    from fvcore.nn import FlopCountAnalysis\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"fvcore\"])\n",
        "    from fvcore.nn import FlopCountAnalysis\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def _probe_in_channels(m: nn.Module, img_sz: int, dev, dtype) -> int:\n",
        "    \"\"\"\n",
        "    Our backbones accept 1‑ch inputs (Sobel expands to 3 inside), but some\n",
        "    wrappers may accept 3 directly. Probe 1 then 3 safely.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        for c in (1, 3):\n",
        "            try:\n",
        "                _ = m(torch.zeros(1, c, img_sz, img_sz, device=dev, dtype=dtype))\n",
        "                return c\n",
        "            except Exception:\n",
        "                pass\n",
        "    # Fallback to first conv’s channel count\n",
        "    for mod in m.modules():\n",
        "        if isinstance(mod, nn.Conv2d):\n",
        "            return int(mod.weight.shape[1])\n",
        "    return 1\n",
        "\n",
        "def _accuracy_for_efficiency(tag: str) -> float | None:\n",
        "    # Prefer the measured test accuracy from Cell 20\n",
        "    rec = results.get(tag, None)\n",
        "    if rec and isinstance(rec.get(\"test\", None), (int, float)):\n",
        "        return float(rec[\"test\"])\n",
        "    # Otherwise attempt a quick pass on test set (rare)\n",
        "    m = loaded_models.get(tag, None)\n",
        "    if m is None:\n",
        "        return None\n",
        "    return eval_loader(m, test_dl)\n",
        "\n",
        "eff_table = []\n",
        "IMG_SIZE = int(CONFIG.get(\"IMG_SIZE\", 96))\n",
        "\n",
        "for tag, model in loaded_models.items():\n",
        "    model.eval()\n",
        "    dev   = next(model.parameters()).device\n",
        "    dtype = next(model.parameters()).dtype\n",
        "    in_ch = _probe_in_channels(model, IMG_SIZE, dev, dtype)\n",
        "\n",
        "    dummy = torch.zeros(1, in_ch, IMG_SIZE, IMG_SIZE, device=dev, dtype=dtype)\n",
        "    flops = float(FlopCountAnalysis(model, dummy).total())   # operations\n",
        "    gflops = flops / 1e9\n",
        "\n",
        "    acc = _accuracy_for_efficiency(tag)  # fraction\n",
        "    eff = (acc * 100.0) / gflops if (isinstance(acc, (int, float)) and gflops > 0) else None\n",
        "\n",
        "    eff_table.append({\"model\": tag, \"GFLOPs\": gflops, \"acc_test\": acc, \"efficiency_%/GFLOP\": eff})\n",
        "\n",
        "eff_df = pd.DataFrame(eff_table).set_index(\"model\").sort_values(\"efficiency_%/GFLOP\", ascending=False)\n",
        "print(eff_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OFUc1dZ5vcIz",
      "metadata": {
        "id": "OFUc1dZ5vcIz"
      },
      "outputs": [],
      "source": [
        "# === Cell: FLOPs, GFLOPs, and Efficiency (CONFIG-aware, no required IN_CHANNELS) ===\n",
        "# Uses CONFIG[\"IMG_SIZE\"] if set; otherwise falls back to 48.\n",
        "# If CONFIG[\"IN_CHANNELS\"] is not set, it probes the model (1→3) safely.\n",
        "\n",
        "# fvcore import\n",
        "try:\n",
        "    from fvcore.nn import FlopCountAnalysis\n",
        "except Exception:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"fvcore\"])\n",
        "    from fvcore.nn import FlopCountAnalysis\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ---------- Model / device ----------\n",
        "model.eval()\n",
        "_dev   = next(model.parameters()).device\n",
        "_dtype = next(model.parameters()).dtype\n",
        "\n",
        "# ---------- Read from CONFIG (no hardcoding) ----------\n",
        "_cfg = globals().get(\"CONFIG\", {})\n",
        "IMG_SIZE    = int(_cfg.get(\"IMG_SIZE\", 48))              # you set 98; this will use it\n",
        "IN_CHANNELS = _cfg.get(\"IN_CHANNELS\", None)              # optional\n",
        "\n",
        "# Optional eval normalisation config; default matches your eval path\n",
        "NORM_SPEC = _cfg.get(\"EVAL_NORMALIZE\", {\"mode\": \"minus1to1\", \"scale_255\": True})\n",
        "LOADER_PREF = _cfg.get(\"EVAL_LOADER_PRIORITY\",\n",
        "                       [\"test_dl_hybrid\",\"test_dl\",\"val_dl_hybrid\",\"val_dl\"])\n",
        "\n",
        "def _apply_eval_norm(x: torch.Tensor) -> torch.Tensor:\n",
        "    mode = str(NORM_SPEC.get(\"mode\", \"minus1to1\")).lower()\n",
        "    scale_255 = bool(NORM_SPEC.get(\"scale_255\", True))\n",
        "    if mode == \"meanstd\":\n",
        "        mean = torch.as_tensor(NORM_SPEC[\"mean\"], device=x.device, dtype=x.dtype).view(1, -1, 1, 1)\n",
        "        std  = torch.as_tensor(NORM_SPEC[\"std\"],  device=x.device, dtype=x.dtype).view(1, -1, 1, 1)\n",
        "        x = x / 255.0 if scale_255 else x\n",
        "        return (x - mean) / std\n",
        "    if mode == \"minus1to1\":\n",
        "        x = x / 255.0 if scale_255 else x\n",
        "        return (x - 0.5) * 2.0\n",
        "    return x  # \"none\"\n",
        "\n",
        "# ---------- Decide input channels (probe if not provided) ----------\n",
        "def _resolve_in_ch(m: nn.Module, img_sz: int, device, dtype, cfg_ch):\n",
        "    if isinstance(cfg_ch, (int, float)) and int(cfg_ch) in (1, 3):\n",
        "        return int(cfg_ch)\n",
        "    with torch.no_grad():\n",
        "        for c in (1, 3):\n",
        "            try:\n",
        "                _ = m(torch.zeros(1, c, img_sz, img_sz, device=device, dtype=dtype))\n",
        "                return c\n",
        "            except Exception:\n",
        "                pass\n",
        "    for mod in m.modules():                 # fallback: infer from first Conv2d\n",
        "        if isinstance(mod, nn.Conv2d):\n",
        "            return int(mod.weight.shape[1])\n",
        "    return 1\n",
        "\n",
        "IN_CH = _resolve_in_ch(model, IMG_SIZE, _dev, _dtype, IN_CHANNELS)\n",
        "\n",
        "# ---------- FLOPs (reference-compatible print) ----------\n",
        "_dummy = torch.zeros(1, IN_CH, IMG_SIZE, IMG_SIZE, device=_dev, dtype=_dtype)\n",
        "_total_flops = float(FlopCountAnalysis(model, _dummy).total())\n",
        "_gflops = _total_flops / 1e9\n",
        "\n",
        "print(f\"FLOPs: {_gflops:.5f} GFLOPs\")          # exact reference format\n",
        "print(f\"Total FLOPs (ops): {_total_flops:,.0f}\")\n",
        "\n",
        "# ---------- Accuracy for efficiency ----------\n",
        "def _best_recorded_acc():\n",
        "    for k in (\"test_acc_tta\",\"test_acc_ema\",\"test_acc_base\",\"val_acc_ema\",\"val_acc_base\"):\n",
        "        v = globals().get(k, None)\n",
        "        if isinstance(v, (int, float)):\n",
        "            return float(v)\n",
        "    hist = globals().get(\"history\", None)\n",
        "    if isinstance(hist, list) and hist and isinstance(hist[-1].get(\"val_acc\", None), (int, float)):\n",
        "        return float(hist[-1][\"val_acc\"])\n",
        "    return None\n",
        "\n",
        "@torch.no_grad()\n",
        "def _eval_top1(m, loader):\n",
        "    if loader is None:\n",
        "        return None\n",
        "    m.eval()\n",
        "    tot = cor = 0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(_dev, non_blocking=True)\n",
        "        yb = yb.to(_dev, non_blocking=True)\n",
        "        xb = _apply_eval_norm(xb)\n",
        "        logits = m(xb)\n",
        "        cor += (logits.argmax(1) == yb).sum().item()\n",
        "        tot += yb.size(0)\n",
        "    return (cor / tot) if tot > 0 else None\n",
        "\n",
        "acc_frac = _best_recorded_acc()\n",
        "if acc_frac is None:\n",
        "    # Try loaders in the order given by CONFIG (or the default list)\n",
        "    for name in LOADER_PREF:\n",
        "        loader = globals().get(name, None)\n",
        "        acc_frac = _eval_top1(model, loader)\n",
        "        if isinstance(acc_frac, (int, float)):\n",
        "            break\n",
        "\n",
        "# ---------- Efficiency = accuracy(%) / GFLOPs ----------\n",
        "if isinstance(acc_frac, (int, float)) and _gflops > 0:\n",
        "    efficiency = (acc_frac * 100.0) / _gflops\n",
        "    print(f\"Efficiency (acc% / GFLOPs): {efficiency:.2f}\")\n",
        "else:\n",
        "    print(\"Efficiency not computed: missing accuracy and/or GFLOPs==0.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a604866",
      "metadata": {
        "id": "8a604866"
      },
      "source": [
        "### Final Evaluation Results\n",
        "\n",
        "| Metric                         | Value        |\n",
        "|--------------------------------|--------------|\n",
        "| **FLOPs** (GFLOPs)             | 0.07465      |\n",
        "| **Total FLOPs (ops)**          | 74,650,482   |\n",
        "| **Efficiency** (Acc% / GFLOPs) | 934.64       |\n",
        "| **Validation Accuracy**        | 0.6952 (~69.5%) |\n",
        "| **Test Accuracy**              | 0.7040 (~70.4%) |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0S0aIp-riFim",
      "metadata": {
        "id": "0S0aIp-riFim"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# TESTING: accuracy + image predictions\n",
        "# =========================\n",
        "import torch, numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as VF\n",
        "\n",
        "# --- Class names for readability ---\n",
        "CLASS_NAMES = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']\n",
        "\n",
        "# --- Normalisation used for eval ---\n",
        "def _to_m11(x: torch.Tensor) -> torch.Tensor:\n",
        "    # x in [0,255] (uint8/float) -> float in [-1,1]\n",
        "    return ((x.float() / 255.0) - 0.5) * 2.0\n",
        "\n",
        "# --- 1) Load the trained checkpoint ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "ckpt_path = str(CONFIG.get(\"SAVE_BEST_PATH\", \"\"))  # e.g., \"project/checkpoints/best_fer.pth\"\n",
        "assert ckpt_path, \"CONFIG['SAVE_BEST_PATH'] is empty; set the checkpoint path first.\"\n",
        "\n",
        "# Rebuild model skeleton exactly as trained\n",
        "model = HybridEffNet(num_classes=7, classifier_dropout=0.30, use_cbam=True).to(device)\n",
        "ckpt = torch.load(ckpt_path, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model_state\"], strict=True)\n",
        "model.eval()\n",
        "\n",
        "# --- 2) Compute and print test accuracy (PrivateTest) ---\n",
        "@torch.no_grad()\n",
        "def eval_accuracy(model, loader) -> float:\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb = _to_m11(xb.to(device, non_blocking=True))\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "        logits = model(xb)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += (pred == yb).sum().item()\n",
        "        total   += yb.numel()\n",
        "    return correct / max(1, total)\n",
        "\n",
        "test_acc = eval_accuracy(model, test_dl)\n",
        "print(f\"[Test] accuracy (PrivateTest) = {test_acc:.4f}\")\n",
        "\n",
        "# --- 3a) Predict a few samples from the FER2013 test loader ---\n",
        "@torch.no_grad()\n",
        "def preview_test_predictions(model, loader, n=12):\n",
        "    model.eval()\n",
        "    xb, yb = next(iter(loader))                 # one batch\n",
        "    xb = xb[:n]                                 # first n images\n",
        "    gt = yb[:n].cpu().numpy()\n",
        "    xb_dev = _to_m11(xb.to(device, non_blocking=True))\n",
        "    logits = model(xb_dev)\n",
        "    probs  = logits.softmax(1).cpu().numpy()\n",
        "    pred   = probs.argmax(1)\n",
        "    # print a small table\n",
        "    print(\"\\n[Index]  Pred (pmax)     |  GT\")\n",
        "    for i in range(n):\n",
        "        pclass = int(pred[i]); gclass = int(gt[i])\n",
        "        pmax   = float(probs[i, pclass])\n",
        "        print(f\"{i:>6d}  {CLASS_NAMES[pclass]:<12s} ({pmax:0.3f}) |  {CLASS_NAMES[gclass]}\")\n",
        "    return pred, gt\n",
        "\n",
        "_ = preview_test_predictions(model, test_dl, n=12)\n",
        "\n",
        "# --- 3b) Predict arbitrary external images (file paths) ---\n",
        "@torch.no_grad()\n",
        "def predict_images(model, image_paths):\n",
        "    \"\"\"\n",
        "    image_paths: List[str] to arbitrary images.\n",
        "    Preprocessing: grayscale -> resize to 96x96 -> tensor [1,H,W] -> [-1,1].\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    batch = []\n",
        "    for path in image_paths:\n",
        "        img = Image.open(path).convert(\"L\")            # force grayscale\n",
        "        img = img.resize((CONFIG[\"IMG_SIZE\"], CONFIG[\"IMG_SIZE\"]), Image.BILINEAR)\n",
        "        x = torch.from_numpy(np.array(img, dtype=np.uint8))[None, ...]  # [1,H,W] uint8\n",
        "        batch.append(x)\n",
        "    xb = torch.stack(batch, dim=0)                     # [B,1,H,W]\n",
        "    xb = _to_m11(xb).to(device, non_blocking=True)\n",
        "    logits = model(xb)\n",
        "    probs  = logits.softmax(1).cpu().numpy()\n",
        "    preds  = probs.argmax(1)\n",
        "    # pretty-print\n",
        "    print(\"\\n[External Image Predictions]\")\n",
        "    for path, p in zip(image_paths, preds):\n",
        "        print(f\"{path}  ->  {CLASS_NAMES[int(p)]} (p={probs[list(preds).index(p), int(p)]:.3f})\")\n",
        "    return preds, probs\n",
        "\n",
        "# Example usage for external files (uncomment and set your paths):\n",
        "# preds, probs = predict_images(model, [\n",
        "#     \"/content/some_face1.png\",\n",
        "#     \"/content/some_face2.jpg\",\n",
        "# ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rvuKRYewo88q",
      "metadata": {
        "id": "rvuKRYewo88q"
      },
      "outputs": [],
      "source": [
        "# === Cell B: Visualize predictions on Test images (robust / auto-range) ===\n",
        "import torch, numpy as np, matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Class names (adjust if your label order differs)\n",
        "Labels = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']\n",
        "\n",
        "# If you used my BN-recal cell, EVAL_TF may exist. Otherwise this stays None.\n",
        "EVAL_TF = globals().get('EVAL_TF', None)\n",
        "\n",
        "def _transform_includes_normalize(transform) -> bool:\n",
        "    \"\"\"Detects torchvision.transforms.Normalize inside a Compose-like transform.\"\"\"\n",
        "    try:\n",
        "        from torchvision.transforms import Normalize\n",
        "        seq = getattr(transform, 'transforms', None)\n",
        "        return any(isinstance(t, Normalize) for t in (seq or []))\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "@torch.no_grad()\n",
        "def _model_ready_batch(xb: torch.Tensor, dev: torch.device) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Returns a tensor ready for the model:\n",
        "    - If val/test transform already contains Normalize(0.5,0.5), pass through.\n",
        "    - Otherwise apply explicit ((x/255)-0.5)*2 normalization.\n",
        "    \"\"\"\n",
        "    xb = xb.to(dev, non_blocking=True)\n",
        "    need_explicit_norm = True\n",
        "    if EVAL_TF is not None and _transform_includes_normalize(EVAL_TF):\n",
        "        need_explicit_norm = False\n",
        "    if need_explicit_norm:\n",
        "        xb = ((xb / 255.0) - 0.5) * 2.0\n",
        "    return xb\n",
        "\n",
        "def _to_display(img: torch.Tensor) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert a single image tensor to HxW in [0,1] for imshow.\n",
        "    Works for uint8 [0,255], float [0,1], or float [-1,1].\n",
        "    \"\"\"\n",
        "    x = img.detach().cpu()\n",
        "    if x.ndim == 3 and x.size(0) == 1:  # [1,H,W] -> [H,W]\n",
        "        x = x[0]\n",
        "    x = x.float()\n",
        "    m, M = float(x.min()), float(x.max())\n",
        "    if M > 1.5:          # likely uint8 [0,255]\n",
        "        x = x / 255.0\n",
        "    elif m < -0.25:      # likely [-1,1]\n",
        "        x = (x * 0.5) + 0.5\n",
        "    x = x.clamp(0, 1)\n",
        "    return x.numpy()\n",
        "\n",
        "@torch.no_grad()\n",
        "def fetch_batch_and_predict(model, loader: DataLoader):\n",
        "    \"\"\"Fetch first batch from loader, run model, return (xb_raw, yb, pred, conf).\"\"\"\n",
        "    model.eval()\n",
        "    dev = next(model.parameters()).device\n",
        "    xb, yb = next(iter(loader))\n",
        "    xb_for_model = _model_ready_batch(xb.clone(), dev)\n",
        "    logits = model(xb_for_model)\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    pred  = probs.argmax(1)\n",
        "    conf  = probs.max(1).values\n",
        "    return xb, yb, pred.cpu(), conf.cpu()\n",
        "\n",
        "# If you want EMA weights evaluated, uncomment:\n",
        "# try: ema_tail.apply_shadow(model)\n",
        "# except NameError: pass\n",
        "\n",
        "xb, yb, pred, conf = fetch_batch_and_predict(model, test_dl)\n",
        "\n",
        "# --- Grid render ---\n",
        "K = min(25, xb.size(0))   # number of images to show\n",
        "cols = 5\n",
        "rows = int(np.ceil(K / cols))\n",
        "plt.figure(figsize=(cols * 3, rows * 3))\n",
        "\n",
        "for i in range(K):\n",
        "    ax = plt.subplot(rows, cols, i + 1)\n",
        "    ax.imshow(_to_display(xb[i]), cmap='gray', interpolation='nearest')\n",
        "    t = Labels[int(yb[i])]\n",
        "    p = Labels[int(pred[i])]\n",
        "    c = float(conf[i])\n",
        "    ax.set_title(f\"P:{p} ({c:.2f})\\nT:{t}\", fontsize=9)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# If you applied EMA above and want to revert to base weights, uncomment:\n",
        "# try: ema_tail.restore(model)\n",
        "# except NameError: pass\n",
        "\n",
        "\"\"\"# === Cell 22: Save final checkpoint & reload sanity ===\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# === Cell 22: Save final checkpoint & reload sanity ===\n",
        "FINAL_PATH = CKPT_DIR / \"final_fer_model.pth\"\n",
        "torch.save({\"model_state\": model.state_dict()}, FINAL_PATH)\n",
        "print(f\"[Save] {FINAL_PATH}\")\n",
        "\n",
        "ckpt = torch.load(FINAL_PATH, map_location=\"cpu\")\n",
        "# ======= BUG FIX START: Fixed model.device attribute error =======\n",
        "# Original: model.to(model.device).eval() - models don't have .device attribute by default\n",
        "# Fixed to use the device variable defined at line 57\n",
        "model.load_state_dict(ckpt[\"model_state\"]); model.to(device).eval()\n",
        "# ======= BUG FIX END =======\n",
        "with torch.no_grad():\n",
        "    xb, yb = next(iter(val_dl))\n",
        "    xb = ((xb/255.) - 0.5) * 2.0\n",
        "    # ======= BUG FIX START: Fixed another model.device reference =======\n",
        "    out = model(xb.to(device))\n",
        "    # ======= BUG FIX END =======\n",
        "print(\"[Reload] Sanity forward OK:\", tuple(out.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "359b37fe",
      "metadata": {
        "id": "359b37fe"
      },
      "source": [
        "# === Cell 22: Save final checkpoint & reload sanity ===\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30b73fe1",
      "metadata": {
        "id": "30b73fe1"
      },
      "outputs": [],
      "source": [
        "# === Cell 22: Save final checkpoint & reload sanity ===\n",
        "FINAL_PATH = CKPT_DIR / \"final_fer_model.pth\"\n",
        "torch.save({\"model_state\": model.state_dict()}, FINAL_PATH)\n",
        "print(f\"[Save] {FINAL_PATH}\")\n",
        "\n",
        "ckpt = torch.load(FINAL_PATH, map_location=\"cpu\")\n",
        "model.load_state_dict(ckpt[\"model_state\"]); model.to(model.device).eval()\n",
        "with torch.no_grad():\n",
        "    xb, yb = next(iter(val_dl))\n",
        "    xb = ((xb/255.) - 0.5) * 2.0\n",
        "    out = model(xb.to(model.device))\n",
        "print(\"[Reload] Sanity forward OK:\", tuple(out.shape))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "first_test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "870146efa1da47b695e2c73011f0160c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_453a903497d347599c749c8e4b16715f",
              "IPY_MODEL_8642576775984af18266bf7076d2da2a",
              "IPY_MODEL_9cadcc229b4c47dd9581be483bfdcc37"
            ],
            "layout": "IPY_MODEL_c3faa1a2802245b6ae5939ebe878f99a"
          }
        },
        "453a903497d347599c749c8e4b16715f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4c39e2e28ef47e2aa8a06548395d50a",
            "placeholder": "​",
            "style": "IPY_MODEL_49a75aa201394996851c676b93fa7a5b",
            "value": "model.safetensors: 100%"
          }
        },
        "8642576775984af18266bf7076d2da2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1dc2b7a2a4c6415591de96e0226e67f9",
            "max": 24957808,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ffec57b32be441a9dd3984d4b55fd78",
            "value": 24957808
          }
        },
        "9cadcc229b4c47dd9581be483bfdcc37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c40cc2da4744e868ff9ac999dd78bf8",
            "placeholder": "​",
            "style": "IPY_MODEL_3fd8fd786e8543148ca04cf046bf237a",
            "value": " 25.0M/25.0M [00:00&lt;00:00, 108MB/s]"
          }
        },
        "c3faa1a2802245b6ae5939ebe878f99a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4c39e2e28ef47e2aa8a06548395d50a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49a75aa201394996851c676b93fa7a5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1dc2b7a2a4c6415591de96e0226e67f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ffec57b32be441a9dd3984d4b55fd78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c40cc2da4744e868ff9ac999dd78bf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fd8fd786e8543148ca04cf046bf237a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}