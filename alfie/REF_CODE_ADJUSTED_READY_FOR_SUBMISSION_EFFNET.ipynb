{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "187e27d3",
      "metadata": {
        "id": "187e27d3"
      },
      "source": [
        "# Cell 01 — Environment, Paths, and Reproducibility\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Imports system, math, random number, and ML libraries; suppresses non-critical warnings.  \n",
        "- Detects whether the notebook runs in Google Colab; if so, mounts Drive for dataset access.  \n",
        "- Creates project directory structure (`project/`, `data/`, `checkpoints/`, `logs/`).  \n",
        "- Sets a global random seed across Python, NumPy, and PyTorch, and enables deterministic CuDNN (turns off autotuner).  \n",
        "- Detects and prints the compute device.  \n",
        "- Defines `FER_CSV_PATH` pointing to `fer2013.csv` from Drive (Colab) or local path.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Determinism first*: Fixing seeds and disabling `cudnn.benchmark` ensures reproducibility.  \n",
        "- *Early I/O fail-fast*: Mounting/reporting paths upfront prevents runtime surprises.  \n",
        "- *Explicit project scaffold*: Standard locations simplify checkpointing, logging, and automation.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- `torch.set_float32_matmul_precision('high')` (faster on Ampere+, minor numeric drift).  \n",
        "- Keep `cudnn.benchmark=True` (faster on fixed shapes, but non-deterministic).  \n",
        "- Centralize configs via Hydra/pydantic (cleaner experiments, added dependency).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "This is the **foundation layer**: reproducible environment + stable file system paths for downstream components.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell 02 — Global Config (Single Source of Truth)\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Declares `CONFIG` and `HP` dictionaries with data, compute toggles, augmentation, optimizer/scheduler, and hyperparameters.  \n",
        "- Derives schedule numbers (e.g., taper epochs, fine-tune tail length, augmentation ramp).  \n",
        "- Defines factories for optimizer and LR schedulers.  \n",
        "- Prints summary of config and derived values.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Single source of truth*: Prevents config drift and supports auditability.  \n",
        "- *Derived values once*: Avoids off-by-one errors inside training loop.  \n",
        "- *Scheduler abstraction*: Clean comparison between policies.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- Hydra/OmegaConf (more powerful, more boilerplate).  \n",
        "- K-fold CV config toggle (robust, higher compute).  \n",
        "- OneCycleLR vs CosineAnnealingLR (faster escape vs stability).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Central config guarantees **consistent behavior** across pipeline, model, and loop.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell 03 — Load FER2013 and Split\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Validates CSV path, reads into DataFrame, asserts columns.  \n",
        "- Splits into `train_df`, `valid_df`, `test_df` based on official `Usage` column.  \n",
        "- Prints split counts.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Schema validation*: Catches column mismatches early.  \n",
        "- *Official splits*: Ensures benchmark comparability.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- Stratified re-split (flexible, diverges from standard).  \n",
        "- Extra “dev” split (early stopping fairness, fewer train samples).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Defines the **ground truth partitions** used for training and evaluation.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell 04 — Dataset (48→96), Tensors in [0..255], Shape 1×H×W\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Implements `FER2013Dataset` to parse pixel strings → `float32` tensors.  \n",
        "- Validates each image is `48×48`.  \n",
        "- Resizes to `96×96` with antialiasing.  \n",
        "- Returns tensors in `[0..255]` with shape `[1,H,W]` and integer labels.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Strict contract*: Normalization/augmentation deferred downstream.  \n",
        "- *Grayscale preserved*: Sobel stem handles 1→3 conversion explicitly.  \n",
        "- *Deterministic dataset*: Easier debugging and reproducibility.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- Store `uint8` (saves memory, requires casting).  \n",
        "- Resize later in augmentation (flexible, redundant work).  \n",
        "- Convert to 3-ch grayscale (convenient, risks misuse).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Provides a **clean I/O boundary** for later augmentation/normalization.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell 05 — DataLoaders\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Builds DataLoaders for train/val/test.  \n",
        "- Uses `pin_memory` on CUDA, persistent workers, shuffling for train.  \n",
        "- Prints batch shapes and ranges.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Throughput*: Efficient GPU feeding.  \n",
        "- *Sanity checks*: Catches shape/range errors early.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- Tune `prefetch_factor` (faster, higher memory).  \n",
        "- Deterministic sampling (replicable, less regularization).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Efficient batch streaming into the model, ensuring invariants.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell 06 — Advanced Augmentation Primitives\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Defines grayscale-safe photometric transforms (noise, gamma, contrast, equalization, JPEG, vignette).  \n",
        "- Implements geometric (affine, crop, flip) and elastic distortions.  \n",
        "- Adds safe blur.  \n",
        "- Provides band occlusion targeting face regions.  \n",
        "- (Later) includes localized erasing.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Task-aware diversity*: Covers lighting, pose, occlusion.  \n",
        "- *Range safety*: Prevents numeric drift.  \n",
        "- *Elastic distortions*: Simulate natural deformations.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- Albumentations/imgaug (broad, less control).  \n",
        "- RandAugment/AugMix (standardized, weaker facial priors).  \n",
        "- Learned policies (optimal, compute-heavy).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Defines the **augmentation vocabulary** for Cell 07.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell 06.x — band_occlusion Utility\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Occludes regions (eyes/mouth/etc.) with controlled height fractions.  \n",
        "- Uses neutral fill for brightness consistency.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Domain prior*: Models occlusions common in real faces.  \n",
        "- *Controlled magnitude*: Preserves overall face integrity.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- Cutout (simpler, less face-specific).  \n",
        "- GridMask (uniform, may be too aggressive).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "A **specialized occlusion tool** for robustness.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell 07 — AugMix-Lite and Augmentation Builder\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Builds mixed augmentation policy from primitive banks.  \n",
        "- Scales probability/magnitude with `strength∈[0,1]`.  \n",
        "- Normalizes outputs to `[-1,1]`.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Mixture policies*: Broad distortion coverage.  \n",
        "- *Strength scheduling*: Avoids overwhelming early learning.  \n",
        "- *Consistent normalization*: Stable model input domain.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- AutoAugment/RandAugment (standardized, weaker priors).  \n",
        "- Curriculum augmentation (adaptive, complex).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Creates a **callable transform** for the training loop.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell 08 — Metrics, Class Weights, and Composite Loss\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Implements accuracy metric.  \n",
        "- Computes class weights.  \n",
        "- Defines LS-CE, Focal Loss, and hybrid SmoothedFocal.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Imbalance robustness*: Class weights/focal reduce skew.  \n",
        "- *Label smoothing*: Improves calibration.  \n",
        "- *Hybrid*: Balances calibration and hard-example learning.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- ASL/Balanced Softmax (tailored, more params).  \n",
        "- Post-hoc calibration (simpler, doesn’t affect training).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Provides the **loss toolkit** for training.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell 09 — MixUp / CutMix and Mixed Criterion\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Implements MixUp and CutMix with label mixing.  \n",
        "- Defines `mixed_criterion` for convex loss combination.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Manifold smoothing*: Improves generalization.  \n",
        "- *Aug regularization*: Complements geometric/photometric aug.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- FMix (smoother masks, more complexity).  \n",
        "- No mixing (simpler, less robust).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Plugs into the **per-batch training loop policy**.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell 10 — EMA\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Tracks EMA “shadow” parameters.  \n",
        "- Provides update/apply/restore.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Smoother hypothesis*: Improves stability/generalization.  \n",
        "- *Cheap*: No extra forward compute.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- SWA (stronger, needs BN recalibration).  \n",
        "- Snapshot ensembles (accuracy boost, more storage).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "A **lightweight ensemble** for evaluation.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell 11 — CBAM and Sobel Stem\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Implements CBAM attention.  \n",
        "- Sobel stem: converts 1-ch input into `[x, Gx, Gy]`.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Attention*: Focuses backbone on salient regions.  \n",
        "- *Sobel*: Edge priors aid expression cues.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- SE block (lighter, less spatial attention).  \n",
        "- Learned first conv (standard, no explicit edges).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Enhances the **feature extractor** with priors.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell 12 — Model: EfficientNet-B0 + CBAM + Sobel\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Loads EfficientNet-B0 backbone.  \n",
        "- Sobel stem → EfficientNet features.  \n",
        "- Optional CBAM, then pool → BN → Dropout → Linear(7).  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Strong baseline*: EfficientNet-B0 balances accuracy/FLOPs.  \n",
        "- *Regularized head*: BN+Dropout reduce overfit.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- ResNet-18/34 (simpler, lower accuracy).  \n",
        "- Vision Transformers (global context, data-hungry).  \n",
        "- GroupNorm instead of BN (batch-size independence).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Defines the **core hypothesis class**.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell 13 — Optimizer, Scheduler, Early Stopping\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Abstracted `make_optimizer` (AdamW/SGD).  \n",
        "- Warmup+cosine helper.  \n",
        "- EarlyStopping utility.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Modularity*: Swap optimizers easily.  \n",
        "- *Guardrails*: Prevents overfitting/wasted epochs.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- AdamW vs SGD: speed vs generalization.  \n",
        "- OneCycle (batch-level) vs Cosine/Plateau (epoch-level).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Provides **training stability and flexibility**.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell 14 — Training Loop\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Builds optimizer/scheduler, early stopper, EMA, AMP scaler.  \n",
        "- Schedules aug strength, LS, MixUp/CutMix, fine-tune tail.  \n",
        "- For each batch: normalize → aug → mix → loss → AMP backward → grad clip → step → EMA update.  \n",
        "- Scheduler step (batch or epoch).  \n",
        "- Tracks metrics, saves best checkpoint, early stopping.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Curriculum*: Heavy aug early, taper later, clean tail.  \n",
        "- *Stability stack*: Combines LS, weights, EMA, AMP, clipping.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- Logit adjustment/Balanced Softmax.  \n",
        "- Cosine+warmup vs OneCycle.  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "This is the **learning core**.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell 15 — Build Model and Quick Probe\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Instantiates model, switches to train.  \n",
        "- Runs forward/backward on one batch.  \n",
        "- Prints logits shape and sample loss.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Sanity first*: Catches shape/device/AMP issues early.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- `torchinfo.summary` (rich, extra dep).  \n",
        "- Synthetic unit tests (CI-friendly).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "A **pre-flight check** before training.  \n",
        "\n",
        "---\n",
        "\n",
        "# Aug Utils — localized_erasing\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Adds random small-patch erasures with mid-gray fills.  \n",
        "- Integrates into occlusion bank.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Hard negatives*: Mimics occlusion/noise.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- Cutout/GridMask (simpler, more aggressive).  \n",
        "- Blur/Noise (softer, less challenging).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Complements the **occlusion augmentations**.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell 16 — Train (Main Run)\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Calls `fit_with_aug` with loaders + hyperparams.  \n",
        "- Trains with OneCycle schedule.  \n",
        "- Saves best checkpoint by `val_loss`.  \n",
        "- Early stopping if patience exceeded.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Efficient convergence*: OneCycle + EMA.  \n",
        "- *Resource aware*: Early stopping.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- Cosine+warmup (simpler, less exploratory).  \n",
        "- Longer training with stronger regularization.  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Executes the **full training procedure**.  \n",
        "\n",
        "---\n",
        "\n",
        "# Pre-18 — BN Recalibration\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Detects BN layers.  \n",
        "- Builds clean loader (no aug).  \n",
        "- Updates BN running stats.  \n",
        "- Reports val/test accuracy post-recal.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *BN mismatch fix*: Aligns eval stats.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- GroupNorm/LayerNorm (batch-size independent).  \n",
        "- Keep augmented BN (robust eval, less comparable).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Prepares the model for **clean evaluation**.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell 18 — Evaluation (Val/Test + TTA)\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Defines eval loaders (clean + TTA).  \n",
        "- Computes accuracies for base, EMA, and optional TTA.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Multiple views*: Base, EMA, TTA cover common eval strategies.  \n",
        "- *Probability averaging*: Safer than logits averaging.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- Richer TTA (better, slower).  \n",
        "- Calibrated thresholds (improves decisions, extra step).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Produces **final metrics**.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell 19 — Confusion Matrix and Per-Class Accuracy\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Accumulates confusion matrix.  \n",
        "- Prints per-class accuracies.  \n",
        "- Uses EMA weights if present.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Error forensics*: Reveals systematic confusions.  \n",
        "- *Class-wise analysis*: Guides future aug/loss tweaks.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- Precision/Recall/F1 per class.  \n",
        "- Calibration metrics.  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Supports **diagnostic analysis**.  \n",
        "\n",
        "---\n",
        "\n",
        "# Plot Accuracy and Loss Curves\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Plots training/validation accuracy and loss across epochs.  \n",
        "- Optionally overlays test accuracy.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Learning dynamics*: Detects over/under-fitting.  \n",
        "- *Regression detection*: Spots leakage or augmentation bugs.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- TensorBoard/W&B (interactive, setup required).  \n",
        "- Rolling averages (smoother, hides spikes).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Provides **visual diagnostics**.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell 20 — Install fvcore\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Installs `fvcore` for FLOP counting.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Efficiency accounting*: Accuracy/GFLOP matters for deployment.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- `thop`, `ptflops` (lighter, coverage varies).  \n",
        "- Torch Dynamo/AOT tracing (modern, complex).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Prepares **tooling** for efficiency profiling.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell 21 — FLOPs + Accuracy/GFLOP\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Uses `FlopCountAnalysis` to measure FLOPs.  \n",
        "- Reports GFLOPs and accuracy/GFLOP ratio.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Pareto analysis*: Balances accuracy vs cost.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- MACs instead of FLOPs.  \n",
        "- Latency benchmarks (real-world, env-specific).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Provides **computational efficiency metrics**.  \n",
        "\n",
        "---\n",
        "\n",
        "# Testing — Accuracy + Image Predictions\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Rebuilds model, loads checkpoint, computes test accuracy.  \n",
        "- Prints sample predictions with confidence.  \n",
        "- Defines helper for arbitrary external images.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Artifact validation*: Confirms saved weights work.  \n",
        "- *Qualitative check*: Inspects plausibility of errors.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- Save exact transforms (strict reproducibility).  \n",
        "- Export to TorchScript/ONNX (deployment portability).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Bridges **trained artifacts** to inference.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell B — Visualization of Predictions\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Fetches batch, normalizes appropriately.  \n",
        "- Renders grid with predicted label, confidence, ground truth.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Human-in-the-loop*: Visual debugging of errors.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- Saliency/Grad-CAM (interpretability, more compute).  \n",
        "- Misclassification collectors (focused error analysis).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Complements metrics with **qualitative insight**.  \n",
        "\n",
        "---\n",
        "\n",
        "# Cell 22 — Save Final Checkpoint & Reload Sanity\n",
        "\n",
        "**What this cell does (step-by-step)**  \n",
        "- Saves final checkpoint distinct from “best val”.  \n",
        "- Reloads, performs a forward pass to confirm.  \n",
        "\n",
        "**Why this approach**  \n",
        "- *Traceability*: Retain both final and best artifacts.  \n",
        "- *Integrity check*: Ensures model reloads cleanly.  \n",
        "\n",
        "**Alternatives and trade-offs**  \n",
        "- Save EMA/SWA versions too.  \n",
        "- Save full training state (resume-friendly, larger).  \n",
        "\n",
        "**How it fits in the overall flow**  \n",
        "Delivers **durable artifacts** for sharing or deployment.  \n",
        "\n",
        "---\n",
        "\n",
        "# Notes on the Skipped Cell\n",
        "\n",
        "**Cell 17** — Optional late-phase fine-tune skipped as instructed.  \n",
        "- Purpose: short clean fine-tune with warmup or SWA to extract final gains after plateau.  \n",
        "\n",
        "---\n",
        "\n",
        "# Suggested “How to Read the Notebook” Map\n",
        "\n",
        "- **Cells 01–03:** Environment and data foundations.  \n",
        "- **Cells 04–07:** Data interface and augmentation.  \n",
        "- **Cells 08–13:** Losses, optimizer, and training utilities.  \n",
        "- **Cell 14:** Training engine.  \n",
        "- **Cells 15–16:** Sanity probe and main run.  \n",
        "- **Pre-18 & 18–19:** Clean eval, confusion matrix, class analysis.  \n",
        "- **Cells 20–21:** Efficiency profiling.  \n",
        "- **Testing/B/22:** Artifacts, visualization, and final save.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cf6f21f",
      "metadata": {
        "id": "1cf6f21f"
      },
      "source": [
        "#Cell 01 — Environment, paths, reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fvcore"
      ],
      "metadata": {
        "id": "GOEyLl7WDmoG"
      },
      "id": "GOEyLl7WDmoG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "599964ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "599964ba",
        "outputId": "0eef05d3-fb1d-4eb7-c47f-5b464fcc739e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Env][WARN] Drive mount failed: mount failed\n",
            "[Env] torch=2.8.0+cu126, torchvision=0.23.0+cu126, device=cuda\n",
            "[Env] CSV path → /content/drive/MyDrive/Colab Notebooks/deep_learning_challenge/fer2013.csv\n"
          ]
        }
      ],
      "source": [
        "# === Cell 01: Environment, paths, reproducibility ===\n",
        "import os, sys, math, random, shutil, warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def in_colab() -> bool:\n",
        "    return \"google.colab\" in sys.modules\n",
        "\n",
        "if in_colab():\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount(\"/content/drive\", force_remount=False)\n",
        "        print(\"[Env] Google Drive mounted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"[Env][WARN] Drive mount failed: {e}\")\n",
        "\n",
        "# Root dirs (local)\n",
        "PROJECT_ROOT  = Path(\"./project\"); PROJECT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "DATA_ROOT     = Path(\"./data\");    DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "CKPT_DIR      = PROJECT_ROOT / \"checkpoints\"; CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "LOG_DIR       = PROJECT_ROOT / \"logs\";        LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Seeds\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# Optional strict determinism (slower but reproducible)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"[Env] torch={torch.__version__}, torchvision={torchvision.__version__}, device={device}\")\n",
        "\n",
        "# Data path (edit if needed)\n",
        "FER_CSV_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/deep_learning_challenge/fer2013.csv\") if in_colab() else Path(\"./fer2013.csv\")\n",
        "print(f\"[Env] CSV path → {FER_CSV_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36994e40",
      "metadata": {
        "id": "36994e40"
      },
      "source": [
        "#Cell 02 — Global config (single source of truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01z4hUq9nYSC",
      "metadata": {
        "id": "01z4hUq9nYSC"
      },
      "source": [
        "| **Category**              | **Setting**                                                                                           |\n",
        "| ------------------------- | ----------------------------------------------------------------------------------------------------- |\n",
        "| **Dataset**               | FER2013 (`fer2013.csv`)                                                                               |\n",
        "| **Image Size**            | 96 × 96                                                                                               |\n",
        "| **Batch Size**            | 192                                                                                                   |\n",
        "| **Workers**               | 6 (for DataLoader parallelism)                                                                        |\n",
        "| **Augmentations**         | Basic (flip, crop, color, etc.)<br>Advanced FER policy (Cell 07)<br>MixUp (α=0.30)<br>CutMix (α=1.00) |\n",
        "| **Late-Phase Controls**   | Aug cap in final 30%<br>Mix tapering (25% → 90% of epochs)                                            |\n",
        "| **Label Smoothing**       | Scheduled: 0.10 → 0.02                                                                                |\n",
        "| **Fine-Tune Tail**        | Last 12% of epochs: no aug/mix, low LR                                                                |\n",
        "| **Optimizer**             | AdamW (default)<br>Option to switch to SGD+Nesterov (momentum=0.9)                                    |\n",
        "| **Learning Rate**         | Initial: 3e-4<br>Floor: 5e-5 (cosine/fine-tune)<br>Plateau min: 1e-6                                  |\n",
        "| **Scheduler**             | ReduceLROnPlateau (factor=0.5, patience=5)<br>Option: CosineAnnealing with warmup=4                   |\n",
        "| **Weight Decay**          | 5e-5                                                                                                  |\n",
        "| **Training Epochs**       | 70                                                                                                    |\n",
        "| **EMA (Exponential MA)**  | Enabled (decay=0.9995)                                                                                |\n",
        "| **Early Stopping**        | Patience = 18                                                                                         |\n",
        "| **Test-Time Aug. (TTA)**  | Enabled (only for test set, not validation)                                                           |\n",
        "| **AMP (Mixed Precision)** | Enabled if CUDA available                                                                             |\n",
        "| **Checkpointing**         | Saves best model to: `project/checkpoints/best_fer.pth`                                               |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c1223cbb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1223cbb",
        "outputId": "2648d4d1-fbfc-4f16-9274-a90be791c59d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[CONFIG]\n",
            "  AUG_CAP_LATE        : True\n",
            "  BASE_CUTMIX_PROB    : 0.25\n",
            "  BASE_MIXUP_PROB     : 0.45\n",
            "  BATCH_SIZE          : 192\n",
            "  FER_CSV_PATH        : /content/drive/MyDrive/Colab Notebooks/deep_learning_challenge/fer2013.csv\n",
            "  FINE_TUNE_EPOCHS    : 8\n",
            "  FINE_TUNE_FRACTION  : 0.12\n",
            "  IMG_SIZE            : 96\n",
            "  LABEL_SMOOTH_END    : 0.02\n",
            "  LABEL_SMOOTH_START  : 0.08\n",
            "  NUM_WORKERS         : 10\n",
            "  SAVE_BEST_PATH      : project/checkpoints/best_fer.pth\n",
            "  SCHEDULER           : onecycle\n",
            "  TAPER_END_EPOCH     : 60\n",
            "  TAPER_END_FRAC      : 0.85\n",
            "  TAPER_MIX_LATE      : True\n",
            "  TAPER_START_EPOCH   : 14\n",
            "  TAPER_START_FRAC    : 0.2\n",
            "  USE_AMP             : True\n",
            "  USE_AUG             : True\n",
            "  USE_AUG_ADV         : True\n",
            "  USE_CUTMIX          : True\n",
            "  USE_EMA             : True\n",
            "  USE_MIXUP           : True\n",
            "  USE_SGD             : False\n",
            "  USE_TTA             : True\n",
            "\n",
            "[HP]\n",
            "  AUG_RAMP_EPOCHS     : 28\n",
            "  CUTMIX_ALPHA        : 1.0\n",
            "  EMA_DECAY           : 0.9995\n",
            "  EPOCHS              : 70\n",
            "  LR                  : 0.0003\n",
            "  LR_MAX              : 0.001\n",
            "  LR_MIN              : 5e-05\n",
            "  MIN_LR              : 1e-06\n",
            "  MIXUP_ALPHA         : 0.3\n",
            "  OCL_DIV_FACTOR      : 12.0\n",
            "  OCL_FINAL_DIV       : 20.0\n",
            "  OCL_PCT_START       : 0.15\n",
            "  PATIENCE            : 18\n",
            "  PLATEAU_FACTOR      : 0.5\n",
            "  PLATEAU_PATIENCE    : 5\n",
            "  SGD_MOMENTUM        : 0.9\n",
            "  SGD_NESTEROV        : True\n",
            "  WARMUP_EPOCHS       : 4\n",
            "  WD                  : 5e-05\n",
            "\n",
            "[DERIVED]\n",
            "  TAPER_START_EPOCH : 14\n",
            "  TAPER_END_EPOCH   : 60\n",
            "  FINE_TUNE_EPOCHS  : 8\n",
            "  AUG_RAMP_EPOCHS   : 28\n"
          ]
        }
      ],
      "source": [
        "# === Cell 02: Global config (single source of truth, tuned) ===\n",
        "import os\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from torch.optim import AdamW, SGD\n",
        "from torch.optim.lr_scheduler import (\n",
        "    ReduceLROnPlateau, CosineAnnealingLR, LinearLR, SequentialLR, OneCycleLR\n",
        ")\n",
        "\n",
        "# Respect upstream paths if defined; otherwise use local defaults\n",
        "FER_CSV_PATH = FER_CSV_PATH if 'FER_CSV_PATH' in globals() else Path(\"./fer2013.csv\")\n",
        "CKPT_DIR     = CKPT_DIR     if 'CKPT_DIR'     in globals() else Path(\"./project/checkpoints\")\n",
        "CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def _amp_available() -> bool:\n",
        "    mps_ok = getattr(torch.backends, \"mps\", None)\n",
        "    return bool(torch.cuda.is_available() or (mps_ok and torch.backends.mps.is_available()))\n",
        "\n",
        "CONFIG = {\n",
        "    # IO\n",
        "    \"FER_CSV_PATH\": FER_CSV_PATH,\n",
        "    \"SAVE_BEST_PATH\": CKPT_DIR / \"best_fer.pth\",\n",
        "\n",
        "    # Data\n",
        "    \"IMG_SIZE\": 96,\n",
        "    \"BATCH_SIZE\": 192,\n",
        "    \"NUM_WORKERS\": max(2, (os.cpu_count() or 4) - 2),\n",
        "\n",
        "    # Compute\n",
        "    \"USE_AMP\": _amp_available(),\n",
        "\n",
        "    # Augmentation & evaluation toggles\n",
        "    \"USE_AUG\": True,\n",
        "    \"USE_AUG_ADV\": True,       # advanced FER policy\n",
        "    \"USE_MIXUP\": True,\n",
        "    \"USE_CUTMIX\": True,\n",
        "    \"USE_EMA\": True,\n",
        "    \"USE_TTA\": True,           # val clean; TTA only for test\n",
        "\n",
        "    # Late-phase controls\n",
        "    \"AUG_CAP_LATE\": True,\n",
        "    \"TAPER_MIX_LATE\": True,\n",
        "\n",
        "    # Optimiser and scheduler toggles\n",
        "    \"USE_SGD\": False,          # False: AdamW ; True: SGD+Nesterov\n",
        "    \"SCHEDULER\": \"onecycle\",   # 'onecycle' | 'cosine' | 'plateau'\n",
        "\n",
        "    # Base probabilities for Cell 14’s taper logic\n",
        "    \"BASE_MIXUP_PROB\": 0.45,\n",
        "    \"BASE_CUTMIX_PROB\": 0.25,\n",
        "\n",
        "    # Mix prob taper schedule (fractions of total epochs)\n",
        "    \"TAPER_START_FRAC\": 0.20,  # start taper a bit earlier\n",
        "    \"TAPER_END_FRAC\":   0.85,  # finish taper a bit sooner\n",
        "\n",
        "    # Label smoothing schedule\n",
        "    \"LABEL_SMOOTH_START\": 0.08,  # a touch lower early to recover signal faster\n",
        "    \"LABEL_SMOOTH_END\":   0.02,\n",
        "\n",
        "    # Fine-tuning tail as a fraction of total epochs (no aug/mix, low LR)\n",
        "    \"FINE_TUNE_FRACTION\": 0.12,\n",
        "}\n",
        "\n",
        "HP = {\n",
        "    # Core training\n",
        "    \"EPOCHS\": 70,\n",
        "    \"LR\": 3e-4,        # base LR for AdamW/SGD factories\n",
        "    \"WD\": 5e-5,\n",
        "\n",
        "    # EMA\n",
        "    \"EMA_DECAY\": 0.9995,\n",
        "\n",
        "    # Aug/mix schedule\n",
        "    # If < 1 it is treated as a FRACTION of total epochs; if >= 1 it is treated as absolute epochs\n",
        "    \"AUG_RAMP_EPOCHS\": 0.40,   # longer ramp delays early settling\n",
        "\n",
        "    \"MIXUP_ALPHA\": 0.30,\n",
        "    \"CUTMIX_ALPHA\": 1.00,\n",
        "\n",
        "    # Early stopping\n",
        "    \"PATIENCE\": 18,\n",
        "\n",
        "    # LR floors / warmup (cosine path)\n",
        "    \"LR_MIN\": 5e-5,            # cosine eta_min\n",
        "    \"WARMUP_EPOCHS\": 4,        # cosine warmup only\n",
        "\n",
        "    # ReduceLROnPlateau (only if SCHEDULER == \"plateau\")\n",
        "    \"PLATEAU_FACTOR\": 0.5,\n",
        "    \"PLATEAU_PATIENCE\": 5,\n",
        "    \"MIN_LR\": 1e-6,\n",
        "\n",
        "    # SGD specifics (if you toggle to SGD)\n",
        "    \"SGD_MOMENTUM\": 0.9,\n",
        "    \"SGD_NESTEROV\": True,\n",
        "\n",
        "    # One‑Cycle / high‑peak settings (iteration‑level scheduler)\n",
        "    \"LR_MAX\": 1.0e-3,          # higher peak to break the ~0.665 plateau\n",
        "    \"OCL_PCT_START\": 0.15,     # longer rise phase; smoother early catch‑up\n",
        "    \"OCL_DIV_FACTOR\": 12.0,    # initial_lr = LR_MAX / 12  (~8.3e-5) — no tiny LRs\n",
        "    \"OCL_FINAL_DIV\": 20.0,     # min_lr ≈ initial_lr / 20\n",
        "}\n",
        "\n",
        "# ---- Derived schedule numbers (single source of truth) ----\n",
        "E = int(HP[\"EPOCHS\"])\n",
        "assert CONFIG[\"SCHEDULER\"] in (\"plateau\", \"cosine\", \"onecycle\"), \\\n",
        "       \"SCHEDULER must be 'plateau', 'cosine', or 'onecycle'\"\n",
        "\n",
        "# Convert AUG_RAMP_EPOCHS to absolute epochs if provided as a fraction\n",
        "if HP[\"AUG_RAMP_EPOCHS\"] < 1.0:\n",
        "    HP[\"AUG_RAMP_EPOCHS\"] = max(1, int(round(E * HP[\"AUG_RAMP_EPOCHS\"])))\n",
        "else:\n",
        "    HP[\"AUG_RAMP_EPOCHS\"] = int(HP[\"AUG_RAMP_EPOCHS\"])\n",
        "\n",
        "# Taper windows and fine-tune tail (as absolute epochs)\n",
        "CONFIG[\"TAPER_START_EPOCH\"] = int(round(CONFIG[\"TAPER_START_FRAC\"] * E))\n",
        "CONFIG[\"TAPER_END_EPOCH\"]   = max(CONFIG[\"TAPER_START_EPOCH\"] + 1,\n",
        "                                  int(round(CONFIG[\"TAPER_END_FRAC\"] * E)))\n",
        "CONFIG[\"FINE_TUNE_EPOCHS\"]  = int(round(CONFIG[\"FINE_TUNE_FRACTION\"] * E))\n",
        "\n",
        "# Sanity caps\n",
        "CONFIG[\"TAPER_START_EPOCH\"] = min(CONFIG[\"TAPER_START_EPOCH\"], E - 1)\n",
        "CONFIG[\"TAPER_END_EPOCH\"]   = min(CONFIG[\"TAPER_END_EPOCH\"],   E)\n",
        "CONFIG[\"FINE_TUNE_EPOCHS\"]  = min(CONFIG[\"FINE_TUNE_EPOCHS\"],  max(0, E - 1))\n",
        "\n",
        "# ---- Optimizer factory ----\n",
        "def make_optimizer(model):\n",
        "    \"\"\"Factory: AdamW (default) or SGD+Nesterov.\"\"\"\n",
        "    if CONFIG[\"USE_SGD\"]:\n",
        "        return SGD(model.parameters(),\n",
        "                   lr=HP[\"LR\"],\n",
        "                   momentum=HP[\"SGD_MOMENTUM\"],\n",
        "                   nesterov=HP[\"SGD_NESTEROV\"],\n",
        "                   weight_decay=HP[\"WD\"])\n",
        "    else:\n",
        "        return AdamW(model.parameters(), lr=HP[\"LR\"], weight_decay=HP[\"WD\"])\n",
        "\n",
        "# ---- Scheduler factories ----\n",
        "def _make_cosine(optimizer):\n",
        "    \"\"\"Linear warmup (no LR change here) then cosine to LR_MIN (epoch-level stepping).\"\"\"\n",
        "    warmup_e = max(0, int(HP[\"WARMUP_EPOCHS\"]))\n",
        "    warmup = LinearLR(optimizer, start_factor=1.0, end_factor=1.0, total_iters=max(1, warmup_e))\n",
        "    cosine = CosineAnnealingLR(optimizer, T_max=max(1, E - warmup_e), eta_min=HP[\"LR_MIN\"])\n",
        "    if warmup_e > 0:\n",
        "        return SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_e])\n",
        "    return cosine\n",
        "\n",
        "def _make_plateau(optimizer):\n",
        "    \"\"\"ReduceLROnPlateau on val_loss (epoch-level stepping with metric).\"\"\"\n",
        "    return ReduceLROnPlateau(optimizer,\n",
        "                             mode=\"min\",\n",
        "                             factor=HP[\"PLATEAU_FACTOR\"],\n",
        "                             patience=HP[\"PLATEAU_PATIENCE\"],\n",
        "                             min_lr=HP[\"MIN_LR\"],\n",
        "                             verbose=True)\n",
        "\n",
        "def _make_onecycle(optimizer, steps_per_epoch: int):\n",
        "    \"\"\"\n",
        "    OneCycleLR: iteration-level scheduler. Requires steps_per_epoch.\n",
        "    Ramps LR to LR_MAX quickly, then anneals (cosine).\n",
        "    \"\"\"\n",
        "    return OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=float(HP[\"LR_MAX\"]),\n",
        "        epochs=E,\n",
        "        steps_per_epoch=int(steps_per_epoch),\n",
        "        pct_start=float(HP[\"OCL_PCT_START\"]),\n",
        "        anneal_strategy=\"cos\",\n",
        "        cycle_momentum=False,              # AdamW: momentum cycling unnecessary\n",
        "        div_factor=float(HP[\"OCL_DIV_FACTOR\"]),\n",
        "        final_div_factor=float(HP[\"OCL_FINAL_DIV\"])\n",
        "    )\n",
        "\n",
        "def build_scheduler(optimizer, steps_per_epoch: int | None = None):\n",
        "    \"\"\"\n",
        "    Unified scheduler builder:\n",
        "      - onecycle : requires steps_per_epoch (iteration-level stepping)\n",
        "      - cosine   : epoch-level stepping\n",
        "      - plateau  : epoch-level stepping, needs val_loss each epoch\n",
        "    \"\"\"\n",
        "    sched = CONFIG[\"SCHEDULER\"]\n",
        "    if sched == \"onecycle\":\n",
        "        if steps_per_epoch is None:\n",
        "            raise ValueError(\"OneCycleLR requires steps_per_epoch; pass len(train_dl).\")\n",
        "        return _make_onecycle(optimizer, steps_per_epoch)\n",
        "    elif sched == \"cosine\":\n",
        "        return _make_cosine(optimizer)\n",
        "    elif sched == \"plateau\":\n",
        "        return _make_plateau(optimizer)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown scheduler: {sched}\")\n",
        "\n",
        "# ---- Utilities ----\n",
        "def current_lr(optimizer) -> float:\n",
        "    \"\"\"Report the first param group's LR for logging.\"\"\"\n",
        "    return float(optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "def scheduler_steps_per_batch() -> bool:\n",
        "    \"\"\"True if scheduler.step() should be called every iteration (batch).\"\"\"\n",
        "    return CONFIG[\"SCHEDULER\"] == \"onecycle\"\n",
        "\n",
        "def print_config():\n",
        "    print(\"\\n[CONFIG]\")\n",
        "    for k in sorted(CONFIG.keys()):\n",
        "        print(f\"  {k:20s}: {CONFIG[k]}\")\n",
        "    print(\"\\n[HP]\")\n",
        "    for k in sorted(HP.keys()):\n",
        "        print(f\"  {k:20s}: {HP[k]}\")\n",
        "    print(\"\\n[DERIVED]\")\n",
        "    print(f\"  TAPER_START_EPOCH : {CONFIG['TAPER_START_EPOCH']}\")\n",
        "    print(f\"  TAPER_END_EPOCH   : {CONFIG['TAPER_END_EPOCH']}\")\n",
        "    print(f\"  FINE_TUNE_EPOCHS  : {CONFIG['FINE_TUNE_EPOCHS']}\")\n",
        "    print(f\"  AUG_RAMP_EPOCHS   : {HP['AUG_RAMP_EPOCHS']}\")\n",
        "\n",
        "print_config()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "883ec470",
      "metadata": {
        "id": "883ec470"
      },
      "source": [
        "#Cell 03 — Load FER2013 and split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_cdznT_YnsPh",
      "metadata": {
        "id": "_cdznT_YnsPh"
      },
      "source": [
        "| **Step**                | **Explanation**                                                                                                                                          |\n",
        "| ----------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Import & Path Setup** | Uses `Path` to ensure `FER_CSV_PATH` is correctly resolved from `CONFIG`. Also includes an assertion to stop execution if the file is missing.           |\n",
        "| **CSV Read**            | Loads the FER2013 dataset via `pd.read_csv(FER_CSV_PATH)`.                                                                                               |\n",
        "| **Schema Check**        | Ensures dataset has at least `[\"emotion\", \"pixels\"]` columns, otherwise throws an error.                                                                 |\n",
        "| **Dataset Splits**      | Splits the dataset into: <br>• **Training**: 28,709 samples <br>• **Validation (PublicTest)**: 3,589 samples <br>• **Test (PrivateTest)**: 3,589 samples |\n",
        "| **Index Reset**         | Each split has `.reset_index(drop=True)` so that indices are clean and independent across splits.                                                        |\n",
        "| **Diagnostics**         | Prints the split sizes and confirms counts (`[Split] train=28709, val=3589, test=3589`).                                                                 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "cc13acbe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc13acbe",
        "outputId": "cd28de39-89e1-4912-8039-25703e82d75f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage\n",
            "Training       28709\n",
            "PublicTest      3589\n",
            "PrivateTest     3589\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# === Cell 03: Load FER2013 and split ===\n",
        "from pathlib import Path\n",
        "FER_CSV_PATH = Path(CONFIG[\"FER_CSV_PATH\"])\n",
        "assert FER_CSV_PATH.exists(), f\"CSV not found: {FER_CSV_PATH}\"\n",
        "\n",
        "df = pd.read_csv(FER_CSV_PATH)\n",
        "assert {\"emotion\",\"pixels\"}.issubset(set(df.columns)), f\"Bad columns: {df.columns.tolist()}\"\n",
        "print(df[\"Usage\"].value_counts())\n",
        "\n",
        "data_df = df  # Alias for compatibility with reference code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52b2f60b",
      "metadata": {
        "id": "52b2f60b"
      },
      "source": [
        "#Cell 04 — Dataset (48→96), returns tensor in [0..255], 1×H×W"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78tHm3VSo1Sc",
      "metadata": {
        "id": "78tHm3VSo1Sc"
      },
      "source": [
        "| **Aspect**                | **Design Choice**                                                             | **Rationale / Benefit**                                                                                |\n",
        "| ------------------------- | ----------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------ |\n",
        "| **Objective**             | Provide a minimal, deterministic interface from FER2013 CSV to training loop. | Ensures consistent contract across train/val/test, no hidden randomness or preprocessing in dataset.   |\n",
        "| **Tensor Contract**       | `x ∈ ℝ[1,H,W]` with values in **0–255 float32**, `y ∈ {0,…,6}`                | Stable shape (1 channel, H×W), raw grayscale preserved, labels cleanly passed to loss.                 |\n",
        "| **Representation**        | `[1, H, W]` (single grayscale channel)                                        | Matches FER’s original format, avoids 3-channel broadcasting bugs.                                     |\n",
        "| **Pixel Range**           | Raw float32 in **0–255**                                                      | Keeps dataset as “pure I/O,” centralizes normalization later (avoids double-normalization).            |\n",
        "| **Parsing**               | `np.fromstring(str(px), sep=\" \", dtype=np.float32)`                           | Efficient conversion from CSV’s space-separated pixel string to dense vector.                          |\n",
        "| **Corruption Guard**      | `assert arr.size == 48*48`                                                    | Fails fast if a row is malformed (data corruption check).                                              |\n",
        "| **Resizing**              | 48×48 → **96×96**, bilinear, `antialias=True`                                 | More spatial detail for features (eyes, mouth) without heavy compute cost. Standardized interpolation. |\n",
        "| **Tensor Handling**       | `.contiguous()`                                                               | Guarantees compact memory layout for CUDA/augmentations (avoids strided tensor issues).                |\n",
        "| **Label Handling**        | `int(self.df.iloc[i][\"emotion\"])` → auto batch collation to `torch.long`      | Compatible with `CrossEntropyLoss`, no extra conversion required.                                      |\n",
        "| **No Normalization Here** | Dataset does *not* normalize or augment                                       | Prevents double normalization; all stochasticity handled later (augmentor, loop).                      |\n",
        "| **Pipeline Fit**          | Compatible with Sobel/CBAM/EfficientNet-B0 stack and grayscale augmentor.     | Dataset delivers exactly what later stages expect, no hidden surprises.                                |\n",
        "| **Determinism**           | Zero random operations                                                        | Makes debugging easier; any stochasticity is controlled explicitly in augmentation stage.              |\n",
        "| **Efficiency**            | Uses float32 (4× RAM vs uint8)                                                | Simplifies pipeline, avoids repeated casting; memory trade-off acceptable on modern GPUs.              |\n",
        "| **Known Limits**          | Needs `InterpolationMode` import in fresh kernel; float32 may waste RAM       | Can optimize later (uint8 → float cast in augmentor) if memory constrained.                            |\n",
        "| **Scalability**           | Changing `IMG_SIZE` propagates safely downstream                              | Later stages adapt to whatever \\[1,H,W] dataset produces.                                              |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reference Codes"
      ],
      "metadata": {
        "id": "3yB_G51lP9tt"
      },
      "id": "3yB_G51lP9tt"
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorizing the dataset to three categories.\n",
        "# Training: To train the model.\n",
        "# PrivateTest: To test the train model; commonly known as Validation.\n",
        "# PublicTest: To test the final model on Test set to check how your model perfomed. Do not use this data as your validation data.\n",
        "train_df = data_df[data_df['Usage']=='Training']\n",
        "valid_df = data_df[data_df['Usage']=='PublicTest']\n",
        "test_df = data_df[data_df['Usage']=='PrivateTest']\n",
        "print(train_df.head())\n",
        "print(valid_df.head(-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZujksoEOem5",
        "outputId": "1545205b-7682-4c93-e829-5af111c6941b"
      },
      "id": "rZujksoEOem5",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   emotion                                             pixels     Usage\n",
            "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
            "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
            "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
            "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
            "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n",
            "       emotion                                             pixels       Usage\n",
            "28709        0  254 254 254 254 254 249 255 160 2 58 53 70 77 ...  PublicTest\n",
            "28710        1  156 184 198 202 204 207 210 212 213 214 215 21...  PublicTest\n",
            "28711        4  69 118 61 60 96 121 103 87 103 88 70 90 115 12...  PublicTest\n",
            "28712        6  205 203 236 157 83 158 120 116 94 86 155 180 2...  PublicTest\n",
            "28713        3  87 79 74 66 74 96 77 80 80 84 83 89 102 91 84 ...  PublicTest\n",
            "...        ...                                                ...         ...\n",
            "32292        3  0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 3 4 21 40 53 65 ...  PublicTest\n",
            "32293        4  178 176 172 173 173 174 176 173 166 166 206 22...  PublicTest\n",
            "32294        3  25 34 42 44 42 47 57 59 59 58 54 51 50 56 63 6...  PublicTest\n",
            "32295        4  255 255 255 255 255 255 255 255 255 255 255 25...  PublicTest\n",
            "32296        4  33 25 31 36 36 42 69 103 132 163 175 183 187 1...  PublicTest\n",
            "\n",
            "[3588 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"[Split] train={len(train_df)}, val={len(valid_df)}, test={len(test_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ear8_B0XOlop",
        "outputId": "6016f06c-e781-4b18-e1d6-a595cd23fd3a"
      },
      "id": "Ear8_B0XOlop",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Split] train=28709, val=3589, test=3589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test-check to see wether usage labels have been allocated to the dataset/not.\n",
        "valid_df = valid_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "print(test_df.head())\n",
        "print('   -----   -------    -------    --------     -----    -------')\n",
        "print(valid_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4meIfqRP4cR",
        "outputId": "61ec0cb9-a90c-4087-88b0-0fe8a38af37b"
      },
      "id": "l4meIfqRP4cR",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   emotion                                             pixels        Usage\n",
            "0        0  170 118 101 88 88 75 78 82 66 74 68 59 63 64 6...  PrivateTest\n",
            "1        5  7 5 8 6 7 3 2 6 5 4 4 5 7 5 5 5 6 7 7 7 10 10 ...  PrivateTest\n",
            "2        6  232 240 241 239 237 235 246 117 24 24 22 13 12...  PrivateTest\n",
            "3        4  200 197 149 139 156 89 111 58 62 95 113 117 11...  PrivateTest\n",
            "4        2  40 28 33 56 45 33 31 78 152 194 200 186 196 20...  PrivateTest\n",
            "   -----   -------    -------    --------     -----    -------\n",
            "   emotion                                             pixels       Usage\n",
            "0        0  254 254 254 254 254 249 255 160 2 58 53 70 77 ...  PublicTest\n",
            "1        1  156 184 198 202 204 207 210 212 213 214 215 21...  PublicTest\n",
            "2        4  69 118 61 60 96 121 103 87 103 88 70 90 115 12...  PublicTest\n",
            "3        6  205 203 236 157 83 158 120 116 94 86 155 180 2...  PublicTest\n",
            "4        3  87 79 74 66 74 96 77 80 80 84 83 89 102 91 84 ...  PublicTest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization of the train and validation data.\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import torch.nn as nn\n",
        "\n",
        "class expressions(Dataset):\n",
        "    def __init__(self, df, transforms=None):\n",
        "        self.df = df\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.loc[index]\n",
        "        image, label = np.array([x.split() for x in self.df.loc[index, ['pixels']]]), row['emotion']\n",
        "        #image = image.reshape(1,48,48)\n",
        "        image = np.asarray(image).astype(np.uint8).reshape(48,48,1)\n",
        "        #image = np.reshape(image,(1,48,48))\n",
        "\n",
        "\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return image.clone().detach(), label\n",
        "\n",
        "#import albumentations as A\n",
        "stats = ([0.5],[0.5])\n",
        "\n",
        "Labels = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']\n",
        "\n",
        "train_tsfm = T.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.Grayscale(num_output_channels=1),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(*stats,inplace=True),\n",
        "])\n",
        "valid_tsfm = T.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.Grayscale(num_output_channels=1),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(*stats,inplace=True)\n",
        "])\n",
        "\n",
        "train_ds = expressions(train_df, train_tsfm)\n",
        "valid_ds = expressions(valid_df, valid_tsfm)\n",
        "test_ds = expressions(test_df, valid_tsfm)\n",
        "val_ds = valid_ds  # Alias for existing code that expects val_ds\n",
        "\n",
        "batch_size = 400\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True,\n",
        "                      num_workers=2, pin_memory=True)\n",
        "valid_dl = DataLoader(valid_ds, batch_size*2,\n",
        "                    num_workers=2, pin_memory=True)\n",
        "test_dl = DataLoader(test_ds, batch_size*2,\n",
        "                    num_workers=2, pin_memory=True)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "input_size = 48*48\n",
        "output_size = len(Labels)\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    predictions, preds = torch.max(output, dim=1)\n",
        "    return torch.tensor(torch.sum(preds==labels).item()/len(preds))"
      ],
      "metadata": {
        "id": "OynQ5TCQP8Cg"
      },
      "id": "OynQ5TCQP8Cg",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expression model class for training and validation purpose.\n",
        "\n",
        "class expression_model(nn.Module):\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)\n",
        "        loss = F.cross_entropy(out, labels)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)\n",
        "        loss = F.cross_entropy(out, labels)\n",
        "        acc = accuracy(out, labels)\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()\n",
        "        batch_acc = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_acc).mean()\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "\n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch[{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))"
      ],
      "metadata": {
        "id": "JvyBvhPpTASX"
      },
      "id": "JvyBvhPpTASX",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To check wether Google Colab GPU has been assigned/not.\n",
        "torch.cuda.is_available()\n",
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "device = get_default_device()\n",
        "print(f'You are training on: {device}.')\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7TBBlepTA8K",
        "outputId": "ef2b322b-2149-4707-e00a-81187ddc9d2d"
      },
      "id": "d7TBBlepTA8K",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are training on: cuda.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "valid_dl = DeviceDataLoader(valid_dl, device)\n",
        "test_dl = DeviceDataLoader(test_dl, device)\n",
        "\n",
        "val_dl = valid_dl  # Alias for existing code that expects val_dl\n",
        "\n",
        "baseline_train_dl = train_dl  # Reference code training DataLoader (48x48, DeviceDataLoader wrapped)\n",
        "baseline_valid_dl = valid_dl  # Reference code validation DataLoader (48x48, DeviceDataLoader wrapped)\n",
        "baseline_test_dl = test_dl    # Reference code test DataLoader (48x48, DeviceDataLoader wrapped)"
      ],
      "metadata": {
        "id": "nyyv0X9ATFsR"
      },
      "id": "nyyv0X9ATFsR",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model - 7 layer\n",
        "class expression(expression_model):\n",
        "    def __init__(self,classes):\n",
        "        super().__init__()\n",
        "        self.num_classes = classes\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=3, padding=1),  #(input channels, output channels)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(8, 32, kernel_size=3, padding=1),  #(input channels, output channels)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 64 x 24 x 24\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 128 x 12 x 12\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # output: 256 x 6 x 6\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256*6*6, 2304),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2304, 1152),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1152, 576),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(576,288),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(288,144),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(144,self.num_classes))\n",
        "\n",
        "    def forward(self, xb):\n",
        "        return self.network(xb)"
      ],
      "metadata": {
        "id": "zM_d5oNZTGUD"
      },
      "id": "zM_d5oNZTGUD",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fvcore.nn import FlopCountAnalysis\n",
        "input = torch.randn(1, 1, 48, 48) # The input size should be the same as the size that you put into your model\n",
        "#Get the network and its FLOPs\n",
        "num_classes = 7\n",
        "model = expression(num_classes)\n",
        "flops = FlopCountAnalysis(model, input)\n",
        "print(f\"FLOPs: {flops.total()/1e9:.5f} GFLOPs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coBkqKfRTJYf",
        "outputId": "7e09e02f-9b45-45b8-9dd2-0bfe02a11180"
      },
      "id": "coBkqKfRTJYf",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.nn.jit_analysis:Unsupported operator aten::max_pool2d encountered 3 time(s)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOPs: 0.32751 GFLOPs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### End of Reference Code"
      ],
      "metadata": {
        "id": "2NWP8JxzIVm7"
      },
      "id": "2NWP8JxzIVm7"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "3c93cf74",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c93cf74",
        "outputId": "3edc506e-ea81-4f22-8d6c-2a52db18d8c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Dataset] HybridEffNet datasets (96x96) and reference code datasets (48x48) both ready.\n"
          ]
        }
      ],
      "source": [
        "# === Cell 04: Dataset (48→96), returns [1,H,W] in 0..255 float ===\n",
        "import torchvision.transforms.functional as VF\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class FER2013Dataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, img_size: int = 96):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_size = int(img_size)\n",
        "        if len(self.df) > 0:\n",
        "            _ = self._get_x(0)\n",
        "\n",
        "    def _get_x(self, i: int) -> torch.Tensor:\n",
        "        px = self.df.iloc[i][\"pixels\"]\n",
        "        arr = np.fromstring(str(px), sep=\" \", dtype=np.float32)\n",
        "        assert arr.size == 48*48, f\"Row {i}: expected 2304 pixels, got {arr.size}\"\n",
        "        x = torch.from_numpy(arr.reshape(48, 48)).unsqueeze(0)  # [1,48,48], float32 in 0..255\n",
        "        x = VF.resize(\n",
        "            x,\n",
        "            [self.img_size, self.img_size],\n",
        "            interpolation=torchvision.transforms.InterpolationMode.BILINEAR,\n",
        "            antialias=True,\n",
        "        )\n",
        "        return x\n",
        "\n",
        "    def __len__(self): return len(self.df)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        x = self._get_x(i).contiguous()  # [1,H,W], float32 0..255\n",
        "        y = int(self.df.iloc[i][\"emotion\"])\n",
        "        return x, y\n",
        "\n",
        "# Create HybridEffNet datasets (96x96)\n",
        "IMG_SIZE = int(CONFIG[\"IMG_SIZE\"])\n",
        "train_ds_hybrid = FER2013Dataset(train_df, IMG_SIZE)\n",
        "val_ds_hybrid   = FER2013Dataset(valid_df,   IMG_SIZE)\n",
        "test_ds_hybrid  = FER2013Dataset(test_df,  IMG_SIZE)\n",
        "\n",
        "# Keep reference code datasets available for baseline experiments (48x48)\n",
        "# train_ds, valid_ds, test_ds from reference code are still available above\n",
        "\n",
        "# Set default datasets to HybridEffNet versions for main training\n",
        "train_ds = train_ds_hybrid\n",
        "val_ds = val_ds_hybrid\n",
        "test_ds = test_ds_hybrid\n",
        "\n",
        "print(\"[Dataset] HybridEffNet datasets (96x96) and reference code datasets (48x48) both ready.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fc72637",
      "metadata": {
        "id": "3fc72637"
      },
      "source": [
        "#Cell 05 — DataLoaders"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ywuLnWmpLnz",
      "metadata": {
        "id": "1ywuLnWmpLnz"
      },
      "source": [
        "| **Aspect**             | **Design Choice**             | **Rationale / Benefit**                                                  |\n",
        "| ---------------------- | ----------------------------- | ------------------------------------------------------------------------ |\n",
        "| **Batch size**         | `BATCH` from config           | Flexible tuning; balances GPU memory usage and throughput.               |\n",
        "| **Workers**            | `NUM_WORKERS` from config     | Parallel data loading → faster pipeline.                                 |\n",
        "| **Pin memory**         | Enabled if CUDA available     | Faster CPU→GPU transfer, avoids page faults.                             |\n",
        "| **Training loader**    | Shuffle=True, batch=BATCH     | Randomized sampling prevents bias, improves generalization.              |\n",
        "| **Validation loader**  | Shuffle=False, batch=2×BATCH  | Deterministic eval + speedup with larger batches.                        |\n",
        "| **Test loader**        | Same as validation            | Ensures reproducible benchmarking, efficient evaluation.                 |\n",
        "| **Persistent workers** | Kept alive if workers>0       | Reduces overhead of re-spawning processes per epoch.                     |\n",
        "| **Sanity check**       | Print val batch shape + range | Debug safeguard: verifies `[N,1,96,96]` shape and pixel range `[0,255]`. |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "ed12317c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed12317c",
        "outputId": "cc695110-e374-4c72-8a28-d9a34c068845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DataLoaders] HybridEffNet DataLoaders (96x96) and reference code DataLoaders (48x48) both ready.\n",
            "[Check] val batch: torch.Size([384, 1, 96, 96]), range [0.0,255.0]\n"
          ]
        }
      ],
      "source": [
        "# === Cell 05: DataLoaders ===\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH = int(CONFIG[\"BATCH_SIZE\"])\n",
        "NUM_WORKERS = int(CONFIG[\"NUM_WORKERS\"])\n",
        "PIN = bool(torch.cuda.is_available())  # safer on CPU-only runs\n",
        "\n",
        "train_dl_hybrid = DataLoader(\n",
        "    train_ds, batch_size=BATCH, shuffle=True,\n",
        "    num_workers=NUM_WORKERS, pin_memory=PIN, persistent_workers=(NUM_WORKERS>0)\n",
        ")\n",
        "val_dl_hybrid   = DataLoader(\n",
        "    val_ds,   batch_size=BATCH*2, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=PIN, persistent_workers=(NUM_WORKERS>0)\n",
        ")\n",
        "test_dl_hybrid  = DataLoader(\n",
        "    test_ds,  batch_size=BATCH*2, shuffle=False,\n",
        "    num_workers=NUM_WORKERS, pin_memory=PIN, persistent_workers=(NUM_WORKERS>0)\n",
        ")\n",
        "\n",
        "# Set default DataLoaders to HybridEffNet versions for main training\n",
        "train_dl = train_dl_hybrid\n",
        "val_dl = val_dl_hybrid\n",
        "test_dl = test_dl_hybrid\n",
        "\n",
        "# Reference code DataLoaders (train_dl, valid_dl, test_dl from DeviceDataLoader) still available for baseline experiments\n",
        "\n",
        "print(\"[DataLoaders] HybridEffNet DataLoaders (96x96) and reference code DataLoaders (48x48) both ready.\")\n",
        "\n",
        "xb, yb = next(iter(val_dl))\n",
        "print(f\"[Check] val batch: {xb.shape}, range [{xb.min():.1f},{xb.max():.1f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb43e1aa",
      "metadata": {
        "id": "bb43e1aa"
      },
      "source": [
        "#Cell 06 — Advanced augmentation primitives (photometric, geometric, occlusion, elastic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "0cba5bd2",
      "metadata": {
        "id": "0cba5bd2"
      },
      "outputs": [],
      "source": [
        "# === Cell 06: Advanced augmentation primitives (grayscale) ===\n",
        "import io\n",
        "from PIL import Image, ImageOps\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def _to_pil_gray(x255: torch.Tensor) -> Image.Image:\n",
        "    x = x255.clamp(0,255).to(torch.uint8).squeeze(0).cpu().numpy()\n",
        "    return Image.fromarray(x, mode='L')\n",
        "\n",
        "def _from_pil_gray(img: Image.Image) -> torch.Tensor:\n",
        "    return torch.tensor(np.array(img, dtype=np.uint8), dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "def _clip(x): return x.clamp(0.0, 255.0)\n",
        "\n",
        "# photometric\n",
        "def gauss_noise(x, sigma=0.02): return _clip(x + torch.randn_like(x)*(sigma*255.))\n",
        "def rand_gamma(x, gmin=0.85, gmax=1.25):\n",
        "    g = random.uniform(gmin,gmax); x01=(x/255.).clamp(0,1); return (x01**g)*255.\n",
        "def rand_contrast(x, scale=0.25):\n",
        "    c = 1.0+random.uniform(-scale,scale); m=x.mean(dim=(1,2),keepdim=True); return _clip((x-m)*c+m)\n",
        "def rand_equalize(x):\n",
        "    img=_to_pil_gray(x); img=ImageOps.equalize(img); return _from_pil_gray(img).to(x.dtype).to(x.device)\n",
        "def rand_jpeg(x, qmin=55, qmax=85):\n",
        "    img=_to_pil_gray(x); buf=io.BytesIO(); img.save(buf,format='JPEG',quality=random.randint(qmin,qmax))\n",
        "    buf.seek(0); img2=Image.open(buf).convert('L'); return _from_pil_gray(img2).to(x.dtype).to(x.device)\n",
        "def rand_vignette(x, strength=0.25):\n",
        "    _,H,W=x.shape; yy,xx=torch.meshgrid(torch.linspace(-1,1,H,device=x.device),\n",
        "                                        torch.linspace(-1,1,W,device=x.device),indexing='ij')\n",
        "    r=torch.sqrt(xx**2+yy**2); mask=1.0-strength*(r/r.max()).clamp(0,1)\n",
        "    return _clip(x*mask.unsqueeze(0))\n",
        "\n",
        "# Range-safe blur: convert to [0,1] → blur → back to [0,255]\n",
        "def rand_blur(x, k=3):\n",
        "    x01 = (x/255.).clamp(0,1)\n",
        "    y01 = torchvision.transforms.functional.gaussian_blur(x01, kernel_size=k)\n",
        "    return (y01 * 255.0).clamp(0,255)\n",
        "\n",
        "# geometric\n",
        "def rand_affine_small(x, max_rot=12, max_trans=0.08, max_shear=8.0, max_scale=0.08):\n",
        "    H,W=x.shape[-2:]\n",
        "    angle=random.uniform(-max_rot,max_rot)\n",
        "    trans=[int(random.uniform(-max_trans,max_trans)*W),int(random.uniform(-max_trans,max_trans)*H)]\n",
        "    scale=1.0+random.uniform(-max_scale,max_scale)\n",
        "    shear=[random.uniform(-max_shear,max_shear),0.0]\n",
        "    return torchvision.transforms.functional.affine(x, angle=angle, translate=trans, scale=scale, shear=shear)\n",
        "\n",
        "def rand_pad_crop(x, pad=3):\n",
        "    _,H,W=x.shape; xpad=F.pad(x,(pad,pad,pad,pad),mode='reflect'); i=random.randint(0,2*pad); j=random.randint(0,2*pad)\n",
        "    return xpad[:,i:i+H, j:j+W]\n",
        "\n",
        "def rand_hflip(x, p=0.5): return torchvision.transforms.functional.hflip(x) if random.random()<p else x\n",
        "\n",
        "# elastic\n",
        "def rand_elastic(x, alpha=1.0, sigma=4.0):\n",
        "    _,H,W=x.shape\n",
        "    def _gkern(k=21,s=sigma):\n",
        "        ax=torch.arange(k,device=x.device)-(k-1)/2; ker=torch.exp(-(ax**2)/(2*s*s)); ker/=ker.sum(); return ker\n",
        "    k=21; gx=_gkern(k).view(1,1,1,k); gy=_gkern(k).view(1,1,k,1)\n",
        "    dx=F.conv2d(F.conv2d(torch.randn(1,1,H,W,device=x.device),gx,padding=(0,k//2)),gy,padding=(k//2,0)).squeeze()*alpha\n",
        "    dy=F.conv2d(F.conv2d(torch.randn(1,1,H,W,device=x.device),gx,padding=(0,k//2)),gy,padding=(k//2,0)).squeeze()*alpha\n",
        "    yy,xx=torch.meshgrid(torch.linspace(-1,1,H,device=x.device),\n",
        "                         torch.linspace(-1,1,W,device=x.device),indexing='ij')\n",
        "    xx=(xx+dx/(W/2)).clamp(-1,1); yy=(yy+dy/(H/2)).clamp(-1,1)\n",
        "    grid=torch.stack([xx,yy],dim=-1).unsqueeze(0)\n",
        "    return F.grid_sample(x.unsqueeze(0), grid, mode='bilinear', padding_mode='border', align_corners=True).squeeze(0)\n",
        "\n",
        "# occlusio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "pBRdqZrNfOQO",
      "metadata": {
        "id": "pBRdqZrNfOQO"
      },
      "outputs": [],
      "source": [
        "# === Cell 06.x: Aug Utils — band_occlusion  ===\n",
        "import torch\n",
        "import random\n",
        "\n",
        "def _neutral_fill_value_range_aware(x: torch.Tensor) -> float:\n",
        "    \"\"\"\n",
        "    Mid-gray fill compatible with [-1,1], [0,1], or [0,255] ranges.\n",
        "    \"\"\"\n",
        "    xmin = float(x.min())\n",
        "    xmax = float(x.max())\n",
        "    if xmin >= -1.0 and xmax <= 1.0:\n",
        "        return 0.0\n",
        "    if xmin >= 0.0 and xmax <= 1.0:\n",
        "        return 0.5\n",
        "    if xmax > 1.0:\n",
        "        return 127.5\n",
        "    return 0.0\n",
        "\n",
        "def band_occlusion(img: torch.Tensor, mode: str = 'eyes', frac: float = 0.18) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Draw a horizontal band occlusion corresponding to a face region.\n",
        "    Args:\n",
        "        img: [C,H,W] tensor (float). Accepted ranges: [-1,1], [0,1], [0,255].\n",
        "        mode: 'eyes' | 'mouth' | 'top' | 'bottom' | 'mid'\n",
        "        frac: vertical band height as fraction of H.\n",
        "    Returns:\n",
        "        Tensor with an occluded band (on a clone).\n",
        "    \"\"\"\n",
        "    if not (torch.is_tensor(img) and img.ndim == 3):\n",
        "        raise TypeError(\"band_occlusion expects a tensor of shape [C,H,W].\")\n",
        "\n",
        "    C, H, W = img.shape\n",
        "    band_h = max(1, int(H * float(frac)))\n",
        "    fill = _neutral_fill_value_range_aware(img)\n",
        "    out = img.clone()\n",
        "\n",
        "    # Default anchors (approximate facial landmarks for FER crops)\n",
        "    if mode == 'eyes':\n",
        "        top = int(0.30 * H) - band_h // 2\n",
        "    elif mode == 'mouth':\n",
        "        top = int(0.72 * H) - band_h // 2\n",
        "    elif mode == 'mid':\n",
        "        top = int(0.50 * H) - band_h // 2\n",
        "    elif mode == 'top':\n",
        "        top = int(0.10 * H)\n",
        "    elif mode == 'bottom':\n",
        "        top = int(0.85 * H) - band_h\n",
        "    else:\n",
        "        # Fallback: random vertical placement\n",
        "        top = random.randint(0, max(0, H - band_h))\n",
        "\n",
        "    top = max(0, min(top, H - band_h))\n",
        "    out[:, top:top + band_h, :] = fill\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6602b98",
      "metadata": {
        "id": "a6602b98"
      },
      "source": [
        "#Cell 07 — AugMix‑lite and advanced augmentation builder (returns [-1,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "8d7bef03",
      "metadata": {
        "id": "8d7bef03"
      },
      "outputs": [],
      "source": [
        "# === Cell 07: AugMix-lite + advanced augmentation builder (→ [-1,1]) ===\n",
        "def _apply_bank(x, bank, k=2):\n",
        "    y=x.clone()\n",
        "    for _ in range(k):\n",
        "        y = random.choice(bank)(y)\n",
        "    return y\n",
        "\n",
        "def augmix_lite(x, banks, alpha=0.65, branches=2, depth=2):\n",
        "    mix=x.clone()\n",
        "    for _ in range(branches):\n",
        "        y=_apply_bank(x, random.choice(banks), k=depth)\n",
        "        mix=mix+y\n",
        "    mix = mix / (branches+1.0)\n",
        "    return (1-alpha)*x + alpha*mix\n",
        "\n",
        "def build_advanced_fer_augment(strength: float):\n",
        "    s=float(max(0.0,min(1.0,strength)))\n",
        "    # probabilities\n",
        "    p_photo=0.7*(0.5+0.5*s); p_geom=0.6*(0.5+0.5*s); p_occl=0.40*(0.5+0.5*s)\n",
        "    p_equal=0.20*s; p_blur=0.15*s\n",
        "    # magnitudes\n",
        "    gamma_rng=(0.85-0.15*s, 1.20+0.05*s)\n",
        "    contrast=0.20+0.10*s\n",
        "    jpeg_q=(55-int(10*s), 85)\n",
        "    vignette=0.15+0.20*s\n",
        "    elastic_a=0.6+0.8*s\n",
        "    rot=10+5*s; shear=6+4*s; trans=0.06+0.03*s; scale=0.06+0.04*s\n",
        "\n",
        "    photometric_bank = [\n",
        "        lambda z: gauss_noise(z, sigma=0.015+0.02*s),\n",
        "        lambda z: rand_gamma(z, *gamma_rng),\n",
        "        lambda z: rand_contrast(z, scale=contrast),\n",
        "        lambda z: rand_jpeg(z, qmin=jpeg_q[0], qmax=jpeg_q[1]),\n",
        "        lambda z: rand_vignette(z, strength=vignette),\n",
        "    ]\n",
        "    geometric_bank = [\n",
        "        lambda z: rand_affine_small(z, max_rot=rot, max_trans=trans, max_shear=shear, max_scale=scale),\n",
        "        lambda z: rand_pad_crop(z, pad=3),\n",
        "        lambda z: rand_hflip(z, p=0.5),\n",
        "        lambda z: rand_elastic(z, alpha=elastic_a, sigma=4.0),\n",
        "    ]\n",
        "    occlusion_bank = [\n",
        "        lambda z: band_occlusion(z, mode='eyes',  frac=0.16+0.06*s),\n",
        "        lambda z: band_occlusion(z, mode='mouth', frac=0.16+0.06*s),\n",
        "        lambda z: band_occlusion(z, mode='top',   frac=0.14+0.06*s),\n",
        "        lambda z: localized_erasing(z, min_frac=0.01, max_frac=0.05),\n",
        "    ]\n",
        "    banks=[photometric_bank, geometric_bank, occlusion_bank]\n",
        "\n",
        "    def _norm_to_m11(x255):\n",
        "        x01=(x255/255.).clamp(0,1)\n",
        "        return (x01 - 0.5) * 2.0\n",
        "\n",
        "    def _augment(x):\n",
        "        if random.random() < p_geom:  x = rand_pad_crop(x, pad=3)\n",
        "        if random.random() < p_blur:  x = rand_blur(x, k=3)\n",
        "        if random.random() < p_photo: x = random.choice(photometric_bank)(x)\n",
        "        x = augmix_lite(x, banks=banks, alpha=CONFIG.get(\"AUG_ALPHA\",0.65), branches=2, depth=2)\n",
        "        if random.random() < p_geom:  x = random.choice(geometric_bank)(x)\n",
        "        if random.random() < p_occl:  x = random.choice(occlusion_bank)(x)\n",
        "        if random.random() < p_equal: x = rand_equalize(x)\n",
        "        return _norm_to_m11(x)\n",
        "\n",
        "    return _augment\n",
        "\n",
        "FER_AUG_FACTORY = build_advanced_fer_augment if CONFIG.get(\"USE_AUG_ADV\", False) else build_advanced_fer_augment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f016c61",
      "metadata": {
        "id": "9f016c61"
      },
      "source": [
        "#Cell 08 — Metrics, class weights, losses (Label‑Smoothing + Focal composite)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "2c35ae68",
      "metadata": {
        "id": "2c35ae68"
      },
      "outputs": [],
      "source": [
        "# === Cell 08: Metrics, class weights, composite loss ===\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "\n",
        "def accuracy(logits, targets): return (logits.argmax(1) == targets).float().mean()\n",
        "\n",
        "def compute_class_weights(df) -> torch.Tensor:\n",
        "    counts = Counter(int(e) for e in df[\"emotion\"].tolist())\n",
        "    total = sum(counts.values())\n",
        "    w = torch.tensor([total / max(1, counts.get(c,1)) for c in range(7)], dtype=torch.float32)\n",
        "    return w / w.mean()\n",
        "\n",
        "CLASS_WEIGHTS = compute_class_weights(train_df)\n",
        "\n",
        "class LabelSmoothingCE(nn.Module):\n",
        "    def __init__(self, eps=0.10, reduction='mean'):\n",
        "        super().__init__(); self.eps=eps; self.reduction=reduction\n",
        "    def forward(self, logits, targets):\n",
        "        n = logits.size(-1); logp = F.log_softmax(logits, dim=-1)\n",
        "        with torch.no_grad():\n",
        "            true = torch.zeros_like(logp).fill_(self.eps/(n-1))\n",
        "            true.scatter_(1, targets.unsqueeze(1), 1.0 - self.eps)\n",
        "        loss = -(true * logp).sum(dim=1)\n",
        "        return loss.mean() if self.reduction=='mean' else loss\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=1.5, reduction='mean'):\n",
        "        super().__init__(); self.g=gamma; self.reduction=reduction\n",
        "    def forward(self, logits, targets):\n",
        "        ce = F.cross_entropy(logits, targets, reduction='none')\n",
        "        pt = torch.exp(-ce)\n",
        "        fl = (1-pt).pow(self.g) * ce\n",
        "        return fl.mean() if self.reduction=='mean' else fl\n",
        "\n",
        "class SmoothedFocal(nn.Module):\n",
        "    def __init__(self, eps=0.10, gamma=1.5, alpha=0.70, weight=None):\n",
        "        super().__init__(); self.a=alpha; self.w=weight\n",
        "        self.lsce = LabelSmoothingCE(eps); self.focal = FocalLoss(gamma)\n",
        "    def forward(self, logits, targets):\n",
        "        if self.w is not None:\n",
        "            ce = F.cross_entropy(logits, targets, reduction='none', weight=self.w.to(logits.device))\n",
        "            pt = torch.exp(-ce); fl = (1-pt).pow(1.5) * ce\n",
        "            ls = self.lsce(logits, targets)\n",
        "            return self.a*ls + (1-self.a)*fl.mean()\n",
        "        return self.a*self.lsce(logits, targets) + (1-self.a)*self.focal(logits, targets)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54368702",
      "metadata": {
        "id": "54368702"
      },
      "source": [
        "#Cell 09 — MixUp / CutMix and mixed criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "9aa56ef2",
      "metadata": {
        "id": "9aa56ef2"
      },
      "outputs": [],
      "source": [
        "# === Cell 09: MixUp / CutMix and mixed criterion ===\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    if alpha <= 0.0: return x, y, 1.0, None\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    idx = torch.randperm(x.size(0), device=x.device)\n",
        "    return lam*x + (1-lam)*x[idx], (y, y[idx]), lam, idx\n",
        "\n",
        "def cutmix_data(x, y, alpha=1.0, min_lam=0.3, max_lam=0.7):\n",
        "    if alpha <= 0.0: return x, y, 1.0, None\n",
        "    lam = float(np.clip(np.random.beta(alpha, alpha), min_lam, max_lam))\n",
        "    B,C,H,W = x.size(); idx = torch.randperm(B, device=x.device)\n",
        "    cut_w = int(W * math.sqrt(1 - lam)); cut_h = int(H * math.sqrt(1 - lam))\n",
        "    cx, cy = np.random.randint(W), np.random.randint(H)\n",
        "    x1, x2 = np.clip(cx - cut_w//2, 0, W), np.clip(cx + cut_w//2, 0, W)\n",
        "    y1, y2 = np.clip(cy - cut_h//2, 0, H), np.clip(cy + cut_h//2, 0, H)\n",
        "    x[:, :, y1:y2, x1:x2] = x[idx, :, y1:y2, x1:x2]\n",
        "    lam = 1 - ((x2-x1)*(y2-y1) / (W*H + 1e-9))\n",
        "    return x, (y, y[idx]), lam, idx\n",
        "\n",
        "def mixed_criterion(criterion, logits, targets_mix, lam):\n",
        "    if isinstance(targets_mix, tuple):\n",
        "        y_a, y_b = targets_mix\n",
        "        return lam * criterion(logits, y_a) + (1-lam) * criterion(logits, y_b)\n",
        "    return criterion(logits, targets_mix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b0a3aba",
      "metadata": {
        "id": "3b0a3aba"
      },
      "source": [
        "#Cell 10 — EMA (exponential moving average)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "40fbb1f9",
      "metadata": {
        "id": "40fbb1f9"
      },
      "outputs": [],
      "source": [
        "# === Cell 10: EMA ===\n",
        "import torch.nn as nn\n",
        "\n",
        "class EMA:\n",
        "    def __init__(self, model: nn.Module, decay: float = 0.999):\n",
        "        self.decay=float(decay); self.shadow={}; self.backup={}\n",
        "        for n,p in model.named_parameters():\n",
        "            if p.requires_grad: self.shadow[n]=p.data.clone()\n",
        "    def update(self, model):\n",
        "        for n,p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.shadow[n]=(1-self.decay)*p.data + self.decay*self.shadow[n]\n",
        "    def apply_shadow(self, model):\n",
        "        self.backup={}\n",
        "        for n,p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.backup[n]=p.data.clone(); p.data=self.shadow[n].clone()\n",
        "    def restore(self, model):\n",
        "        for n,p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                p.data=self.backup[n].clone()\n",
        "        self.backup={}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e41b5d6b",
      "metadata": {
        "id": "e41b5d6b"
      },
      "source": [
        "#Cell 11 — CBAM and Sobel stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "4548e23b",
      "metadata": {
        "id": "4548e23b"
      },
      "outputs": [],
      "source": [
        "# === Cell 11: CBAM + Sobel stem ===\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, ch, r=8):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Conv2d(ch, max(1,ch//r), 1, bias=True), nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(max(1,ch//r), ch, 1, bias=True)\n",
        "        )\n",
        "        self.spatial = nn.Sequential(nn.Conv2d(2,1,kernel_size=7,padding=3,bias=False), nn.Sigmoid())\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        ca = self.sigmoid(self.mlp(F.adaptive_avg_pool2d(x,1) + F.adaptive_max_pool2d(x,1)))\n",
        "        x = x * ca\n",
        "        ms = torch.cat([x.mean(1,keepdim=True), x.max(1,keepdim=True)[0]], dim=1)\n",
        "        return x * self.spatial(ms)\n",
        "\n",
        "class SobelLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        kx = torch.tensor([[1,0,-1],[2,0,-2],[1,0,-1]], dtype=torch.float32)\n",
        "        ky = torch.tensor([[1,2,1],[0,0,0],[-1,-2,-1]], dtype=torch.float32)\n",
        "        w  = torch.stack([kx, ky]).unsqueeze(1)   # (2,1,3,3)\n",
        "        self.register_buffer('w', w)\n",
        "    def forward(self, x):                          # x: [B,1,H,W]\n",
        "        edges = F.conv2d(x, self.w, padding=1)     # [B,2,H,W]\n",
        "        return torch.cat([x, edges], dim=1)        # [B,3,H,W]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b3be854",
      "metadata": {
        "id": "8b3be854"
      },
      "source": [
        "#Cell 12 — Model: EfficientNet‑B0 + CBAM + Sobel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "7efc18e5",
      "metadata": {
        "id": "7efc18e5"
      },
      "outputs": [],
      "source": [
        "# === Cell 12: HybridEffNet (EfficientNet‑B0 + CBAM + Sobel) ===\n",
        "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
        "\n",
        "CLASSIFIER_DROPOUT = 0.30\n",
        "USE_CBAM = True\n",
        "\n",
        "class HybridEffNet(nn.Module):\n",
        "    def __init__(self, num_classes=7, classifier_dropout=CLASSIFIER_DROPOUT, use_cbam=USE_CBAM):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        base = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
        "        self.sobel    = SobelLayer()          # 1→3 ch stem\n",
        "        self.features = base.features\n",
        "        self.pool     = nn.AdaptiveAvgPool2d(1)\n",
        "        self.cbam     = CBAM(1280) if use_cbam else None\n",
        "        self.bn       = nn.BatchNorm1d(1280)\n",
        "        self.drop     = nn.Dropout(p=classifier_dropout)\n",
        "        self.head     = nn.Linear(1280, num_classes)\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, x1):                    # x1 in [-1,1], [B,1,H,W]\n",
        "        x3 = self.sobel(x1)                   # [B,3,H,W]\n",
        "        f  = self.features(x3)                # [B,1280,h,w]\n",
        "        if self.cbam is not None:\n",
        "            f = self.cbam(f)\n",
        "        f  = self.pool(f).flatten(1)          # [B,1280]\n",
        "        f  = self.bn(f)\n",
        "        f  = self.drop(f)\n",
        "        return self.head(f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf0d8191",
      "metadata": {
        "id": "bf0d8191"
      },
      "source": [
        "#Cell 13 — Optimizer, scheduler, early stopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "a4c368f5",
      "metadata": {
        "id": "a4c368f5"
      },
      "outputs": [],
      "source": [
        "# === Cell 13: Optimizer + Scheduler + EarlyStopping (updated) ===\n",
        "import math\n",
        "import torch\n",
        "\n",
        "def make_optimizer(params, hp, cfg):\n",
        "    \"\"\"\n",
        "    AdamW (default) or SGD+Nesterov via CONFIG['USE_SGD'] toggle.\n",
        "    Uses HP['LR'] and HP['WD'].\n",
        "    \"\"\"\n",
        "    if cfg.get(\"USE_SGD\", False):\n",
        "        return torch.optim.SGD(\n",
        "            params,\n",
        "            lr=hp[\"LR\"],\n",
        "            momentum=hp[\"SGD_MOMENTUM\"],\n",
        "            nesterov=hp[\"SGD_NESTEROV\"],\n",
        "            weight_decay=hp[\"WD\"],\n",
        "        )\n",
        "    return torch.optim.AdamW(params, lr=hp[\"LR\"], weight_decay=hp[\"WD\"])\n",
        "\n",
        "# Back‑compat cosine helper (epoch-level stepping)\n",
        "class WarmupCosine:\n",
        "    def __init__(self, opt, warmup_epochs, max_epochs, lr_min=1e-6, lr_max=None):\n",
        "        self.opt = opt\n",
        "        self.warmup = max(1, int(warmup_epochs))\n",
        "        self.maxe = int(max_epochs)\n",
        "        self.t = 0\n",
        "        if lr_max is None:\n",
        "            lr_max = max(pg['lr'] for pg in opt.param_groups)\n",
        "        self.lr_min, self.lr_max = lr_min, lr_max\n",
        "    def step(self):\n",
        "        self.t += 1\n",
        "        if self.t <= self.warmup:\n",
        "            lr = self.lr_min + (self.lr_max - self.lr_min) * (self.t / self.warmup)\n",
        "        else:\n",
        "            tt = (self.t - self.warmup) / max(1, (self.maxe - self.warmup))\n",
        "            lr = self.lr_min + 0.5 * (self.lr_max - self.lr_min) * (1 + math.cos(math.pi * tt))\n",
        "        for g in self.opt.param_groups: g['lr'] = lr\n",
        "        return lr\n",
        "\n",
        "# NOTE:\n",
        "# In Cell 02 you already have build_scheduler(...) and helpers like\n",
        "#   - scheduler_steps_per_batch()\n",
        "#   - current_lr()\n",
        "# We simply keep EarlyStopping here and let Cell 14 call build_scheduler.\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stop on validation loss. Call .step(val_loss) each epoch.\"\"\"\n",
        "    def __init__(self, patience=8, min_delta=1e-4):\n",
        "        self.patience = int(patience)\n",
        "        self.min_delta = float(min_delta)\n",
        "        self.best = float('inf')\n",
        "        self.bad = 0\n",
        "    def step(self, val_loss: float) -> bool:\n",
        "        if val_loss < self.best - self.min_delta:\n",
        "            self.best = val_loss\n",
        "            self.bad = 0\n",
        "            return False\n",
        "        self.bad += 1\n",
        "        return self.bad >= self.patience\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "446b04a2",
      "metadata": {
        "id": "446b04a2"
      },
      "source": [
        "#Cell 14 — Training loop with advanced aug, MixUp/CutMix, EMA, AMP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "4ac8bd3c",
      "metadata": {
        "id": "4ac8bd3c"
      },
      "outputs": [],
      "source": [
        "# === Cell 14: Training loop (Plateau uses val_loss; OneCycle steps per batch) ===\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LabelSmoothingCE(nn.Module):\n",
        "    def __init__(self, eps: float = 0.10, reduction: str = 'mean'):\n",
        "        super().__init__()\n",
        "        self.eps = float(eps); self.reduction = reduction\n",
        "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
        "        n = logits.size(-1)\n",
        "        logp = F.log_softmax(logits, dim=-1)\n",
        "        with torch.no_grad():\n",
        "            true = torch.zeros_like(logp).fill_(self.eps / (n - 1))\n",
        "            true.scatter_(1, targets.unsqueeze(1), 1.0 - self.eps)\n",
        "        loss = -(true * logp).sum(dim=1)\n",
        "        return loss.mean() if self.reduction == 'mean' else loss\n",
        "\n",
        "def _get_class_weights_fallback(device: torch.device) -> torch.Tensor | None:\n",
        "    # 1) Allow explicit override via CONFIG\n",
        "    if \"CLASS_WEIGHTS\" in CONFIG and CONFIG[\"CLASS_WEIGHTS\"] is not None:\n",
        "        cw = CONFIG[\"CLASS_WEIGHTS\"]\n",
        "        if isinstance(cw, torch.Tensor):\n",
        "            return cw.to(device=device, dtype=torch.float32)\n",
        "        return torch.tensor(list(cw), dtype=torch.float32, device=device)\n",
        "    # 2) FER2013 counts fallback\n",
        "    class_counts = [4957, 547, 5121, 8989, 6077, 4002, 6198]\n",
        "    total = sum(class_counts); K = len(class_counts)\n",
        "    weights = [total / (K * c) for c in class_counts]\n",
        "    return torch.tensor(weights, dtype=torch.float32, device=device)\n",
        "\n",
        "def fit_with_aug(model: nn.Module, train_dl, val_dl, hp: dict, cfg: dict):\n",
        "    \"\"\"\n",
        "    - Supports 'plateau' (epoch-level, keyed to **val_loss**), 'cosine' (epoch-level),\n",
        "      and 'onecycle' (batch-level).\n",
        "    - Tapered MixUp/CutMix, label smoothing schedule, fine-tune tail, EMA, AMP, clipping.\n",
        "    \"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # --- Optimizer & Scheduler ---\n",
        "    opt = make_optimizer(model.parameters(), hp, cfg)\n",
        "    steps_per_epoch = len(train_dl)\n",
        "    sched = build_scheduler(opt, steps_per_epoch=steps_per_epoch)  # from Cell 02\n",
        "    use_batch_sched = scheduler_steps_per_batch()                  # True for OneCycle\n",
        "\n",
        "    stopper = EarlyStopping(patience=hp[\"PATIENCE\"], min_delta=1e-4)\n",
        "    ema = EMA(model, decay=hp[\"EMA_DECAY\"]) if cfg.get(\"USE_EMA\", True) else None\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.get(\"USE_AMP\", torch.cuda.is_available()))\n",
        "    cls_weights = _get_class_weights_fallback(device)\n",
        "\n",
        "    E = int(hp[\"EPOCHS\"])\n",
        "    ramp_epochs  = max(1, int(float(hp.get(\"AUG_RAMP_EPOCHS\", 0.25)) * E))\n",
        "    t_start = float(cfg.get(\"TAPER_START_FRAC\", 0.25))\n",
        "    t_end   = float(cfg.get(\"TAPER_END_FRAC\",   0.90))\n",
        "    base_mix_p = float(cfg.get(\"BASE_MIXUP_PROB\", 0.5))\n",
        "    base_cut_p = float(cfg.get(\"BASE_CUTMIX_PROB\", 0.5))\n",
        "    tail_frac = float(cfg.get(\"FINE_TUNE_FRACTION\", 0.12))\n",
        "    tail_start_epoch = max(1, int((1.0 - tail_frac) * E))\n",
        "\n",
        "    history = []\n",
        "    best_val_loss = float(\"inf\")\n",
        "\n",
        "    for epoch in range(1, E + 1):\n",
        "        model.train()\n",
        "        frac = (epoch - 1) / max(1, E - 1)\n",
        "\n",
        "        # Aug strength schedule\n",
        "        if cfg.get(\"USE_AUG\", True):\n",
        "            s = 0.2 + 0.6 * min(1.0, epoch / ramp_epochs)\n",
        "            if cfg.get(\"AUG_CAP_LATE\", True) and epoch >= int(0.7 * E):\n",
        "                s = min(s, 0.6)\n",
        "            augment = FER_AUG_FACTORY(s)\n",
        "        else:\n",
        "            augment = None\n",
        "\n",
        "        # MixUp/CutMix taper\n",
        "        if frac < t_start:\n",
        "            p_mix, p_cut = base_mix_p, base_cut_p\n",
        "        elif frac > t_end:\n",
        "            p_mix, p_cut = 0.0, 0.0\n",
        "        else:\n",
        "            tf = (frac - t_start) / max(1e-8, (t_end - t_start))\n",
        "            p_mix, p_cut = base_mix_p * (1.0 - tf), base_cut_p * (1.0 - tf)\n",
        "\n",
        "        # Fine-tune tail\n",
        "        if epoch >= tail_start_epoch:\n",
        "            augment = None; p_mix = p_cut = 0.0\n",
        "            for g in opt.param_groups:\n",
        "                g['lr'] = max(hp[\"LR_MIN\"], 0.2 * hp[\"LR\"])\n",
        "\n",
        "        # Label smoothing schedule\n",
        "        eps0 = float(cfg.get(\"LABEL_SMOOTH_START\", 0.10))\n",
        "        eps1 = float(cfg.get(\"LABEL_SMOOTH_END\",   0.02))\n",
        "        eps_now = eps0 + (eps1 - eps0) * frac\n",
        "        ce_ls = LabelSmoothingCE(eps=eps_now)\n",
        "\n",
        "        # ---- Train one epoch ----\n",
        "        seen, loss_sum = 0, 0.0\n",
        "        for xb, yb in train_dl:\n",
        "            xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "\n",
        "            # Normalize/augment\n",
        "            if augment is not None:\n",
        "                xb = torch.stack([augment(x) for x in xb])\n",
        "            else:\n",
        "                xb = ((xb / 255.0) - 0.5) * 2.0\n",
        "\n",
        "            # Mix strategies\n",
        "            do = random.random()\n",
        "            targets_mix, lam = yb, 1.0\n",
        "            if cfg.get(\"USE_MIXUP\", True) and do < p_mix:\n",
        "                xb, targets_mix, lam, _ = mixup_data(xb, yb, alpha=hp[\"MIXUP_ALPHA\"])\n",
        "            elif cfg.get(\"USE_CUTMIX\", True) and do < (p_mix + p_cut):\n",
        "                xb, targets_mix, lam, _ = cutmix_data(xb, yb, alpha=hp[\"CUTMIX_ALPHA\"])\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=cfg.get(\"USE_AMP\", False)):\n",
        "                logits = model(xb)\n",
        "                if isinstance(targets_mix, tuple):\n",
        "                    loss_main = lam * ce_ls(logits, targets_mix[0]) + (1.0 - lam) * ce_ls(logits, targets_mix[1])\n",
        "                else:\n",
        "                    loss_main = ce_ls(logits, yb)\n",
        "                if cls_weights is not None:\n",
        "                    ce_raw = F.cross_entropy(logits, yb, weight=cls_weights, reduction='mean')\n",
        "                    loss = 0.75 * loss_main + 0.25 * ce_raw\n",
        "                else:\n",
        "                    loss = loss_main\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            if torch.cuda.is_available():\n",
        "                scaler.unscale_(opt)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "            scaler.step(opt); scaler.update()\n",
        "\n",
        "            if ema is not None: ema.update(model)\n",
        "\n",
        "            # OneCycleLR: step every batch\n",
        "            if use_batch_sched:\n",
        "                sched.step()\n",
        "\n",
        "            bs = xb.size(0)\n",
        "            loss_sum += loss.item() * bs\n",
        "            seen += bs\n",
        "\n",
        "        # ---- Validation (clean) ----\n",
        "        @torch.no_grad()\n",
        "        def _eval(loader):\n",
        "            model.eval(); losses=[]; accs=[]\n",
        "            for xb, yb in loader:\n",
        "                xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
        "                xb = ((xb / 255.0) - 0.5) * 2.0\n",
        "                logits = model(xb)\n",
        "                losses.append(ce_ls(logits, yb).item())\n",
        "                accs.append((logits.argmax(1) == yb).float().mean().item())\n",
        "            return float(np.mean(losses)), float(np.mean(accs))\n",
        "\n",
        "        val_loss, val_acc = _eval(val_dl)\n",
        "        train_loss = loss_sum / max(1, seen)\n",
        "\n",
        "        # Epoch-level scheduler steps (cosine/plateau)\n",
        "        if not use_batch_sched:\n",
        "            if isinstance(sched, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
        "                # IMPORTANT: plateau is configured with mode='min' in Cell 02 → step on **val_loss**\n",
        "                sched.step(val_loss)\n",
        "            else:\n",
        "                sched.step()\n",
        "\n",
        "        lr_now = current_lr(opt) if 'current_lr' in globals() else opt.param_groups[0]['lr']\n",
        "\n",
        "        history.append({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss, \"val_acc\": val_acc, \"lr\": lr_now})\n",
        "        print(f\"[{epoch:03d}/{E}] train={train_loss:.4f}  val={val_loss:.4f}  acc={val_acc:.4f}  lr={lr_now:.2e}\")\n",
        "\n",
        "        # Checkpoint on best validation loss (consistent with prior cells)\n",
        "        if val_loss < best_val_loss - 1e-6:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save({\"model_state\": model.state_dict()}, CONFIG[\"SAVE_BEST_PATH\"])\n",
        "\n",
        "        if stopper.step(val_loss):\n",
        "            print(\"[EarlyStopping] stopping.\")\n",
        "            break\n",
        "\n",
        "    return history, ema\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19e00e83",
      "metadata": {
        "id": "19e00e83"
      },
      "source": [
        "#Cell 15 — Build model and quick probe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "416b0639",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "416b0639",
        "outputId": "e4869246-d839-47d4-b27c-e6e63930fe81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 221MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Probe] logits=(192, 7), loss=2.1280\n"
          ]
        }
      ],
      "source": [
        "# === Cell 15: Build model and quick probe ===\n",
        "model = HybridEffNet(num_classes=7, classifier_dropout=0.30, use_cbam=True)\n",
        "model.train()\n",
        "xb, yb = next(iter(train_dl))\n",
        "xb = ((xb/255.) - 0.5) * 2.0\n",
        "with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=CONFIG[\"USE_AMP\"]):\n",
        "    logits = model(xb.to(model.device))\n",
        "    loss = F.cross_entropy(logits, yb.to(model.device))\n",
        "loss.backward(); model.zero_grad(set_to_none=True)\n",
        "print(f\"[Probe] logits={tuple(logits.shape)}, loss={loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Model"
      ],
      "metadata": {
        "id": "0_nFfLM2-_sZ"
      },
      "id": "0_nFfLM2-_sZ"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=== BASELINE MODEL EXPERIMENT (Reference Code) ===\")\n",
        "baseline_model = expression(7)\n",
        "baseline_model.train()\n",
        "baseline_model = to_device(baseline_model, device)\n",
        "# Use reference code data loaders for baseline model\n",
        "xb_baseline, yb_baseline = next(iter(valid_dl))  # Use reference code DataLoader\n",
        "with torch.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=CONFIG[\"USE_AMP\"]):\n",
        "    logits_baseline = baseline_model(xb_baseline)\n",
        "    loss_baseline = F.cross_entropy(logits_baseline, yb_baseline)\n",
        "loss_baseline.backward(); baseline_model.zero_grad(set_to_none=True)\n",
        "print(f\"[Baseline Probe] logits={tuple(logits_baseline.shape)}, loss={loss_baseline.item():.4f}\")\n",
        "print(\"=== END BASELINE MODEL EXPERIMENT ===\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIXwbpbR-DCH",
        "outputId": "951881b6-19c0-4ba0-ae9c-be782a0dc738"
      },
      "id": "kIXwbpbR-DCH",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== BASELINE MODEL EXPERIMENT (Reference Code) ===\n",
            "[Baseline Probe] logits=(800, 7), loss=1.9599\n",
            "=== END BASELINE MODEL EXPERIMENT ===\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "2bdUrCRcdw-g",
      "metadata": {
        "id": "2bdUrCRcdw-g"
      },
      "outputs": [],
      "source": [
        "# === Cell (Aug Utils): localized_erasing (drop in before Cell 16, near your aug utilities) ===\n",
        "import random\n",
        "import torch\n",
        "\n",
        "def _neutral_fill_value(x: torch.Tensor) -> float:\n",
        "    \"\"\"\n",
        "    Choose a mid-gray fill value consistent with the current numeric range.\n",
        "    \"\"\"\n",
        "    xmin = float(x.min())\n",
        "    xmax = float(x.max())\n",
        "    # Heuristics for common ranges\n",
        "    if xmin >= -1.0 and xmax <= 1.0:\n",
        "        return 0.0          # mid-gray in [-1, 1]\n",
        "    if xmin >= 0.0 and xmax <= 1.0:\n",
        "        return 0.5          # mid-gray in [0, 1]\n",
        "    if xmax > 1.0:          # likely [0,255]\n",
        "        return 127.5\n",
        "    return 0.0              # safe default\n",
        "\n",
        "def localized_erasing(img: torch.Tensor, min_frac: float = 0.01, max_frac: float = 0.05) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Randomly erase a localized rectangular region in the input image tensor.\n",
        "    Intended to be used inside your augmentation bank (e.g., occlusion_bank).\n",
        "\n",
        "    Args:\n",
        "        img: Tensor of shape [C, H, W]. Works with ranges [-1,1], [0,1], or [0,255].\n",
        "        min_frac: Minimum fraction of the image area to erase.\n",
        "        max_frac: Maximum fraction of the image area to erase.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: image with an erased patch (in-place on a clone).\n",
        "    \"\"\"\n",
        "    if not torch.is_tensor(img) or img.ndim != 3:\n",
        "        raise TypeError(\"localized_erasing expects a tensor of shape [C, H, W].\")\n",
        "\n",
        "    c, h, w = img.shape\n",
        "    area = h * w\n",
        "    if area <= 0:\n",
        "        return img\n",
        "\n",
        "    # Sample target erase area and aspect ratio\n",
        "    erase_area = random.uniform(min_frac, max_frac) * area\n",
        "    aspect = random.uniform(0.3, 3.3)\n",
        "\n",
        "    eh = int(round((erase_area * aspect) ** 0.5))\n",
        "    ew = int(round((erase_area / aspect) ** 0.5))\n",
        "\n",
        "    if eh < 1 or ew < 1 or eh >= h or ew >= w:\n",
        "        return img  # skip if the patch is degenerate or too large\n",
        "\n",
        "    top = random.randint(0, h - eh)\n",
        "    left = random.randint(0, w - ew)\n",
        "\n",
        "    fill_val = _neutral_fill_value(img)\n",
        "    out = img.clone()\n",
        "    out[:, top:top+eh, left:left+ew] = fill_val\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RUN_BASELINE_EXPERIMENT = True\n",
        "\n",
        "if RUN_BASELINE_EXPERIMENT:\n",
        "    print(\"=== TRAINING REFERENCE CODE BASELINE MODEL ===\")\n",
        "    baseline_model = expression(7)\n",
        "    baseline_model = to_device(baseline_model, device)\n",
        "\n",
        "    # Create a simple training function for baseline model using reference code methods\n",
        "    def train_baseline_model(model, train_loader, valid_loader, epochs=10):\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "        best_acc = 0.0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            for batch in train_loader:\n",
        "                loss = model.training_step(batch)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "\n",
        "            # Validation\n",
        "            model.eval()\n",
        "            val_outputs = []\n",
        "            with torch.no_grad():\n",
        "                for batch in valid_loader:\n",
        "                    val_out = model.validation_step(batch)\n",
        "                    val_outputs.append(val_out)\n",
        "\n",
        "            val_result = model.validation_epoch_end(val_outputs)\n",
        "            model.epoch_end(epoch, val_result)\n",
        "\n",
        "            if val_result['val_acc'] > best_acc:\n",
        "                best_acc = val_result['val_acc']\n",
        "                # Save best baseline model\n",
        "                torch.save({\"model_state\": model.state_dict()},\n",
        "                          CONFIG[\"SAVE_BEST_PATH\"].parent / \"best_baseline_model.pth\")\n",
        "\n",
        "        return best_acc\n",
        "\n",
        "    print(f\"[Baseline] Using reference code DataLoaders - train batches: {len(baseline_train_dl)}, valid batches: {len(baseline_valid_dl)}\")\n",
        "    best_baseline_acc = train_baseline_model(baseline_model, baseline_train_dl, baseline_valid_dl, epochs=5)\n",
        "\n",
        "    # Train baseline model using reference code DataLoaders\n",
        "    print(f\"Best baseline model accuracy: {best_baseline_acc:.4f}\")\n",
        "    print(\"=== BASELINE MODEL TRAINING COMPLETE ===\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "U4pG4GLq_LLa",
        "outputId": "d8cc5cd0-4a7a-4895-fca6-5e6e4ae7e940"
      },
      "id": "U4pG4GLq_LLa",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== TRAINING REFERENCE CODE BASELINE MODEL ===\n",
            "[Baseline] Using reference code DataLoaders - train batches: 150, valid batches: 5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (192x36864 and 9216x2304)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2831530929.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Baseline] Using reference code DataLoaders - train batches: {len(baseline_train_dl)}, valid batches: {len(baseline_valid_dl)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mbest_baseline_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_baseline_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_train_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_valid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Train baseline model using reference code DataLoaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2831530929.py\u001b[0m in \u001b[0;36mtrain_baseline_model\u001b[0;34m(model, train_loader, valid_loader, epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-999728271.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1197506825.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xb)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (192x36864 and 9216x2304)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1542643e",
      "metadata": {
        "id": "1542643e"
      },
      "source": [
        "Cell 16 — Train (main run)\n",
        "# === Cell 16: Train main run ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "16f302e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "16f302e7",
        "outputId": "9acf2f39-439a-4007-f613-01ea84efff06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== TRAINING HYBRIDEFFNET MODEL ===\n",
            "[001/70] train=1.8340  val=1.9945  acc=0.1292  lr=1.04e-04\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Parent directory project/checkpoints does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3031100028.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# === Cell 16: Train main run ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=== TRAINING HYBRIDEFFNET MODEL ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mema_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_with_aug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=== HYBRIDEFFNET TRAINING COMPLETE ===\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2910829703.py\u001b[0m in \u001b[0;36mfit_with_aug\u001b[0;34m(model, train_dl, val_dl, hp, cfg)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_val_loss\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mbest_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"model_state\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SAVE_BEST_PATH\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstopper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m             _save(\n\u001b[1;32m    968\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             super().__init__(\n\u001b[0;32m--> 792\u001b[0;31m                 torch._C.PyTorchFileWriter(\n\u001b[0m\u001b[1;32m    793\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_crc32_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_get_storage_alignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                 )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Parent directory project/checkpoints does not exist."
          ]
        }
      ],
      "source": [
        "# === Cell 16: Train main run ===\n",
        "print(\"=== TRAINING HYBRIDEFFNET MODEL ===\")\n",
        "history, ema_obj = fit_with_aug(model, train_dl, val_dl, HP, CONFIG)\n",
        "print(\"=== HYBRIDEFFNET TRAINING COMPLETE ===\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c03e38",
      "metadata": {
        "id": "13c03e38"
      },
      "source": [
        "# === Cell 18: Evaluation (val clean; test clean + optional TTA) ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZNyRyb5kYMFY",
      "metadata": {
        "id": "ZNyRyb5kYMFY"
      },
      "outputs": [],
      "source": [
        "# === Cell pre-18\n",
        "from torch.optim.swa_utils import update_bn\n",
        "import torch, copy\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "def has_batchnorm(m: nn.Module) -> bool:\n",
        "    return any(isinstance(x, nn.modules.batchnorm._BatchNorm) for x in m.modules())\n",
        "\n",
        "# --- 0) Evaluation transform from the validation loader (if available) ---\n",
        "def _get_eval_transform():\n",
        "    if 'val_dl' in globals() and hasattr(val_dl, 'dataset') and hasattr(val_dl.dataset, 'transform'):\n",
        "        return val_dl.dataset.transform\n",
        "    return None\n",
        "\n",
        "EVAL_TF = _get_eval_transform()\n",
        "\n",
        "# --- 1) Build a CLEAN train loader that uses the eval transform (no aug) ---\n",
        "class _TransformView(Dataset):\n",
        "    \"\"\"Wrap an existing Dataset and override its transform for __getitem__.\"\"\"\n",
        "    def __init__(self, base_ds, transform):\n",
        "        self.base = base_ds\n",
        "        self.transform = transform\n",
        "    def __len__(self):  return len(self.base)\n",
        "    def __getitem__(self, i):\n",
        "        x, y = self.base[i]\n",
        "        if self.transform is not None:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "if EVAL_TF is not None:\n",
        "    train_ds_clean = _TransformView(train_dl.dataset, EVAL_TF)\n",
        "    train_dl_clean = DataLoader(\n",
        "        train_ds_clean,\n",
        "        batch_size=val_dl.batch_size,   # match val/test batch size\n",
        "        shuffle=False,\n",
        "        num_workers=val_dl.num_workers,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True\n",
        "    )\n",
        "else:\n",
        "    print(\"[BN][WARN] No eval transform detected; reusing train_dl with explicit normalization.\")\n",
        "    train_dl_clean = train_dl\n",
        "\n",
        "# --- 2) BN recalibration ---\n",
        "if has_batchnorm(model):\n",
        "    print(\"[BN] Recalibrating BN running stats...\")\n",
        "    dev = next(model.parameters()).device\n",
        "    was_training = model.training\n",
        "    model.train()\n",
        "\n",
        "    # If evaluating EMA weights, apply them BEFORE recalibration.\n",
        "    # try: ema_tail.apply_shadow(model)\n",
        "    # except NameError: pass\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _clean_iter(dloader):\n",
        "        for xb, _ in dloader:\n",
        "            xb = xb.to(dev, non_blocking=True)\n",
        "            if EVAL_TF is None:                 # only if we didn't inherit Normalize\n",
        "                xb = ((xb / 255.) - 0.5) * 2.0   # [-1, 1]\n",
        "            yield xb                             # update_bn expects the input tensor\n",
        "\n",
        "    update_bn(_clean_iter(train_dl_clean), model)\n",
        "    model.train(was_training); model.eval()\n",
        "    print(\"[BN] Done.\")\n",
        "\n",
        "    # --- 3) Quick clean evaluation (consistent with above normalization) ---\n",
        "    @torch.no_grad()\n",
        "    def _eval_quick(loader):\n",
        "        model.eval()\n",
        "        correct = total = 0\n",
        "        dev = next(model.parameters()).device\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(dev, non_blocking=True), yb.to(dev, non_blocking=True)\n",
        "            if EVAL_TF is None:\n",
        "                xb = ((xb / 255.) - 0.5) * 2.0\n",
        "            pred = model(xb).argmax(1)\n",
        "            correct += (pred == yb).sum().item()\n",
        "            total   += yb.size(0)\n",
        "        return correct / max(1, total)\n",
        "\n",
        "    print(f\"[BN] val_post={_eval_quick(val_dl):.4f}  test_post={_eval_quick(test_dl):.4f}\")\n",
        "\n",
        "    # If you temporarily applied EMA above and want to revert:\n",
        "    # try: ema_tail.restore(model)\n",
        "    # except NameError: pass\n",
        "else:\n",
        "    print(\"[BN] No BN layers found; skipping.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74d24fd0",
      "metadata": {
        "id": "74d24fd0"
      },
      "outputs": [],
      "source": [
        "# === Cell 18: Evaluation (val clean; test clean + optional TTA) ===\n",
        "@torch.no_grad()\n",
        "def eval_loader(model, loader):\n",
        "    model.eval(); total=0; correct=0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(model.device), yb.to(model.device)\n",
        "        xb = ((xb/255.) - 0.5) * 2.0\n",
        "        pred = model(xb).argmax(1)\n",
        "        correct += (pred==yb).sum().item(); total += yb.size(0)\n",
        "    return correct/total\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_loader_tta(model, loader, n=6):\n",
        "    \"\"\"\n",
        "    TTA with H-flip variants. We average PROBABILITIES (softmax), not logits.\n",
        "    \"\"\"\n",
        "    model.eval(); total=0; correct=0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(model.device), yb.to(model.device)\n",
        "        xb = ((xb/255.) - 0.5) * 2.0\n",
        "        prob_sum = None\n",
        "        for t in range(n):\n",
        "            # simple TTA: alternate original and horizontal flip\n",
        "            xb_t = xb.flip(-1) if (t % 2 == 1) else xb\n",
        "            logits = model(xb_t)\n",
        "            probs  = torch.softmax(logits, dim=1)\n",
        "            prob_sum = probs if prob_sum is None else (prob_sum + probs)\n",
        "        pred = (prob_sum / float(n)).argmax(1)\n",
        "        correct += (pred==yb).sum().item(); total += yb.size(0)\n",
        "    return correct/total\n",
        "\n",
        "val_acc_base  = eval_loader(model, val_dl)\n",
        "test_acc_base = eval_loader(model, test_dl)\n",
        "\n",
        "val_acc_ema = test_acc_ema = None\n",
        "if 'ema_obj' in globals() and ema_obj is not None:\n",
        "    ema_obj.apply_shadow(model)\n",
        "    val_acc_ema  = eval_loader(model, val_dl)\n",
        "    test_acc_ema = eval_loader(model, test_dl)\n",
        "    ema_obj.restore(model)\n",
        "\n",
        "test_acc_tta = None\n",
        "if CONFIG[\"USE_TTA\"]:\n",
        "    if 'ema_obj' in globals() and ema_obj is not None:\n",
        "        ema_obj.apply_shadow(model)\n",
        "        test_acc_tta = eval_loader_tta(model, test_dl, n=6)\n",
        "        ema_obj.restore(model)\n",
        "    else:\n",
        "        test_acc_tta = eval_loader_tta(model, test_dl, n=6)\n",
        "\n",
        "print(f\"[Eval] val_base={val_acc_base:.4f}  val_ema={val_acc_ema}\")\n",
        "print(f\"[Eval] test_base={test_acc_base:.4f} test_ema={test_acc_ema} test_tta={test_acc_tta}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "608eb229",
      "metadata": {
        "id": "608eb229"
      },
      "source": [
        "# === Cell 19: Confusion matrix + per-class accuracy (test) ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36653a3e",
      "metadata": {
        "id": "36653a3e"
      },
      "outputs": [],
      "source": [
        "# === Cell 19: Confusion matrix + per-class accuracy (test) ===\n",
        "import itertools\n",
        "IDX2EMO = {0:'Angry',1:'Disgust',2:'Fear',3:'Happy',4:'Sad',5:'Surprise',6:'Neutral'}\n",
        "\n",
        "@torch.no_grad()\n",
        "def confusion_matrix_and_report(model, loader, num_classes=7, use_ema=True):\n",
        "    ema_local = globals().get(\"ema_obj\", None)\n",
        "    if use_ema and ema_local is not None: ema_local.apply_shadow(model)\n",
        "    model.eval(); cm=torch.zeros(num_classes,num_classes,dtype=torch.int64)\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(model.device), yb.to(model.device)\n",
        "        xb = ((xb/255.) - 0.5) * 2.0\n",
        "        preds = model(xb).argmax(1)\n",
        "        for t,p in zip(yb.view(-1), preds.view(-1)):\n",
        "            cm[t.long(), p.long()] += 1\n",
        "    if use_ema and ema_local is not None: ema_local.restore(model)\n",
        "    denom = cm.sum(1).clamp(min=1).cpu().numpy()\n",
        "    per_class = (cm.diag().cpu().numpy() / denom) * 100.0\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(6,5)); plt.imshow(cm.cpu()); plt.title(\"Confusion (rows=true, cols=pred)\")\n",
        "    plt.xticks(range(7), [IDX2EMO[i] for i in range(7)], rotation=45, ha='right')\n",
        "    plt.yticks(range(7), [IDX2EMO[i] for i in range(7)])\n",
        "    for i,j in itertools.product(range(7), range(7)):\n",
        "        if cm[i,j]>0: plt.text(j,i,int(cm[i,j]),ha='center',va='center',fontsize=7)\n",
        "    plt.tight_layout(); plt.show()\n",
        "    for i,a in enumerate(per_class):\n",
        "        print(f\"{IDX2EMO[i]:>8s}: {a:.2f}%\")\n",
        "\n",
        "confusion_matrix_and_report(model, test_dl, use_ema=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HIwXyFezq_Sf",
      "metadata": {
        "id": "HIwXyFezq_Sf"
      },
      "outputs": [],
      "source": [
        "# === Cell: Plot Accuracy and Loss Curves ===\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example structure of history:\n",
        "# history = [\n",
        "#   {'epoch': 1, 'train_loss': 1.45, 'val_loss': 1.22, 'train_acc': 0.46, 'val_acc': 0.48, 'test_acc': 0.47},\n",
        "#   {'epoch': 2, 'train_loss': 1.38, 'val_loss': 1.18, 'train_acc': 0.51, 'val_acc': 0.53, 'test_acc': 0.52},\n",
        "#   ...\n",
        "# ]\n",
        "\n",
        "epochs     = [m['epoch']      for m in history]\n",
        "train_loss = [m.get('train_loss') for m in history]\n",
        "val_loss   = [m.get('val_loss')   for m in history]\n",
        "train_acc  = [m.get('train_acc')  for m in history]\n",
        "val_acc    = [m.get('val_acc')    for m in history]\n",
        "\n",
        "# If you logged test_acc per epoch\n",
        "test_acc   = [m.get('test_acc') for m in history] if 'test_acc' in history[0] else None\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# 1. Accuracy plot\n",
        "plt.subplot(1,2,1)\n",
        "if train_acc[0] is not None:\n",
        "    plt.plot(epochs, train_acc, label=\"Train Acc\", marker='o')\n",
        "plt.plot(epochs, val_acc, label=\"Val Acc\", marker='o')\n",
        "if test_acc is not None and any(v is not None for v in test_acc):\n",
        "    plt.plot(epochs, test_acc, label=\"Test Acc\", marker='x')\n",
        "elif 'test_post' in globals():\n",
        "    plt.axhline(test_post, color='red', linestyle='--', label=f\"Final Test Acc = {test_post:.4f}\")\n",
        "plt.title(\"Accuracy Curves\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\")\n",
        "plt.legend(); plt.grid(True)\n",
        "\n",
        "# 2. Loss plot\n",
        "plt.subplot(1,2,2)\n",
        "if train_loss[0] is not None:\n",
        "    plt.plot(epochs, train_loss, label=\"Train Loss\", marker='o')\n",
        "plt.plot(epochs, val_loss, label=\"Val Loss\", marker='o', color='orange')\n",
        "plt.title(\"Loss Curves\")\n",
        "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\")\n",
        "plt.legend(); plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d19e1269",
      "metadata": {
        "id": "d19e1269"
      },
      "source": [
        "# === Cell 20: FLOPs (fvcore) + Accuracy/GFLOP ==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DuMfrwctVKMr",
      "metadata": {
        "id": "DuMfrwctVKMr"
      },
      "outputs": [],
      "source": [
        "%pip install fvcore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "050a90f1",
      "metadata": {
        "id": "050a90f1"
      },
      "outputs": [],
      "source": [
        "# === Cell 21: FLOPs (fvcore) + Accuracy/GFLOP ===\n",
        "import importlib, subprocess\n",
        "try:\n",
        "    from fvcore.nn import FlopCountAnalysis\n",
        "except Exception:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"fvcore\"])\n",
        "    from fvcore.nn import FlopCountAnalysis\n",
        "\n",
        "model.eval()\n",
        "# FLOPs don't depend on value scale; this shape matches the model's 1-channel input.\n",
        "dummy = torch.randn(1, 1, CONFIG[\"IMG_SIZE\"], CONFIG[\"IMG_SIZE\"], device=model.device)\n",
        "total_flops = FlopCountAnalysis(model, (dummy,)).total()\n",
        "MFLOPs = total_flops / 1e6\n",
        "GFLOPs = total_flops / 1e9\n",
        "print(f\"[FLOPs] {GFLOPs:.4f} GFLOPs per forward\")\n",
        "\n",
        "def best_acc(*vals):\n",
        "    vals=[v for v in vals if isinstance(v,(float,int))]\n",
        "    return max(vals) if vals else None\n",
        "\n",
        "acc_best_val  = best_acc(val_acc_base, val_acc_ema)\n",
        "acc_best_test = best_acc(test_acc_base, test_acc_ema, test_acc_tta)\n",
        "\n",
        "def efficiency_pct_per_gflop(acc_frac, gflops): return (acc_frac*100.0)/gflops\n",
        "\n",
        "if acc_best_test is not None and GFLOPs>0:\n",
        "    print(f\"[Efficiency] Test Accuracy/GFLOP: {efficiency_pct_per_gflop(acc_best_test, GFLOPs):.2f} (% per GFLOP)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0S0aIp-riFim",
      "metadata": {
        "id": "0S0aIp-riFim"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# TESTING: accuracy + image predictions\n",
        "# =========================\n",
        "import torch, numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms.functional as VF\n",
        "\n",
        "# --- Class names for readability ---\n",
        "CLASS_NAMES = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']\n",
        "\n",
        "# --- Normalisation used for eval ---\n",
        "def _to_m11(x: torch.Tensor) -> torch.Tensor:\n",
        "    # x in [0,255] (uint8/float) -> float in [-1,1]\n",
        "    return ((x.float() / 255.0) - 0.5) * 2.0\n",
        "\n",
        "# --- 1) Load the trained checkpoint ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "ckpt_path = str(CONFIG.get(\"SAVE_BEST_PATH\", \"\"))  # e.g., \"project/checkpoints/best_fer.pth\"\n",
        "assert ckpt_path, \"CONFIG['SAVE_BEST_PATH'] is empty; set the checkpoint path first.\"\n",
        "\n",
        "# Rebuild model skeleton exactly as trained\n",
        "model = HybridEffNet(num_classes=7, classifier_dropout=0.30, use_cbam=True).to(device)\n",
        "ckpt = torch.load(ckpt_path, map_location=device)\n",
        "model.load_state_dict(ckpt[\"model_state\"], strict=True)\n",
        "model.eval()\n",
        "\n",
        "# --- 2) Compute and print test accuracy (PrivateTest) ---\n",
        "@torch.no_grad()\n",
        "def eval_accuracy(model, loader) -> float:\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    for xb, yb in loader:\n",
        "        xb = _to_m11(xb.to(device, non_blocking=True))\n",
        "        yb = yb.to(device, non_blocking=True)\n",
        "        logits = model(xb)\n",
        "        pred = logits.argmax(1)\n",
        "        correct += (pred == yb).sum().item()\n",
        "        total   += yb.numel()\n",
        "    return correct / max(1, total)\n",
        "\n",
        "test_acc = eval_accuracy(model, test_dl)\n",
        "print(f\"[Test] accuracy (PrivateTest) = {test_acc:.4f}\")\n",
        "\n",
        "# --- 3a) Predict a few samples from the FER2013 test loader ---\n",
        "@torch.no_grad()\n",
        "def preview_test_predictions(model, loader, n=12):\n",
        "    model.eval()\n",
        "    xb, yb = next(iter(loader))                 # one batch\n",
        "    xb = xb[:n]                                 # first n images\n",
        "    gt = yb[:n].cpu().numpy()\n",
        "    xb_dev = _to_m11(xb.to(device, non_blocking=True))\n",
        "    logits = model(xb_dev)\n",
        "    probs  = logits.softmax(1).cpu().numpy()\n",
        "    pred   = probs.argmax(1)\n",
        "    # print a small table\n",
        "    print(\"\\n[Index]  Pred (pmax)     |  GT\")\n",
        "    for i in range(n):\n",
        "        pclass = int(pred[i]); gclass = int(gt[i])\n",
        "        pmax   = float(probs[i, pclass])\n",
        "        print(f\"{i:>6d}  {CLASS_NAMES[pclass]:<12s} ({pmax:0.3f}) |  {CLASS_NAMES[gclass]}\")\n",
        "    return pred, gt\n",
        "\n",
        "_ = preview_test_predictions(model, test_dl, n=12)\n",
        "\n",
        "# --- 3b) Predict arbitrary external images (file paths) ---\n",
        "@torch.no_grad()\n",
        "def predict_images(model, image_paths):\n",
        "    \"\"\"\n",
        "    image_paths: List[str] to arbitrary images.\n",
        "    Preprocessing: grayscale -> resize to 96x96 -> tensor [1,H,W] -> [-1,1].\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    batch = []\n",
        "    for path in image_paths:\n",
        "        img = Image.open(path).convert(\"L\")            # force grayscale\n",
        "        img = img.resize((CONFIG[\"IMG_SIZE\"], CONFIG[\"IMG_SIZE\"]), Image.BILINEAR)\n",
        "        x = torch.from_numpy(np.array(img, dtype=np.uint8))[None, ...]  # [1,H,W] uint8\n",
        "        batch.append(x)\n",
        "    xb = torch.stack(batch, dim=0)                     # [B,1,H,W]\n",
        "    xb = _to_m11(xb).to(device, non_blocking=True)\n",
        "    logits = model(xb)\n",
        "    probs  = logits.softmax(1).cpu().numpy()\n",
        "    preds  = probs.argmax(1)\n",
        "    # pretty-print\n",
        "    print(\"\\n[External Image Predictions]\")\n",
        "    for path, p in zip(image_paths, preds):\n",
        "        print(f\"{path}  ->  {CLASS_NAMES[int(p)]} (p={probs[list(preds).index(p), int(p)]:.3f})\")\n",
        "    return preds, probs\n",
        "\n",
        "# Example usage for external files (uncomment and set your paths):\n",
        "# preds, probs = predict_images(model, [\n",
        "#     \"/content/some_face1.png\",\n",
        "#     \"/content/some_face2.jpg\",\n",
        "# ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rvuKRYewo88q",
      "metadata": {
        "id": "rvuKRYewo88q"
      },
      "outputs": [],
      "source": [
        "# === Cell B: Visualize predictions on Test images (robust / auto-range) ===\n",
        "import torch, numpy as np, matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Class names (adjust if your label order differs)\n",
        "Labels = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']\n",
        "\n",
        "# If you used my BN-recal cell, EVAL_TF may exist. Otherwise this stays None.\n",
        "EVAL_TF = globals().get('EVAL_TF', None)\n",
        "\n",
        "def _transform_includes_normalize(transform) -> bool:\n",
        "    \"\"\"Detects torchvision.transforms.Normalize inside a Compose-like transform.\"\"\"\n",
        "    try:\n",
        "        from torchvision.transforms import Normalize\n",
        "        seq = getattr(transform, 'transforms', None)\n",
        "        return any(isinstance(t, Normalize) for t in (seq or []))\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "@torch.no_grad()\n",
        "def _model_ready_batch(xb: torch.Tensor, dev: torch.device) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Returns a tensor ready for the model:\n",
        "    - If val/test transform already contains Normalize(0.5,0.5), pass through.\n",
        "    - Otherwise apply explicit ((x/255)-0.5)*2 normalization.\n",
        "    \"\"\"\n",
        "    xb = xb.to(dev, non_blocking=True)\n",
        "    need_explicit_norm = True\n",
        "    if EVAL_TF is not None and _transform_includes_normalize(EVAL_TF):\n",
        "        need_explicit_norm = False\n",
        "    if need_explicit_norm:\n",
        "        xb = ((xb / 255.0) - 0.5) * 2.0\n",
        "    return xb\n",
        "\n",
        "def _to_display(img: torch.Tensor) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert a single image tensor to HxW in [0,1] for imshow.\n",
        "    Works for uint8 [0,255], float [0,1], or float [-1,1].\n",
        "    \"\"\"\n",
        "    x = img.detach().cpu()\n",
        "    if x.ndim == 3 and x.size(0) == 1:  # [1,H,W] -> [H,W]\n",
        "        x = x[0]\n",
        "    x = x.float()\n",
        "    m, M = float(x.min()), float(x.max())\n",
        "    if M > 1.5:          # likely uint8 [0,255]\n",
        "        x = x / 255.0\n",
        "    elif m < -0.25:      # likely [-1,1]\n",
        "        x = (x * 0.5) + 0.5\n",
        "    x = x.clamp(0, 1)\n",
        "    return x.numpy()\n",
        "\n",
        "@torch.no_grad()\n",
        "def fetch_batch_and_predict(model, loader: DataLoader):\n",
        "    \"\"\"Fetch first batch from loader, run model, return (xb_raw, yb, pred, conf).\"\"\"\n",
        "    model.eval()\n",
        "    dev = next(model.parameters()).device\n",
        "    xb, yb = next(iter(loader))\n",
        "    xb_for_model = _model_ready_batch(xb.clone(), dev)\n",
        "    logits = model(xb_for_model)\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    pred  = probs.argmax(1)\n",
        "    conf  = probs.max(1).values\n",
        "    return xb, yb, pred.cpu(), conf.cpu()\n",
        "\n",
        "# If you want EMA weights evaluated, uncomment:\n",
        "# try: ema_tail.apply_shadow(model)\n",
        "# except NameError: pass\n",
        "\n",
        "xb, yb, pred, conf = fetch_batch_and_predict(model, test_dl)\n",
        "\n",
        "# --- Grid render ---\n",
        "K = min(25, xb.size(0))   # number of images to show\n",
        "cols = 5\n",
        "rows = int(np.ceil(K / cols))\n",
        "plt.figure(figsize=(cols * 3, rows * 3))\n",
        "\n",
        "for i in range(K):\n",
        "    ax = plt.subplot(rows, cols, i + 1)\n",
        "    ax.imshow(_to_display(xb[i]), cmap='gray', interpolation='nearest')\n",
        "    t = Labels[int(yb[i])]\n",
        "    p = Labels[int(pred[i])]\n",
        "    c = float(conf[i])\n",
        "    ax.set_title(f\"P:{p} ({c:.2f})\\nT:{t}\", fontsize=9)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# If you applied EMA above and want to revert to base weights, uncomment:\n",
        "# try: ema_tail.restore(model)\n",
        "# except NameError: pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "359b37fe",
      "metadata": {
        "id": "359b37fe"
      },
      "source": [
        "# === Cell 22: Save final checkpoint & reload sanity ===\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30b73fe1",
      "metadata": {
        "id": "30b73fe1"
      },
      "outputs": [],
      "source": [
        "# === Cell 22: Save final checkpoint & reload sanity ===\n",
        "FINAL_PATH = CKPT_DIR / \"final_fer_model.pth\"\n",
        "torch.save({\"model_state\": model.state_dict()}, FINAL_PATH)\n",
        "print(f\"[Save] {FINAL_PATH}\")\n",
        "\n",
        "ckpt = torch.load(FINAL_PATH, map_location=\"cpu\")\n",
        "model.load_state_dict(ckpt[\"model_state\"]); model.to(model.device).eval()\n",
        "with torch.no_grad():\n",
        "    xb, yb = next(iter(val_dl))\n",
        "    xb = ((xb/255.) - 0.5) * 2.0\n",
        "    out = model(xb.to(model.device))\n",
        "print(\"[Reload] Sanity forward OK:\", tuple(out.shape))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "first_test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}